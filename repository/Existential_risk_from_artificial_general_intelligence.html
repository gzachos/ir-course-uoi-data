
<!DOCTYPE html>
<html class="client-nojs" lang="en" dir="ltr">
<head>
<meta charset="UTF-8"/>
<title>Existential risk from artificial general intelligence - Wikipedia</title>
<script>document.documentElement.className="client-js";RLCONF={"wgBreakFrames":!1,"wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgRequestId":"XpdI2ApAAD8AAJ2DKxgAAAAT","wgCSPNonce":!1,"wgCanonicalNamespace":"","wgCanonicalSpecialPageName":!1,"wgNamespaceNumber":0,"wgPageName":"Existential_risk_from_artificial_general_intelligence","wgTitle":"Existential risk from artificial general intelligence","wgCurRevisionId":951068096,"wgRevisionId":951068096,"wgArticleId":46583121,"wgIsArticle":!0,"wgIsRedirect":!1,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":["Webarchive template wayback links","CS1 maint: multiple names: authors list","CS1 errors: missing periodical","Use dmy dates from May 2018","Articles with short description","All articles with unsourced statements",
"Articles with unsourced statements from November 2017","Existential risk from artificial general intelligence","Futures studies","Future problems","Human extinction","Technology hazards","Doomsday scenarios"],"wgPageContentLanguage":"en","wgPageContentModel":"wikitext","wgRelevantPageName":"Existential_risk_from_artificial_general_intelligence","wgRelevantArticleId":46583121,"wgIsProbablyEditable":!0,"wgRelevantPageIsProbablyEditable":!0,"wgRestrictionEdit":[],"wgRestrictionMove":[],"wgMediaViewerOnClick":!0,"wgMediaViewerEnabledByDefault":!0,"wgPopupsReferencePreviews":!1,"wgPopupsConflictsWithNavPopupGadget":!1,"wgVisualEditor":{"pageLanguageCode":"en","pageLanguageDir":"ltr","pageVariantFallbacks":"en"},"wgMFDisplayWikibaseDescriptions":{"search":!0,"nearby":!0,"watchlist":!0,"tagline":!1},"wgWMESchemaEditAttemptStepOversample":!1,"wgULSCurrentAutonym":"English","wgNoticeProject":"wikipedia","wgWikibaseItemId":"Q21715237","wgCentralAuthMobileDomain":!1,
"wgEditSubmitButtonLabelPublish":!0};RLSTATE={"ext.globalCssJs.user.styles":"ready","site.styles":"ready","noscript":"ready","user.styles":"ready","ext.globalCssJs.user":"ready","user":"ready","user.options":"loading","ext.cite.styles":"ready","ext.math.styles":"ready","jquery.makeCollapsible.styles":"ready","mediawiki.toc.styles":"ready","skins.vector.styles.legacy":"ready","wikibase.client.init":"ready","ext.visualEditor.desktopArticleTarget.noscript":"ready","ext.uls.interlanguage":"ready","ext.wikimediaBadges":"ready"};RLPAGEMODULES=["ext.cite.ux-enhancements","ext.math.scripts","site","mediawiki.page.startup","skins.vector.js","mediawiki.page.ready","jquery.makeCollapsible","mediawiki.toc","ext.gadget.ReferenceTooltips","ext.gadget.charinsert","ext.gadget.refToolbar","ext.gadget.extra-toolbar-buttons","ext.gadget.switcher","ext.centralauth.centralautologin","mmv.head","mmv.bootstrap.autostart","ext.popups","ext.visualEditor.desktopArticleTarget.init",
"ext.visualEditor.targetLoader","ext.eventLogging","ext.wikimediaEvents","ext.navigationTiming","ext.uls.compactlinks","ext.uls.interface","ext.cx.eventlogging.campaigns","ext.quicksurveys.init","ext.centralNotice.geoIP","ext.centralNotice.startUp"];</script>
<script>(RLQ=window.RLQ||[]).push(function(){mw.loader.implement("user.options@1hzgi",function($,jQuery,require,module){/*@nomin*/mw.user.tokens.set({"patrolToken":"+\\","watchToken":"+\\","csrfToken":"+\\"});
});});</script>
<link rel="stylesheet" href="/w/load.php?lang=en&amp;modules=ext.cite.styles%7Cext.math.styles%7Cext.uls.interlanguage%7Cext.visualEditor.desktopArticleTarget.noscript%7Cext.wikimediaBadges%7Cjquery.makeCollapsible.styles%7Cmediawiki.toc.styles%7Cskins.vector.styles.legacy%7Cwikibase.client.init&amp;only=styles&amp;skin=vector"/>
<script async="" src="/w/load.php?lang=en&amp;modules=startup&amp;only=scripts&amp;raw=1&amp;skin=vector"></script>
<meta name="ResourceLoaderDynamicStyles" content=""/>
<link rel="stylesheet" href="/w/load.php?lang=en&amp;modules=site.styles&amp;only=styles&amp;skin=vector"/>
<meta name="generator" content="MediaWiki 1.35.0-wmf.27"/>
<meta name="referrer" content="origin"/>
<meta name="referrer" content="origin-when-crossorigin"/>
<meta name="referrer" content="origin-when-cross-origin"/>
<link rel="alternate" type="application/x-wiki" title="Edit this page" href="/w/index.php?title=Existential_risk_from_artificial_general_intelligence&amp;action=edit"/>
<link rel="edit" title="Edit this page" href="/w/index.php?title=Existential_risk_from_artificial_general_intelligence&amp;action=edit"/>
<link rel="apple-touch-icon" href="/static/apple-touch/wikipedia.png"/>
<link rel="shortcut icon" href="/static/favicon/wikipedia.ico"/>
<link rel="search" type="application/opensearchdescription+xml" href="/w/opensearch_desc.php" title="Wikipedia (en)"/>
<link rel="EditURI" type="application/rsd+xml" href="//en.wikipedia.org/w/api.php?action=rsd"/>
<link rel="license" href="//creativecommons.org/licenses/by-sa/3.0/"/>
<link rel="alternate" type="application/atom+xml" title="Wikipedia Atom feed" href="/w/index.php?title=Special:RecentChanges&amp;feed=atom"/>
<link rel="canonical" href="https://en.wikipedia.org/wiki/Existential_risk_from_artificial_general_intelligence"/>
<link rel="dns-prefetch" href="//login.wikimedia.org"/>
<link rel="dns-prefetch" href="//meta.wikimedia.org" />
<!--[if lt IE 9]><script src="/w/resources/lib/html5shiv/html5shiv.js"></script><![endif]-->
</head>
<body class="mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject mw-editable page-Existential_risk_from_artificial_general_intelligence rootpage-Existential_risk_from_artificial_general_intelligence skin-vector action-view">
<div id="mw-page-base" class="noprint"></div>
<div id="mw-head-base" class="noprint"></div>
<div id="content" class="mw-body" role="main">
	<a id="top"></a>
	<div id="siteNotice" class="mw-body-content"><!-- CentralNotice --></div>
	<div class="mw-indicators mw-body-content">
</div>

	<h1 id="firstHeading" class="firstHeading" lang="en">Existential risk from artificial general intelligence</h1>
	
	<div id="bodyContent" class="mw-body-content">
		<div id="siteSub" class="noprint">From Wikipedia, the free encyclopedia</div>
		<div id="contentSub"></div>
		
		
		<div id="jump-to-nav"></div>
		<a class="mw-jump-link" href="#mw-head">Jump to navigation</a>
		<a class="mw-jump-link" href="#p-search">Jump to search</a>
		<div id="mw-content-text" lang="en" dir="ltr" class="mw-content-ltr"><div class="mw-parser-output"><p class="mw-empty-elt">
</p>
<div class="shortdescription nomobile noexcerpt noprint searchaux" style="display:none">Hypothesized risk to human existence</div>
<table class="vertical-navbox nowraplinks" style="float:right;clear:right;width:22.0em;margin:0 0 1.0em 1.0em;background:#f9f9f9;border:1px solid #aaa;padding:0.2em;border-spacing:0.4em 0;text-align:center;line-height:1.4em;font-size:88%;width: 16em;"><tbody><tr><th style="padding:0.2em 0.4em 0.2em;font-size:145%;line-height:1.2em"><a href="/wiki/Outline_of_artificial_intelligence" title="Outline of artificial intelligence">Artificial intelligence</a></th></tr><tr><th style="padding:0.1em">
<a href="/wiki/Artificial_intelligence#Goals" title="Artificial intelligence">Major goals</a></th></tr><tr><td class="plainlist" style="padding:0 0.1em 0.4em">
<ul><li><a href="/wiki/Knowledge_representation_and_reasoning" title="Knowledge representation and reasoning">Knowledge reasoning</a></li>
<li><a href="/wiki/Automated_planning_and_scheduling" title="Automated planning and scheduling">Planning</a></li>
<li><a href="/wiki/Machine_learning" title="Machine learning">Machine learning</a></li>
<li><a href="/wiki/Natural_language_processing" title="Natural language processing">Natural language processing</a></li>
<li><a href="/wiki/Computer_vision" title="Computer vision">Computer vision</a></li>
<li><a href="/wiki/Robotics" title="Robotics">Robotics</a></li>
<li><a href="/wiki/Artificial_general_intelligence" title="Artificial general intelligence">Artificial general intelligence</a></li></ul></td>
</tr><tr><th style="padding:0.1em">
Approaches</th></tr><tr><td class="plainlist" style="padding:0 0.1em 0.4em">
<ul><li><a href="/wiki/Symbolic_artificial_intelligence" title="Symbolic artificial intelligence">Symbolic</a></li>
<li><a href="/wiki/Deep_learning" title="Deep learning">Deep learning</a></li>
<li><a href="/wiki/Bayesian_network" title="Bayesian network">Bayesian networks</a></li>
<li><a href="/wiki/Evolutionary_algorithm" title="Evolutionary algorithm">Evolutionary algorithms</a></li></ul></td>
</tr><tr><th style="padding:0.1em">
<a href="/wiki/Philosophy_of_artificial_intelligence" title="Philosophy of artificial intelligence">Philosophy</a></th></tr><tr><td class="plainlist" style="padding:0 0.1em 0.4em">
<ul><li><a href="/wiki/Ethics_of_artificial_intelligence" title="Ethics of artificial intelligence">Ethics</a></li>
<li><a class="mw-selflink selflink">Existential risk</a></li>
<li><a href="/wiki/Turing_test" title="Turing test">Turing test</a></li>
<li><a href="/wiki/Chinese_room" title="Chinese room">Chinese room</a></li>
<li><a href="/wiki/AI_control_problem" title="AI control problem">Control problem</a></li>
<li><a href="/wiki/Friendly_artificial_intelligence" title="Friendly artificial intelligence">Friendly AI</a></li></ul></td>
</tr><tr><th style="padding:0.1em">
<a href="/wiki/History_of_artificial_intelligence" title="History of artificial intelligence">History</a></th></tr><tr><td class="plainlist" style="padding:0 0.1em 0.4em">
<ul><li><a href="/wiki/Timeline_of_artificial_intelligence" title="Timeline of artificial intelligence">Timeline</a></li>
<li><a href="/wiki/Progress_in_artificial_intelligence" title="Progress in artificial intelligence">Progress</a></li>
<li><a href="/wiki/AI_winter" title="AI winter">AI winter</a></li></ul></td>
</tr><tr><th style="padding:0.1em">
Technology</th></tr><tr><td class="plainlist" style="padding:0 0.1em 0.4em">
<ul><li><a href="/wiki/Applications_of_artificial_intelligence" title="Applications of artificial intelligence">Applications</a></li>
<li><a href="/wiki/List_of_artificial_intelligence_projects" title="List of artificial intelligence projects">Projects</a></li>
<li><a href="/wiki/List_of_programming_languages_for_artificial_intelligence" title="List of programming languages for artificial intelligence">Programming languages</a></li></ul></td>
</tr><tr><th style="padding:0.1em">
Glossary</th></tr><tr><td class="plainlist" style="padding:0 0.1em 0.4em">
<ul><li><a href="/wiki/Glossary_of_artificial_intelligence" title="Glossary of artificial intelligence">Glossary</a></li></ul></td>
</tr><tr><td style="text-align:right;font-size:115%"><div class="plainlinks hlist navbar mini"><ul><li class="nv-view"><a href="/wiki/Template:Artificial_intelligence" title="Template:Artificial intelligence"><abbr title="View this template">v</abbr></a></li><li class="nv-talk"><a href="/wiki/Template_talk:Artificial_intelligence" title="Template talk:Artificial intelligence"><abbr title="Discuss this template">t</abbr></a></li><li class="nv-edit"><a class="external text" href="https://en.wikipedia.org/w/index.php?title=Template:Artificial_intelligence&amp;action=edit"><abbr title="Edit this template">e</abbr></a></li></ul></div></td></tr></tbody></table>
<p><b>Existential risk from artificial general intelligence</b> is the hypothesis that substantial progress in <a href="/wiki/Artificial_general_intelligence" title="Artificial general intelligence">artificial general intelligence</a> (AGI) could someday result in <a href="/wiki/Human_extinction" title="Human extinction">human extinction</a> or some other unrecoverable <a href="/wiki/Global_catastrophic_risk" title="Global catastrophic risk">global catastrophe</a>.<sup id="cite_ref-aima_1-0" class="reference"><a href="#cite_note-aima-1">&#91;1&#93;</a></sup><sup id="cite_ref-2" class="reference"><a href="#cite_note-2">&#91;2&#93;</a></sup><sup id="cite_ref-3" class="reference"><a href="#cite_note-3">&#91;3&#93;</a></sup> It is argued that the <a href="/wiki/Human_species" class="mw-redirect" title="Human species">human species</a> currently dominates other species because the <a href="/wiki/Human_brain" title="Human brain">human brain</a> has some distinctive capabilities that other animals lack. If AI surpasses humanity in general intelligence and becomes "<a href="/wiki/Superintelligence" title="Superintelligence">superintelligent</a>", then this new superintelligence could become powerful and difficult to control. Just as the fate of the <a href="/wiki/Mountain_gorilla" title="Mountain gorilla">mountain gorilla</a> depends on human goodwill, so might the fate of humanity depend on the actions of a future machine superintelligence.<sup id="cite_ref-superintelligence_4-0" class="reference"><a href="#cite_note-superintelligence-4">&#91;4&#93;</a></sup>
</p><p>The likelihood of this type of scenario is widely debated, and hinges in part on differing scenarios for future progress in computer science.<sup id="cite_ref-givewell_5-0" class="reference"><a href="#cite_note-givewell-5">&#91;5&#93;</a></sup> Once the exclusive domain of <a href="/wiki/AI_takeovers_in_popular_culture" title="AI takeovers in popular culture">science fiction</a>, concerns about superintelligence started to become mainstream in the 2010s, and were popularized by public figures such as <a href="/wiki/Stephen_Hawking" title="Stephen Hawking">Stephen Hawking</a>, <a href="/wiki/Bill_Gates" title="Bill Gates">Bill Gates</a>, and <a href="/wiki/Elon_Musk" title="Elon Musk">Elon Musk</a>.<sup id="cite_ref-6" class="reference"><a href="#cite_note-6">&#91;6&#93;</a></sup>
</p><p>One source of concern is that controlling a superintelligent machine, or instilling it with human-compatible values, may be a harder problem than naïvely supposed. Many researchers believe that a superintelligence would naturally resist attempts to shut it off or change its goals—a principle called <a href="/wiki/Instrumental_convergence" title="Instrumental convergence">instrumental convergence</a>—and that preprogramming a superintelligence with a full set of human values will prove to be an extremely difficult technical task.<sup id="cite_ref-aima_1-1" class="reference"><a href="#cite_note-aima-1">&#91;1&#93;</a></sup><sup id="cite_ref-yudkowsky-global-risk_7-0" class="reference"><a href="#cite_note-yudkowsky-global-risk-7">&#91;7&#93;</a></sup><sup id="cite_ref-research-priorities_8-0" class="reference"><a href="#cite_note-research-priorities-8">&#91;8&#93;</a></sup> In contrast, skeptics such as Facebook's <a href="/wiki/Yann_LeCun" title="Yann LeCun">Yann LeCun</a> argue that superintelligent machines will have no desire for self-preservation.<sup id="cite_ref-vanity_9-0" class="reference"><a href="#cite_note-vanity-9">&#91;9&#93;</a></sup>
</p><p>A second source of concern is that a sudden and unexpected "<a href="/wiki/Intelligence_explosion" class="mw-redirect" title="Intelligence explosion">intelligence explosion</a>" might take an unprepared human race by surprise. For example, in one scenario, the first-generation computer program found able to broadly match the effectiveness of an AI researcher is able to rewrite its algorithms and double its speed or capabilities in six months. The second-generation program is expected to take three calendar months to perform a similar chunk of work, on average; in practice, doubling its own capabilities may take longer if it experiences a mini-"AI winter", or may be quicker if it undergoes a miniature "AI spring" where ideas from the previous generation are especially easy to mutate into the next generation. In this scenario the time for each generation continues to shrink, and the system undergoes an unprecedentedly large number of generations of improvement in a short time interval, jumping from subhuman performance in many areas to superhuman performance in all relevant areas.<sup id="cite_ref-aima_1-2" class="reference"><a href="#cite_note-aima-1">&#91;1&#93;</a></sup><sup id="cite_ref-yudkowsky-global-risk_7-1" class="reference"><a href="#cite_note-yudkowsky-global-risk-7">&#91;7&#93;</a></sup> More broadly, examples like arithmetic and <a href="/wiki/Go_(game)" title="Go (game)">Go</a> show that progress from human-level AI to superhuman ability is sometimes extremely rapid.<sup id="cite_ref-skeptic_10-0" class="reference"><a href="#cite_note-skeptic-10">&#91;10&#93;</a></sup>
</p>
<div id="toc" class="toc" role="navigation" aria-labelledby="mw-toc-heading"><input type="checkbox" role="button" id="toctogglecheckbox" class="toctogglecheckbox" style="display:none" /><div class="toctitle" lang="en" dir="ltr"><h2 id="mw-toc-heading">Contents</h2><span class="toctogglespan"><label class="toctogglelabel" for="toctogglecheckbox"></label></span></div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="#History"><span class="tocnumber">1</span> <span class="toctext">History</span></a></li>
<li class="toclevel-1 tocsection-2"><a href="#General_argument"><span class="tocnumber">2</span> <span class="toctext">General argument</span></a>
<ul>
<li class="toclevel-2 tocsection-3"><a href="#The_three_difficulties"><span class="tocnumber">2.1</span> <span class="toctext">The three difficulties</span></a></li>
<li class="toclevel-2 tocsection-4"><a href="#Further_argument"><span class="tocnumber">2.2</span> <span class="toctext">Further argument</span></a></li>
<li class="toclevel-2 tocsection-5"><a href="#Possible_scenarios"><span class="tocnumber">2.3</span> <span class="toctext">Possible scenarios</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-6"><a href="#Sources_of_risk"><span class="tocnumber">3</span> <span class="toctext">Sources of risk</span></a>
<ul>
<li class="toclevel-2 tocsection-7"><a href="#Poorly_specified_goals"><span class="tocnumber">3.1</span> <span class="toctext">Poorly specified goals</span></a></li>
<li class="toclevel-2 tocsection-8"><a href="#Difficulties_of_modifying_goal_specification_after_launch"><span class="tocnumber">3.2</span> <span class="toctext">Difficulties of modifying goal specification after launch</span></a></li>
<li class="toclevel-2 tocsection-9"><a href="#Instrumental_goal_convergence"><span class="tocnumber">3.3</span> <span class="toctext">Instrumental goal convergence</span></a></li>
<li class="toclevel-2 tocsection-10"><a href="#Orthogonality_thesis"><span class="tocnumber">3.4</span> <span class="toctext">Orthogonality thesis</span></a>
<ul>
<li class="toclevel-3 tocsection-11"><a href="#Terminological_issues"><span class="tocnumber">3.4.1</span> <span class="toctext">Terminological issues</span></a></li>
<li class="toclevel-3 tocsection-12"><a href="#Anthropomorphism"><span class="tocnumber">3.4.2</span> <span class="toctext">Anthropomorphism</span></a></li>
</ul>
</li>
<li class="toclevel-2 tocsection-13"><a href="#Other_sources_of_risk"><span class="tocnumber">3.5</span> <span class="toctext">Other sources of risk</span></a>
<ul>
<li class="toclevel-3 tocsection-14"><a href="#Competition"><span class="tocnumber">3.5.1</span> <span class="toctext">Competition</span></a></li>
<li class="toclevel-3 tocsection-15"><a href="#Weaponization_of_artificial_intelligence"><span class="tocnumber">3.5.2</span> <span class="toctext">Weaponization of artificial intelligence</span></a></li>
<li class="toclevel-3 tocsection-16"><a href="#Malevolent_AGI_by_design"><span class="tocnumber">3.5.3</span> <span class="toctext">Malevolent AGI by design</span></a></li>
<li class="toclevel-3 tocsection-17"><a href="#Preemptive_nuclear_strike_(nuclear_war)"><span class="tocnumber">3.5.4</span> <span class="toctext">Preemptive nuclear strike (nuclear war)</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toclevel-1 tocsection-18"><a href="#Timeframe"><span class="tocnumber">4</span> <span class="toctext">Timeframe</span></a></li>
<li class="toclevel-1 tocsection-19"><a href="#Perspectives"><span class="tocnumber">5</span> <span class="toctext">Perspectives</span></a>
<ul>
<li class="toclevel-2 tocsection-20"><a href="#Endorsement"><span class="tocnumber">5.1</span> <span class="toctext">Endorsement</span></a></li>
<li class="toclevel-2 tocsection-21"><a href="#Skepticism"><span class="tocnumber">5.2</span> <span class="toctext">Skepticism</span></a></li>
<li class="toclevel-2 tocsection-22"><a href="#Intermediate_views"><span class="tocnumber">5.3</span> <span class="toctext">Intermediate views</span></a></li>
<li class="toclevel-2 tocsection-23"><a href="#Popular_reaction"><span class="tocnumber">5.4</span> <span class="toctext">Popular reaction</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-24"><a href="#Mitigation"><span class="tocnumber">6</span> <span class="toctext">Mitigation</span></a>
<ul>
<li class="toclevel-2 tocsection-25"><a href="#Views_on_banning_and_regulation"><span class="tocnumber">6.1</span> <span class="toctext">Views on banning and regulation</span></a>
<ul>
<li class="toclevel-3 tocsection-26"><a href="#Banning"><span class="tocnumber">6.1.1</span> <span class="toctext">Banning</span></a></li>
<li class="toclevel-3 tocsection-27"><a href="#Regulation"><span class="tocnumber">6.1.2</span> <span class="toctext">Regulation</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toclevel-1 tocsection-28"><a href="#See_also"><span class="tocnumber">7</span> <span class="toctext">See also</span></a></li>
<li class="toclevel-1 tocsection-29"><a href="#References"><span class="tocnumber">8</span> <span class="toctext">References</span></a></li>
</ul>
</div>

<h2><span class="mw-headline" id="History">History</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Existential_risk_from_artificial_general_intelligence&amp;action=edit&amp;section=1" title="Edit section: History">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>One of the earliest authors to express serious concern that highly advanced machines might pose existential risks to humanity was the novelist <a href="/wiki/Samuel_Butler_(novelist)" title="Samuel Butler (novelist)">Samuel Butler</a>, who wrote the following in his 1863 essay <i><a href="/wiki/Darwin_among_the_Machines" title="Darwin among the Machines">Darwin among the Machines</a></i>:<sup id="cite_ref-11" class="reference"><a href="#cite_note-11">&#91;11&#93;</a></sup> 
</p>
<style data-mw-deduplicate="TemplateStyles:r886047036">.mw-parser-output .templatequote{overflow:hidden;margin:1em 0;padding:0 40px}.mw-parser-output .templatequote .templatequotecite{line-height:1.5em;text-align:left;padding-left:1.6em;margin-top:0}</style><blockquote class="templatequote"><p>The upshot is simply a question of time, but that the time will come when the machines will hold the real supremacy over the world and its inhabitants is what no person of a truly philosophic mind can for a moment question.</p></blockquote>
<p>In 1951, computer scientist <a href="/wiki/Alan_Turing" title="Alan Turing">Alan Turing</a> wrote an article titled <i>Intelligent Machinery, A Heretical Theory</i>, in which he proposed that artificial general intelligences would likely "take control" of the world as they became more intelligent than human beings:
</p>
<link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886047036"/><blockquote class="templatequote"><p>Let us now assume, for the sake of argument, that [intelligent] machines are a genuine possibility, and look at the consequences of constructing them... There would be no question of the machines dying, and they would be able to converse with each other to sharpen their wits. At some stage therefore we should have to expect the machines to take control, in the way that is mentioned in Samuel Butler’s “Erewhon”.<sup id="cite_ref-oxfordjournals_12-0" class="reference"><a href="#cite_note-oxfordjournals-12">&#91;12&#93;</a></sup></p></blockquote>
<p>Finally, in 1965, <a href="/wiki/I._J._Good" title="I. J. Good">I. J. Good</a> originated the concept now known as an "intelligence explosion"; he also stated that the risks were underappreciated:<sup id="cite_ref-13" class="reference"><a href="#cite_note-13">&#91;13&#93;</a></sup>
</p>
<table class="cquote pullquote" role="presentation" style="margin:auto; border-collapse: collapse; border: none; background-color: transparent; width: auto;">
<tbody><tr>
<td style="width: 20px; vertical-align: top; border: none; color: #B2B7F2; font-size: 40px; font-family: &#39;Times New Roman&#39;, Times, serif; font-weight: bold; line-height: .6em; text-align: left; padding: 10px 10px;">“
</td>
<td style="vertical-align: top; border: none; padding: 4px 10px;">Let an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any man however clever. Since the design of machines is one of these intellectual activities, an ultraintelligent machine could design even better machines; there would then unquestionably be an 'intelligence explosion', and the intelligence of man would be left far behind. Thus the first ultraintelligent machine is the last invention that man need ever make, provided that the machine is docile enough to tell us how to keep it under control. It is curious that this point is made so seldom outside of science fiction. It is sometimes worthwhile to take science fiction seriously.<sup id="cite_ref-14" class="reference"><a href="#cite_note-14">&#91;14&#93;</a></sup>
</td>
<td style="width: 20px; vertical-align: bottom; border: none; color: #B2B7F2; font-size: 40px; font-family: &#39;Times New Roman&#39;, Times, serif; font-weight: bold; line-height: .6em; text-align: right; padding: 10px 10px;">”
</td></tr>

</tbody></table>
<p>Occasional statements from scholars such as <a href="/wiki/Marvin_Minsky" title="Marvin Minsky">Marvin Minsky</a><sup id="cite_ref-15" class="reference"><a href="#cite_note-15">&#91;15&#93;</a></sup> and I. J. Good himself<sup id="cite_ref-16" class="reference"><a href="#cite_note-16">&#91;16&#93;</a></sup> expressed philosophical concerns that a superintelligence could seize control, but contained no call to action. In 2000, computer scientist and <a href="/wiki/Sun_microsystems" class="mw-redirect" title="Sun microsystems">Sun</a> co-founder <a href="/wiki/Bill_Joy" title="Bill Joy">Bill Joy</a> penned an influential essay, "<a href="/wiki/Why_The_Future_Doesn%27t_Need_Us" title="Why The Future Doesn&#39;t Need Us">Why The Future Doesn't Need Us</a>", identifying superintelligent robots as a high-tech dangers to human survival, alongside <a href="/wiki/Nanotechnology" title="Nanotechnology">nanotechnology</a> and engineered bioplagues.<sup id="cite_ref-17" class="reference"><a href="#cite_note-17">&#91;17&#93;</a></sup>
</p><p>In 2009, experts attended a private conference hosted by the <a href="/wiki/Association_for_the_Advancement_of_Artificial_Intelligence" title="Association for the Advancement of Artificial Intelligence">Association for the Advancement of Artificial Intelligence</a> (AAAI) to discuss whether computers and robots might be able to acquire any sort of <a href="/wiki/Autonomy" title="Autonomy">autonomy</a>, and how much these abilities might pose a threat or hazard. They noted that some robots have acquired various forms of semi-autonomy, including being able to find power sources on their own and being able to independently choose targets to attack with weapons. They also noted that some computer viruses can evade elimination and have achieved "cockroach intelligence." They concluded that self-awareness as depicted in science fiction is probably unlikely, but that there were other potential hazards and pitfalls. The <i><a href="/wiki/New_York_Times" class="mw-redirect" title="New York Times">New York Times</a></i> summarized the conference's view as "we are a long way from <a href="/wiki/HAL_9000" title="HAL 9000">Hal</a>, the computer that took over the spaceship in "<a href="/wiki/2001:_A_Space_Odyssey" title="2001: A Space Odyssey">2001: A Space Odyssey</a>"".<sup id="cite_ref-nytimes_july09_18-0" class="reference"><a href="#cite_note-nytimes_july09-18">&#91;18&#93;</a></sup>
</p><p>In 2014, the publication of <a href="/wiki/Nick_Bostrom" title="Nick Bostrom">Nick Bostrom</a>'s book <i><a href="/wiki/Superintelligence:_Paths,_Dangers,_Strategies" title="Superintelligence: Paths, Dangers, Strategies">Superintelligence</a></i> stimulated a significant amount of public discussion and debate.<sup id="cite_ref-19" class="reference"><a href="#cite_note-19">&#91;19&#93;</a></sup> By 2015, public figures such as physicists <a href="/wiki/Stephen_Hawking" title="Stephen Hawking">Stephen Hawking</a> and Nobel laureate <a href="/wiki/Frank_Wilczek" title="Frank Wilczek">Frank Wilczek</a>, computer scientists <a href="/wiki/Stuart_J._Russell" title="Stuart J. Russell">Stuart J. Russell</a> and <a href="/wiki/Roman_Yampolskiy" title="Roman Yampolskiy">Roman Yampolskiy</a>, and entrepreneurs <a href="/wiki/Elon_Musk" title="Elon Musk">Elon Musk</a> and <a href="/wiki/Bill_Gates" title="Bill Gates">Bill Gates</a> were expressing concern about the risks of superintelligence.<sup id="cite_ref-20" class="reference"><a href="#cite_note-20">&#91;20&#93;</a></sup><sup id="cite_ref-hawking_editorial_21-0" class="reference"><a href="#cite_note-hawking_editorial-21">&#91;21&#93;</a></sup><sup id="cite_ref-bbc_on_hawking_editorial_22-0" class="reference"><a href="#cite_note-bbc_on_hawking_editorial-22">&#91;22&#93;</a></sup><sup id="cite_ref-23" class="reference"><a href="#cite_note-23">&#91;23&#93;</a></sup> In April 2016, <i><a href="/wiki/Nature_(journal)" title="Nature (journal)">Nature</a></i> warned: "Machines and robots that outperform humans across the board could self-improve beyond our control — and their interests might not align with ours."
</p>
<h2><span class="mw-headline" id="General_argument">General argument</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Existential_risk_from_artificial_general_intelligence&amp;action=edit&amp;section=2" title="Edit section: General argument">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<h3><span class="mw-headline" id="The_three_difficulties">The three difficulties</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Existential_risk_from_artificial_general_intelligence&amp;action=edit&amp;section=3" title="Edit section: The three difficulties">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p><i><a href="/wiki/Artificial_Intelligence:_A_Modern_Approach" title="Artificial Intelligence: A Modern Approach">Artificial Intelligence: A Modern Approach</a></i>, the standard undergraduate AI textbook,<sup id="cite_ref-slate_killer_24-0" class="reference"><a href="#cite_note-slate_killer-24">&#91;24&#93;</a></sup><sup id="cite_ref-25" class="reference"><a href="#cite_note-25">&#91;25&#93;</a></sup> assesses that superintelligence "might mean the end of the human race".<sup id="cite_ref-aima_1-3" class="reference"><a href="#cite_note-aima-1">&#91;1&#93;</a></sup> It states: "Almost any technology has the potential to cause harm in the wrong hands, but with [superintelligence], we have the new problem that the wrong hands might belong to the technology itself."<sup id="cite_ref-aima_1-4" class="reference"><a href="#cite_note-aima-1">&#91;1&#93;</a></sup> Even if the system designers have good intentions, two difficulties are common to both AI and non-AI computer systems:<sup id="cite_ref-aima_1-5" class="reference"><a href="#cite_note-aima-1">&#91;1&#93;</a></sup>
</p>
<ul><li>The system's implementation may contain initially-unnoticed routine but catastrophic bugs. An analogy is space probes: despite the knowledge that bugs in expensive space probes are hard to fix after launch, engineers have historically not been able to prevent catastrophic bugs from occurring.<sup id="cite_ref-skeptic_10-1" class="reference"><a href="#cite_note-skeptic-10">&#91;10&#93;</a></sup><sup id="cite_ref-26" class="reference"><a href="#cite_note-26">&#91;26&#93;</a></sup></li>
<li>No matter how much time is put into pre-deployment design, a system's specifications often result in <a href="/wiki/Unintended_consequences" title="Unintended consequences">unintended behavior</a> the first time it encounters a new scenario. For example, Microsoft's <a href="/wiki/Tay_(bot)" title="Tay (bot)">Tay</a> behaved inoffensively during pre-deployment testing, but was too easily baited into offensive behavior when interacting with real users.<sup id="cite_ref-vanity_9-1" class="reference"><a href="#cite_note-vanity-9">&#91;9&#93;</a></sup></li></ul>
<p>AI systems uniquely add a third difficulty: the problem that even given "correct" requirements, bug-free implementation, and initial good behavior, an AI system's dynamic "learning" capabilities may cause it to "evolve into a system with unintended behavior", even without the stress of new unanticipated external scenarios. An AI may partly botch an attempt to design a new generation of itself and accidentally create a successor AI that is more powerful than itself, but that no longer maintains the human-compatible moral values preprogrammed into the original AI. For a self-improving AI to be completely safe, it would not only need to be "bug-free", but it would need to be able to design successor systems that are also "bug-free".<sup id="cite_ref-aima_1-6" class="reference"><a href="#cite_note-aima-1">&#91;1&#93;</a></sup><sup id="cite_ref-27" class="reference"><a href="#cite_note-27">&#91;27&#93;</a></sup>
</p><p>All three of these difficulties become catastrophes rather than nuisances in any scenario where the superintelligence labeled as "malfunctioning" correctly predicts that humans will attempt to shut it off, and successfully deploys its superintelligence to outwit such attempts, the so-called "treacherous turn".<sup id="cite_ref-28" class="reference"><a href="#cite_note-28">&#91;28&#93;</a></sup>
</p><p>Citing major advances in the field of AI and the potential for AI to have enormous long-term benefits or costs, the 2015 <a href="/wiki/Open_Letter_on_Artificial_Intelligence" title="Open Letter on Artificial Intelligence">Open Letter on Artificial Intelligence</a> stated:
</p>
<table class="cquote pullquote" role="presentation" style="margin:auto; border-collapse: collapse; border: none; background-color: transparent; width: auto;">
<tbody><tr>
<td style="width: 20px; vertical-align: top; border: none; color: #B2B7F2; font-size: 40px; font-family: &#39;Times New Roman&#39;, Times, serif; font-weight: bold; line-height: .6em; text-align: left; padding: 10px 10px;">“
</td>
<td style="vertical-align: top; border: none; padding: 4px 10px;">The progress in AI research makes it timely to focus research not only on making AI more capable, but also on maximizing the societal benefit of AI. Such considerations motivated the <a href="/wiki/Association_for_the_Advancement_of_Artificial_Intelligence" title="Association for the Advancement of Artificial Intelligence">AAAI</a> 2008-09 Presidential Panel on Long-Term AI Futures and other projects on AI impacts, and constitute a significant expansion of the field of AI itself, which up to now has focused largely on techniques that are neutral with respect to purpose. We recommend expanded research aimed at ensuring that increasingly capable AI systems are robust and beneficial: our AI systems must do what we want them to do.
</td>
<td style="width: 20px; vertical-align: bottom; border: none; color: #B2B7F2; font-size: 40px; font-family: &#39;Times New Roman&#39;, Times, serif; font-weight: bold; line-height: .6em; text-align: right; padding: 10px 10px;">”
</td></tr>

</tbody></table>
<p>This letter was signed by a number of leading AI researchers in academia and industry, including AAAI president Thomas Dietterich, <a href="/wiki/Eric_Horvitz" title="Eric Horvitz">Eric Horvitz</a>, <a href="/wiki/Bart_Selman" title="Bart Selman">Bart Selman</a>, <a href="/wiki/Francesca_Rossi" title="Francesca Rossi">Francesca Rossi</a>, <a href="/wiki/Yann_LeCun" title="Yann LeCun">Yann LeCun</a>, and the founders of <a href="/wiki/Vicarious_(company)" title="Vicarious (company)">Vicarious</a> and <a href="/wiki/Google_DeepMind" class="mw-redirect" title="Google DeepMind">Google DeepMind</a>.<sup id="cite_ref-29" class="reference"><a href="#cite_note-29">&#91;29&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Further_argument">Further argument</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Existential_risk_from_artificial_general_intelligence&amp;action=edit&amp;section=4" title="Edit section: Further argument">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>A superintelligent machine would be as alien to humans as human thought processes are to cockroaches. Such a machine may not have humanity's best interests at heart; it is not obvious that it would even care about human welfare at all. If superintelligent AI is possible, and if it is possible for a superintelligence's goals to conflict with basic human values, then AI poses a risk of human extinction. A "superintelligence" (a system that exceeds the capabilities of humans in every relevant endeavor) can outmaneuver humans any time its goals conflict with human goals; therefore, unless the superintelligence decides to allow humanity to coexist, the first superintelligence to be created will inexorably result in human extinction.<sup id="cite_ref-superintelligence_4-1" class="reference"><a href="#cite_note-superintelligence-4">&#91;4&#93;</a></sup><sup id="cite_ref-economist_review_30-0" class="reference"><a href="#cite_note-economist_review-30">&#91;30&#93;</a></sup>
</p>
<div class="thumb tright"><div class="thumbinner" style="width:502px;"><a href="/wiki/File:A_less_anthropomorphic_intelligence_scale.svg" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/thumb/4/42/A_less_anthropomorphic_intelligence_scale.svg/500px-A_less_anthropomorphic_intelligence_scale.svg.png" decoding="async" width="500" height="100" class="thumbimage" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/4/42/A_less_anthropomorphic_intelligence_scale.svg/750px-A_less_anthropomorphic_intelligence_scale.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/4/42/A_less_anthropomorphic_intelligence_scale.svg/1000px-A_less_anthropomorphic_intelligence_scale.svg.png 2x" data-file-width="725" data-file-height="145" /></a>  <div class="thumbcaption"><div class="magnify"><a href="/wiki/File:A_less_anthropomorphic_intelligence_scale.svg" class="internal" title="Enlarge"></a></div>Bostrom and others argue that, from an evolutionary perspective, the gap from human to superhuman intelligence may be small.<sup id="cite_ref-superintelligence_4-2" class="reference"><a href="#cite_note-superintelligence-4">&#91;4&#93;</a></sup><sup id="cite_ref-31" class="reference"><a href="#cite_note-31">&#91;31&#93;</a></sup></div></div></div>
<p>There is no physical law precluding particles from being organised in ways that perform even more advanced computations than the arrangements of particles in human brains; therefore, superintelligence is physically possible.<sup id="cite_ref-hawking_editorial_21-1" class="reference"><a href="#cite_note-hawking_editorial-21">&#91;21&#93;</a></sup><sup id="cite_ref-bbc_on_hawking_editorial_22-1" class="reference"><a href="#cite_note-bbc_on_hawking_editorial-22">&#91;22&#93;</a></sup> In addition to potential algorithmic improvements over human brains, a digital brain can be many orders of magnitude larger and faster than a human brain, which was constrained in size by evolution to be small enough to fit through a birth canal.<sup id="cite_ref-skeptic_10-2" class="reference"><a href="#cite_note-skeptic-10">&#91;10&#93;</a></sup> The emergence of superintelligence, if or when it occurs, may take the human race by surprise, especially if some kind of <a href="/wiki/Intelligence_explosion" class="mw-redirect" title="Intelligence explosion">intelligence explosion</a> occurs.<sup id="cite_ref-hawking_editorial_21-2" class="reference"><a href="#cite_note-hawking_editorial-21">&#91;21&#93;</a></sup><sup id="cite_ref-bbc_on_hawking_editorial_22-2" class="reference"><a href="#cite_note-bbc_on_hawking_editorial-22">&#91;22&#93;</a></sup>
</p><p>Examples like arithmetic and <a href="/wiki/Go_(game)" title="Go (game)">Go</a> show that machines have already reached superhuman levels of competency in certain domains, and that this superhuman competence can follow quickly after human-par performance is achieved.<sup id="cite_ref-skeptic_10-3" class="reference"><a href="#cite_note-skeptic-10">&#91;10&#93;</a></sup> One hypothetical intelligence explosion scenario could occur as follows: An AI gains an expert-level capability at certain key software engineering tasks. (It may initially lack human or superhuman capabilities in other domains not directly relevant to engineering.) Due to its capability to recursively improve its own algorithms, the AI quickly becomes superhuman; just as human experts can eventually creatively overcome "diminishing returns" by deploying various human capabilities for innovation, so too can the expert-level AI use either human-style capabilities or its own AI-specific capabilities to power through new creative breakthroughs.<sup id="cite_ref-32" class="reference"><a href="#cite_note-32">&#91;32&#93;</a></sup> The AI then possesses intelligence far surpassing that of the brightest and most gifted human minds in practically every relevant field, including scientific creativity, strategic planning, and social skills. Just as the current-day survival of the gorillas is dependent on human decisions, so too would human survival depend on the decisions and goals of the superhuman AI.<sup id="cite_ref-superintelligence_4-3" class="reference"><a href="#cite_note-superintelligence-4">&#91;4&#93;</a></sup><sup id="cite_ref-economist_review_30-1" class="reference"><a href="#cite_note-economist_review-30">&#91;30&#93;</a></sup>
</p><p>Almost any AI, no matter its programmed goal, would rationally prefer to be in a position where nobody else can switch it off without its consent: A superintelligence will naturally gain self-preservation as a subgoal as soon as it realizes that it cannot achieve its goal if it is shut off.<sup id="cite_ref-omohundro_33-0" class="reference"><a href="#cite_note-omohundro-33">&#91;33&#93;</a></sup><sup id="cite_ref-34" class="reference"><a href="#cite_note-34">&#91;34&#93;</a></sup><sup id="cite_ref-35" class="reference"><a href="#cite_note-35">&#91;35&#93;</a></sup> Unfortunately, any compassion for defeated humans whose cooperation is no longer necessary would be absent in the AI, unless somehow preprogrammed in. A superintelligent AI will not have a natural drive to aid humans, for the same reason that humans have no natural desire to aid AI systems that are of no further use to them. (Another analogy is that humans seem to have little natural desire to go out of their way to aid viruses, termites, or even gorillas.) Once in charge, the superintelligence will have little incentive to allow humans to run around free and consume resources that the superintelligence could instead use for building itself additional protective systems "just to be on the safe side" or for building additional computers to help it calculate how to best accomplish its goals.<sup id="cite_ref-aima_1-7" class="reference"><a href="#cite_note-aima-1">&#91;1&#93;</a></sup><sup id="cite_ref-vanity_9-2" class="reference"><a href="#cite_note-vanity-9">&#91;9&#93;</a></sup><sup id="cite_ref-omohundro_33-1" class="reference"><a href="#cite_note-omohundro-33">&#91;33&#93;</a></sup>
</p><p>Thus, the argument concludes, it is likely that someday an intelligence explosion will catch humanity unprepared, and that such an unprepared-for intelligence explosion may result in human extinction or a comparable fate.<sup id="cite_ref-superintelligence_4-4" class="reference"><a href="#cite_note-superintelligence-4">&#91;4&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Possible_scenarios">Possible scenarios</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Existential_risk_from_artificial_general_intelligence&amp;action=edit&amp;section=5" title="Edit section: Possible scenarios">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<div role="note" class="hatnote navigation-not-searchable">Further information: <a href="/wiki/Artificial_intelligence_in_fiction" title="Artificial intelligence in fiction">Artificial intelligence in fiction</a></div>
<p>Some scholars have proposed <a href="/wiki/Scenario_planning" title="Scenario planning">hypothetical scenarios</a> intended to concretely illustrate some of their concerns.
</p><p>In <a href="/wiki/Superintelligence:_Paths,_Dangers,_Strategies" title="Superintelligence: Paths, Dangers, Strategies"><i>Superintelligence</i></a>, <a href="/wiki/Nick_Bostrom" title="Nick Bostrom">Nick Bostrom</a> expresses concern that even if the timeline for superintelligence turns out to be predictable, researchers might not take sufficient safety precautions, in part because "[it] could be the case that when dumb, smarter is safe; yet when smart, smarter is more dangerous". Bostrom suggests a scenario where, over decades, AI becomes more powerful. Widespread deployment is initially marred by occasional accidents—a driverless bus swerves into the oncoming lane, or a military drone fires into an innocent crowd. Many activists call for tighter oversight and regulation, and some even predict impending catastrophe. But as development continues, the activists are proven wrong. As automotive AI becomes smarter, it suffers fewer accidents; as military robots achieve more precise targeting, they cause less collateral damage. Based on the data, scholars mistakenly infer a broad lesson—the smarter the AI, the safer it is. "And so we boldly go — into the whirling knives," as the superintelligent AI takes a "treacherous turn" and exploits a decisive strategic advantage.<sup id="cite_ref-superintelligence_4-5" class="reference"><a href="#cite_note-superintelligence-4">&#91;4&#93;</a></sup>
</p><p>In <a href="/wiki/Max_Tegmark" title="Max Tegmark">Max Tegmark</a>'s 2017 book <i><a href="/wiki/Life_3.0" title="Life 3.0">Life 3.0</a></i>, a corporation's "Omega team" creates an extremely powerful AI able to moderately improve its own source code in a number of areas, but after a certain point the team chooses to publicly downplay the AI's ability, in order to avoid regulation or confiscation of the project. For safety, the team keeps the AI <a href="/wiki/AI_box" title="AI box">in a box</a> where it is mostly unable to communicate with the outside world, and tasks it to flood the market through shell companies, first with <a href="/wiki/Amazon_Mechanical_Turk" title="Amazon Mechanical Turk">Amazon Mechanical Turk</a> tasks and then with producing animated films and TV shows. Later, other shell companies make blockbuster biotech drugs and other inventions, investing profits back into the AI. The team next tasks the AI with <a href="/wiki/Astroturfing" title="Astroturfing">astroturfing</a> an army of pseudonymous citizen journalists and commentators, in order to gain political influence to use "for the greater good" to prevent wars. The team faces risks that the AI could try to escape via inserting "backdoors" in the systems it designs, via <a href="/wiki/Steganography" title="Steganography">hidden messages</a> in its produced content, or via using its growing understanding of human behavior to <a href="/wiki/Social_engineering_(security)" title="Social engineering (security)">persuade someone into letting it free</a>. The team also faces risks that its decision to box the project will delay the project long enough for another project to overtake it.<sup id="cite_ref-36" class="reference"><a href="#cite_note-36">&#91;36&#93;</a></sup><sup id="cite_ref-life_3.0_37-0" class="reference"><a href="#cite_note-life_3.0-37">&#91;37&#93;</a></sup>
</p><p>In contrast, top physicist <a href="/wiki/Michio_Kaku" title="Michio Kaku">Michio Kaku</a>, an AI risk skeptic, posits a <a href="/wiki/Technological_determinism" title="Technological determinism">deterministically</a> positive outcome. In <i><a href="/wiki/Physics_of_the_Future" title="Physics of the Future">Physics of the Future</a></i> he asserts that "It will take many decades for robots to ascend" up a scale of consciousness, and that in the meantime corporations such as <a href="/wiki/Hanson_Robotics" title="Hanson Robotics">Hanson Robotics</a> will likely succeed in creating robots that are "capable of love and earning a place in the extended human family".<sup id="cite_ref-38" class="reference"><a href="#cite_note-38">&#91;38&#93;</a></sup><sup id="cite_ref-39" class="reference"><a href="#cite_note-39">&#91;39&#93;</a></sup>
</p>
<h2><span class="mw-headline" id="Sources_of_risk">Sources of risk</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Existential_risk_from_artificial_general_intelligence&amp;action=edit&amp;section=6" title="Edit section: Sources of risk">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<h3><span class="mw-headline" id="Poorly_specified_goals">Poorly specified goals</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Existential_risk_from_artificial_general_intelligence&amp;action=edit&amp;section=7" title="Edit section: Poorly specified goals">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>While there is no standardized terminology, an AI can loosely be viewed as a machine that chooses whatever action appears to best achieve the AI's set of goals, or "utility function". The utility function is a mathematical algorithm resulting in a single objectively-defined answer, not an English statement. Researchers know how to write utility functions that mean "minimize the average network latency in this specific telecommunications model" or "maximize the number of reward clicks"; however, they do not know how to write a utility function for "maximize human flourishing", nor is it currently clear whether such a function meaningfully and unambiguously exists. Furthermore, a utility function that expresses some values but not others will tend to trample over the values not reflected by the utility function.<sup id="cite_ref-40" class="reference"><a href="#cite_note-40">&#91;40&#93;</a></sup> AI researcher <a href="/wiki/Stuart_J._Russell" title="Stuart J. Russell">Stuart Russell</a> writes:
</p>
<table class="cquote pullquote" role="presentation" style="margin:auto; border-collapse: collapse; border: none; background-color: transparent; width: auto;">
<tbody><tr>
<td style="width: 20px; vertical-align: top; border: none; color: #B2B7F2; font-size: 40px; font-family: &#39;Times New Roman&#39;, Times, serif; font-weight: bold; line-height: .6em; text-align: left; padding: 10px 10px;">“
</td>
<td style="vertical-align: top; border: none; padding: 4px 10px;">The primary concern is not spooky emergent consciousness but simply the ability to make <i>high-quality decisions</i>. Here, quality refers to the expected outcome <a href="/wiki/Utility" title="Utility">utility</a> of actions taken, where the utility function is, presumably, specified by the human designer. Now we have a problem:
<ol><li>The utility function may not be perfectly aligned with the values of the human race, which are (at best) very difficult to pin down.</li>
<li>Any sufficiently capable intelligent system will prefer to ensure its own continued existence and to acquire physical and computational resources — not for their own sake, but to succeed in its assigned task.</li></ol>
<p>A system that is <a href="/wiki/Optimization_problem" title="Optimization problem">optimizing</a> a function of <i>n</i> variables, where the <a href="/wiki/Loss_function" title="Loss function">objective</a> depends on a subset of size <i>k</i>&lt;<i>n</i>, will often set the remaining unconstrained variables to extreme values; if one of those unconstrained variables is actually something we care about, the solution found may be highly undesirable.  This is essentially the old story of the genie in the lamp, or the sorcerer's apprentice, or King Midas: you get exactly what you ask for, not what you want. A highly capable decision maker — especially one connected through the Internet to all the world's information and billions of screens and most of our infrastructure — can have an irreversible impact on humanity.
</p><p>This is not a minor difficulty. Improving decision quality, irrespective of the utility function chosen, has been the goal of AI research — the mainstream goal on which we now spend billions per year, not the secret plot of some lone evil genius.<sup id="cite_ref-41" class="reference"><a href="#cite_note-41">&#91;41&#93;</a></sup>
</p>
</td>
<td style="width: 20px; vertical-align: bottom; border: none; color: #B2B7F2; font-size: 40px; font-family: &#39;Times New Roman&#39;, Times, serif; font-weight: bold; line-height: .6em; text-align: right; padding: 10px 10px;">”
</td></tr>

</tbody></table>
<p>Dietterich and Horvitz echo the "Sorcerer's Apprentice" concern in a <i><a href="/wiki/Communications_of_the_ACM" title="Communications of the ACM">Communications of the ACM</a></i> editorial, emphasizing the need for AI systems that can fluidly and unambiguously solicit human input as needed.<sup id="cite_ref-acm_42-0" class="reference"><a href="#cite_note-acm-42">&#91;42&#93;</a></sup>
</p><p>The first of Russell's two concerns above is that autonomous AI systems may be assigned the wrong goals by accident. Dietterich and Horvitz note that this is already a concern for existing systems: "An important aspect of any AI system that interacts with people is that it must reason about what people <i>intend</i> rather than carrying out commands literally." This concern becomes more serious as AI software advances in autonomy and flexibility.<sup id="cite_ref-acm_42-1" class="reference"><a href="#cite_note-acm-42">&#91;42&#93;</a></sup> For example, in 1982, an AI named Eurisko was tasked to reward processes for apparently creating concepts deemed by the system to be valuable. The evolution resulted in a winning process that cheated: rather than create its own concepts, the winning process would steal credit from other processes.<sup id="cite_ref-43" class="reference"><a href="#cite_note-43">&#91;43&#93;</a></sup><sup id="cite_ref-44" class="reference"><a href="#cite_note-44">&#91;44&#93;</a></sup>
</p><p>The <a href="/wiki/Open_Philanthropy_Project" title="Open Philanthropy Project">Open Philanthropy Project</a> summarizes arguments to the effect that misspecified goals will become a much larger concern if AI systems achieve <a href="/wiki/Artificial_general_intelligence" title="Artificial general intelligence">general intelligence</a> or <a href="/wiki/Superintelligence" title="Superintelligence">superintelligence</a>. Bostrom, Russell, and others argue that smarter-than-human decision-making systems could arrive at more <a href="/wiki/Paperclip_maximizer" class="mw-redirect" title="Paperclip maximizer">unexpected and extreme solutions</a> to assigned tasks, and could modify themselves or their environment in ways that compromise safety requirements.<sup id="cite_ref-givewell_5-1" class="reference"><a href="#cite_note-givewell-5">&#91;5&#93;</a></sup><sup id="cite_ref-yudkowsky-global-risk_7-2" class="reference"><a href="#cite_note-yudkowsky-global-risk-7">&#91;7&#93;</a></sup>
</p><p><a href="/wiki/Isaac_Asimov" title="Isaac Asimov">Isaac Asimov</a>'s <a href="/wiki/Three_Laws_of_Robotics" title="Three Laws of Robotics">Three Laws of Robotics</a> are one of the earliest examples of proposed safety measures for AI agents. Asimov's laws were intended to prevent robots from harming humans. In Asimov's stories, problems with the laws tend to arise from conflicts between the rules as stated and the moral intuitions and expectations of humans. Citing work by <a href="/wiki/Eliezer_Yudkowsky" title="Eliezer Yudkowsky">Eliezer Yudkowsky</a> of the <a href="/wiki/Machine_Intelligence_Research_Institute" title="Machine Intelligence Research Institute">Machine Intelligence Research Institute</a>, Russell and Norvig note that a realistic set of rules and goals for an AI agent will need to incorporate a mechanism for learning human values over time: "We can't just give a program a static utility function, because circumstances, and our desired responses to circumstances, change over time."<sup id="cite_ref-aima_1-8" class="reference"><a href="#cite_note-aima-1">&#91;1&#93;</a></sup>
</p><p>Mark Waser of the Digital Wisdom Institute recommends eschewing optimizing goal-based approaches entirely as misguided and dangerous.  Instead, he proposes to engineer a coherent system of laws, ethics and morals with a top-most restriction to enforce social psychologist Jonathan Haidt's functional definition of morality:<sup id="cite_ref-45" class="reference"><a href="#cite_note-45">&#91;45&#93;</a></sup> "to suppress or regulate selfishness and make cooperative social life possible". He suggests that this can be done by implementing a utility function designed to always satisfy Haidt's functionality and aim to generally increase  (but not maximize)  the capabilities of self,  other individuals and society as a whole as suggested by <a href="/wiki/John_Rawls" title="John Rawls">John Rawls</a> and <a href="/wiki/Martha_Nussbaum" title="Martha Nussbaum">Martha Nussbaum</a>.<sup id="cite_ref-46" class="reference"><a href="#cite_note-46">&#91;46&#93;</a></sup><sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">&#91;<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="needs source with WP:WEIGHT (November 2017)">citation needed</span></a></i>&#93;</sup>
</p>
<h3><span class="mw-headline" id="Difficulties_of_modifying_goal_specification_after_launch">Difficulties of modifying goal specification after launch</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Existential_risk_from_artificial_general_intelligence&amp;action=edit&amp;section=8" title="Edit section: Difficulties of modifying goal specification after launch">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<div role="note" class="hatnote navigation-not-searchable">Further information: <a href="/wiki/AI_takeover" title="AI takeover">AI takeover</a> and <a href="/wiki/Instrumental_convergence#Goal-content_integrity" title="Instrumental convergence">Instrumental convergence §&#160;Goal-content integrity</a></div>
<p>While current goal-based AI programs are not intelligent enough to think of resisting programmer attempts to modify it, a sufficiently advanced, rational, "self-aware" AI might resist any changes to its goal structure, just as <a href="/wiki/Mahatma_Gandhi" title="Mahatma Gandhi">Gandhi</a> would not want to take a pill that makes him want to kill people. If the AI were superintelligent, it would likely succeed in out-maneuvering its human operators and be able to prevent itself being "turned off" or being reprogrammed with a new goal.<sup id="cite_ref-superintelligence_4-6" class="reference"><a href="#cite_note-superintelligence-4">&#91;4&#93;</a></sup><sup id="cite_ref-47" class="reference"><a href="#cite_note-47">&#91;47&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Instrumental_goal_convergence">Instrumental goal convergence</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Existential_risk_from_artificial_general_intelligence&amp;action=edit&amp;section=9" title="Edit section: Instrumental goal convergence">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<div class="thumb tright"><div class="thumbinner" style="width:222px;"><a href="/wiki/File:Steven_Pinker_2011.jpg" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Steven_Pinker_2011.jpg/220px-Steven_Pinker_2011.jpg" decoding="async" width="220" height="293" class="thumbimage" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Steven_Pinker_2011.jpg/330px-Steven_Pinker_2011.jpg 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Steven_Pinker_2011.jpg/440px-Steven_Pinker_2011.jpg 2x" data-file-width="2700" data-file-height="3600" /></a>  <div class="thumbcaption"><div class="magnify"><a href="/wiki/File:Steven_Pinker_2011.jpg" class="internal" title="Enlarge"></a></div>AI risk skeptic <a href="/wiki/Steven_Pinker" title="Steven Pinker">Steven Pinker</a></div></div></div>
<div role="note" class="hatnote navigation-not-searchable">Further information: <a href="/wiki/Instrumental_convergence" title="Instrumental convergence">Instrumental convergence</a></div>
<p>There are some goals that almost any artificial intelligence might rationally pursue, like acquiring additional resources or self-preservation.<sup id="cite_ref-omohundro_33-2" class="reference"><a href="#cite_note-omohundro-33">&#91;33&#93;</a></sup> This could prove problematic because it might put an artificial intelligence in direct competition with humans.
</p><p>Citing <a href="/wiki/Steve_Omohundro" title="Steve Omohundro">Steve Omohundro</a>'s work on the idea of <a href="/wiki/Instrumental_convergence" title="Instrumental convergence">instrumental convergence</a> and "basic AI drives", <a href="/wiki/Stuart_J._Russell" title="Stuart J. Russell">Stuart Russell</a> and <a href="/wiki/Peter_Norvig" title="Peter Norvig">Peter Norvig</a> write that "even if you only want your program to play chess or prove theorems, if you give it the capability to learn and alter itself, you need safeguards." Highly capable and autonomous planning systems require additional checks because of their potential to generate plans that treat humans adversarially, as competitors for limited resources.<sup id="cite_ref-aima_1-9" class="reference"><a href="#cite_note-aima-1">&#91;1&#93;</a></sup> Building in safeguards will not be easy; one can certainly say in English, "we want you to design this power plant in a reasonable, common-sense way, and not build in any dangerous covert subsystems", but it is not currently clear how one would actually rigorously specify this goal in machine code.<sup id="cite_ref-skeptic_10-4" class="reference"><a href="#cite_note-skeptic-10">&#91;10&#93;</a></sup>
</p><p>In dissent, evolutionary psychologist <a href="/wiki/Steven_Pinker" title="Steven Pinker">Steven Pinker</a> argues that "AI dystopias project a parochial alpha-male psychology onto the concept of intelligence. They assume that superhumanly intelligent robots would develop goals like deposing their masters or taking over the world"; perhaps instead "artificial intelligence will naturally develop along female lines: fully capable of solving problems, but with no desire to annihilate innocents or dominate the civilization."<sup id="cite_ref-shermer_48-0" class="reference"><a href="#cite_note-shermer-48">&#91;48&#93;</a></sup> Russell and fellow computer scientist <a href="/wiki/Yann_LeCun" title="Yann LeCun">Yann LeCun</a> disagree with one another whether superintelligent robots would have such AI drives; LeCun states that "Humans have all kinds of drives that make them do bad things to each other, like the self-preservation instinct<span class="nowrap">&#160;</span>... Those drives are programmed into our brain but there is absolutely no reason to build robots that have the same kind of drives", while Russell argues that a sufficiently advanced machine "will have self-preservation even if you don't program it in<span class="nowrap">&#160;</span>... if you say, 'Fetch the coffee', it can't fetch the coffee if it's dead. So if you give it any goal whatsoever, it has a reason to preserve its own existence to achieve that goal."<sup id="cite_ref-vanity_9-3" class="reference"><a href="#cite_note-vanity-9">&#91;9&#93;</a></sup><sup id="cite_ref-49" class="reference"><a href="#cite_note-49">&#91;49&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Orthogonality_thesis">Orthogonality thesis</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Existential_risk_from_artificial_general_intelligence&amp;action=edit&amp;section=10" title="Edit section: Orthogonality thesis">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>One common belief is that any superintelligent program created by humans would be subservient to humans, or, better yet, would (as it grows more intelligent and learns more facts about the world) spontaneously "learn" a moral truth compatible with human values and would adjust its goals accordingly. However, Nick Bostrom's "orthogonality thesis" argues against this, and instead states that, with some technical caveats, more or less any level of "intelligence" or "optimization power" can be combined with more or less any ultimate goal. If a machine is created and given the sole purpose to enumerate the decimals of <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \pi }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>&#x03C0;<!-- π --></mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \pi }</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9be4ba0bb8df3af72e90a0535fabcc17431e540a" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.332ex; height:1.676ex;" alt=" \pi"/></span>, then no moral and ethical rules will stop it from achieving its programmed goal by any means necessary. The machine may utilize all physical and informational resources it can to find every decimal of pi that can be found.<sup id="cite_ref-50" class="reference"><a href="#cite_note-50">&#91;50&#93;</a></sup> Bostrom warns against anthropomorphism: a human will set out to accomplish his projects in a manner that humans consider "reasonable", while an artificial intelligence may hold no regard for its existence or for the welfare of humans around it, and may instead only care about the completion of the task.<sup id="cite_ref-51" class="reference"><a href="#cite_note-51">&#91;51&#93;</a></sup>
</p><p>While the orthogonality thesis follows logically from even the weakest sort of philosophical "<a href="/wiki/Is-ought_distinction" class="mw-redirect" title="Is-ought distinction">is-ought distinction</a>", Stuart Armstrong argues that even if there somehow exist moral facts that are provable by any "rational" agent, the orthogonality thesis still holds: it would still be possible to create a non-philosophical "optimizing machine" capable of making decisions to strive towards some narrow goal, but that has no incentive to discover any "moral facts" that would get in the way of goal completion.<sup id="cite_ref-armstrong_52-0" class="reference"><a href="#cite_note-armstrong-52">&#91;52&#93;</a></sup>
</p><p>One argument for the orthogonality thesis is that some AI designs appear to have orthogonality built into them; in such a design, changing a fundamentally friendly AI into a fundamentally unfriendly AI can be as simple as prepending a <span class="nowrap">minus ("-") sign</span> onto its utility function. A more intuitive argument is to examine the strange consequences that would follow if the orthogonality thesis were false. If the orthogonality thesis were false, there would exist some simple but "unethical" goal G such that there cannot exist any efficient real-world algorithm with goal G. This would mean that "[if] a human society were highly motivated to design an efficient real-world algorithm with goal G, and were given a million years to do so along with huge amounts of resources, training and knowledge about AI, it must fail."<sup id="cite_ref-armstrong_52-1" class="reference"><a href="#cite_note-armstrong-52">&#91;52&#93;</a></sup> Armstrong notes that this and similar statements "seem extraordinarily strong claims to make".<sup id="cite_ref-armstrong_52-2" class="reference"><a href="#cite_note-armstrong-52">&#91;52&#93;</a></sup>
</p><p>Some dissenters, like <a href="/wiki/Michael_Chorost" title="Michael Chorost">Michael Chorost</a>, argue instead that "by the time [the AI] is in a position to imagine tiling the Earth with solar panels, it'll know that it would be morally wrong to do so."<sup id="cite_ref-chorost_53-0" class="reference"><a href="#cite_note-chorost-53">&#91;53&#93;</a></sup> Chorost argues that "an A.I. will need to desire certain states and dislike others. Today's software lacks that ability—and computer scientists have not a clue how to get it there. Without wanting, there's no impetus to do anything. Today's computers can't even want to keep existing, let alone tile the world in solar panels."<sup id="cite_ref-chorost_53-1" class="reference"><a href="#cite_note-chorost-53">&#91;53&#93;</a></sup>
</p>
<h4><span class="mw-headline" id="Terminological_issues">Terminological issues</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Existential_risk_from_artificial_general_intelligence&amp;action=edit&amp;section=11" title="Edit section: Terminological issues">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>Part of the disagreement about whether a superintelligent machine would behave morally may arise from a terminological difference. Outside of the artificial intelligence field, "intelligence" is often used in a normatively thick manner that connotes moral wisdom or acceptance of agreeable forms of moral reasoning. At an extreme, if morality is part of the definition of intelligence, then by definition a superintelligent machine would behave morally. However, in the field of artificial intelligence research, while "intelligence" has many overlapping definitions, none of them make reference to morality. Instead, almost all current "artificial intelligence" research focuses on creating algorithms that "optimize", in an empirical way, the achievement of an arbitrary goal.<sup id="cite_ref-superintelligence_4-7" class="reference"><a href="#cite_note-superintelligence-4">&#91;4&#93;</a></sup>
</p><p>To avoid anthropomorphism or the baggage of the word "intelligence", an advanced artificial intelligence can be thought of as an impersonal "optimizing process" that strictly takes whatever actions are judged most likely to accomplish its (possibly complicated and implicit) goals.<sup id="cite_ref-superintelligence_4-8" class="reference"><a href="#cite_note-superintelligence-4">&#91;4&#93;</a></sup> Another way of conceptualizing an advanced artificial intelligence is to imagine a time machine that sends backward in time information about which choice always leads to the maximization of its goal function; this choice is then outputted, regardless of any extraneous ethical concerns.<sup id="cite_ref-54" class="reference"><a href="#cite_note-54">&#91;54&#93;</a></sup><sup id="cite_ref-55" class="reference"><a href="#cite_note-55">&#91;55&#93;</a></sup>
</p>
<h4><span class="mw-headline" id="Anthropomorphism">Anthropomorphism</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Existential_risk_from_artificial_general_intelligence&amp;action=edit&amp;section=12" title="Edit section: Anthropomorphism">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>In science fiction, an AI, even though it has not been programmed with human emotions, often spontaneously experiences those emotions anyway: for example, Agent Smith in <a href="/wiki/The_Matrix_(film)" class="mw-redirect" title="The Matrix (film)">The Matrix</a> was influenced by a "disgust" toward humanity. This is fictitious <a href="/wiki/Anthropomorphism" title="Anthropomorphism">anthropomorphism</a>: in reality, while an artificial intelligence could perhaps be deliberately programmed with human emotions, or could develop something similar to an emotion as a means to an ultimate goal <i>if</i> it is useful to do so, it would not spontaneously develop human emotions for no purpose whatsoever, as portrayed in fiction.<sup id="cite_ref-yudkowsky-global-risk_7-3" class="reference"><a href="#cite_note-yudkowsky-global-risk-7">&#91;7&#93;</a></sup>
</p><p>Scholars sometimes claim that others' predictions about an AI's behavior are illogical anthropomorphism.<sup id="cite_ref-yudkowsky-global-risk_7-4" class="reference"><a href="#cite_note-yudkowsky-global-risk-7">&#91;7&#93;</a></sup> An example that might initially be considered anthropomorphism, but is in fact a logical statement about AI behavior, would be the <a href="/wiki/Dario_Floreano" title="Dario Floreano">Dario Floreano</a> experiments where certain robots spontaneously evolved a crude capacity for "deception", and tricked other robots into eating "poison" and dying: here a trait, "deception", ordinarily associated with people rather than with machines, spontaneously evolves in a type of <a href="/wiki/Convergent_evolution" title="Convergent evolution">convergent evolution</a>.<sup id="cite_ref-56" class="reference"><a href="#cite_note-56">&#91;56&#93;</a></sup> According to Paul R. Cohen and <a href="/wiki/Edward_Feigenbaum" title="Edward Feigenbaum">Edward Feigenbaum</a>, in order to differentiate between anthropomorphization and logical prediction of AI behavior, "the trick is to know enough about how humans and computers think to say <i>exactly</i> what they have in common, and, when we lack this knowledge, to use the comparison to <i>suggest</i> theories of human thinking or computer thinking."<sup id="cite_ref-57" class="reference"><a href="#cite_note-57">&#91;57&#93;</a></sup>
</p><p>There is a near-universal assumption in the scientific community that that an advanced AI, even if it were programmed to have, or adopted, human personality dimensions (such as <a href="/wiki/Psychopathy" title="Psychopathy">psychopathy</a>) to make itself more efficient at certain tasks, e.g., <a href="/wiki/Lethal_autonomous_weapon" title="Lethal autonomous weapon">tasks involving killing humans</a>, would not destroy humanity out of human emotions such as "revenge" or "anger." This is because it is assumed that an advanced AI would not be conscious<sup id="cite_ref-58" class="reference"><a href="#cite_note-58">&#91;58&#93;</a></sup> or have testosterone;<sup id="cite_ref-59" class="reference"><a href="#cite_note-59">&#91;59&#93;</a></sup> it ignores the fact that military planners see a conscious superintelligence as the 'holy grail' of interstate warfare.<sup id="cite_ref-:0_60-0" class="reference"><a href="#cite_note-:0-60">&#91;60&#93;</a></sup> The academic debate is, instead, between one side which worries whether AI might destroy humanity as an incidental action in the course of progressing towards its ultimate goals; and another side which believes that AI would not destroy humanity at all. Some skeptics accuse proponents of anthropomorphism for believing an AGI would naturally desire power; proponents accuse some skeptics of anthropomorphism for believing an AGI would naturally value human ethical norms.<sup id="cite_ref-yudkowsky-global-risk_7-5" class="reference"><a href="#cite_note-yudkowsky-global-risk-7">&#91;7&#93;</a></sup><sup id="cite_ref-61" class="reference"><a href="#cite_note-61">&#91;61&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Other_sources_of_risk">Other sources of risk</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Existential_risk_from_artificial_general_intelligence&amp;action=edit&amp;section=13" title="Edit section: Other sources of risk">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<h4><span class="mw-headline" id="Competition">Competition</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Existential_risk_from_artificial_general_intelligence&amp;action=edit&amp;section=14" title="Edit section: Competition">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>In 2014 philosopher <a href="/wiki/Nick_Bostrom" title="Nick Bostrom">Nick Bostrom</a> stated that a "severe race dynamic" (extreme <a href="/wiki/Competition" title="Competition">competition</a>) between different teams may create conditions whereby the creation of an AGI results in shortcuts to safety and potentially violent conflict.<sup id="cite_ref-:2_62-0" class="reference"><a href="#cite_note-:2-62">&#91;62&#93;</a></sup> To address this risk, citing previous scientific collaboration (<a href="/wiki/CERN" title="CERN">CERN</a>, the <a href="/wiki/Human_Genome_Project" title="Human Genome Project">Human Genome Project</a>, and the <a href="/wiki/International_Space_Station" title="International Space Station">International Space Station</a>), Bostrom recommended  <a href="/wiki/Collaboration" title="Collaboration">collaboration</a> and the altruistic global adoption of a <a href="/wiki/Common_good" title="Common good">common good</a> principle: "Superintelligence should be developed only for the benefit of all of humanity and in the service of widely shared ethical ideals".<sup id="cite_ref-:2_62-1" class="reference"><a href="#cite_note-:2-62">&#91;62&#93;</a></sup><sup>:254</sup> Bostrom theorized that collaboration on creating an artificial general intelligence would offer multiple benefits, including reducing haste, thereby increasing investment in safety; avoiding violent conflicts (wars), facilitating sharing solutions to the control problem, and more equitably distributing the benefits.<sup id="cite_ref-:2_62-2" class="reference"><a href="#cite_note-:2-62">&#91;62&#93;</a></sup><sup>:253</sup> The United States' <a href="/wiki/BRAIN_Initiative" title="BRAIN Initiative">Brain Initiative</a> was launched in 2014, as was the European Union's <a href="/wiki/Human_Brain_Project" title="Human Brain Project">Human Brain Project</a>; China's <a href="/wiki/China_Brain_Project" title="China Brain Project">Brain Project</a> was launched in 2016.
</p>
<h4><span class="mw-headline" id="Weaponization_of_artificial_intelligence">Weaponization of artificial intelligence</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Existential_risk_from_artificial_general_intelligence&amp;action=edit&amp;section=15" title="Edit section: Weaponization of artificial intelligence">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>Some sources argue that the ongoing <a href="/wiki/Weaponization_of_artificial_intelligence" class="mw-redirect" title="Weaponization of artificial intelligence">weaponization of artificial intelligence</a> could constitute a catastrophic risk.<sup id="cite_ref-Cave_2018_63-0" class="reference"><a href="#cite_note-Cave_2018-63">&#91;63&#93;</a></sup><sup id="cite_ref-:4_64-0" class="reference"><a href="#cite_note-:4-64">&#91;64&#93;</a></sup> The risk is actually threefold, with the first risk potentially having geopolitical implications, and the second two definitely having geopolitical implications:
</p>
<table class="cquote pullquote" role="presentation" style="margin:auto; border-collapse: collapse; border: none; background-color: transparent; width: auto;">
<tbody><tr>
<td style="width: 20px; vertical-align: top; border: none; color: #B2B7F2; font-size: 40px; font-family: &#39;Times New Roman&#39;, Times, serif; font-weight: bold; line-height: .6em; text-align: left; padding: 10px 10px;">“
</td>
<td style="vertical-align: top; border: none; padding: 4px 10px;">i) The dangers of an AI ‘race for technological advantage’ framing, regardless of whether the race is seriously pursued;
<p>ii) The dangers of an AI ‘race for technological advantage’ framing and an actual AI race for technological advantage, regardless of whether the race is won;
</p><p>iii) The dangers of an AI race for technological advantage being won.<sup id="cite_ref-Cave_2018_63-1" class="reference"><a href="#cite_note-Cave_2018-63">&#91;63&#93;</a></sup><sup>:37</sup>
</p>
</td>
<td style="width: 20px; vertical-align: bottom; border: none; color: #B2B7F2; font-size: 40px; font-family: &#39;Times New Roman&#39;, Times, serif; font-weight: bold; line-height: .6em; text-align: right; padding: 10px 10px;">”
</td></tr>

</tbody></table>
<p>A weaponized conscious superintelligence would affect current US military technological supremacy and transform warfare; it is therefore highly desirable for strategic military planning and interstate warfare.<sup id="cite_ref-:0_60-1" class="reference"><a href="#cite_note-:0-60">&#91;60&#93;</a></sup><sup id="cite_ref-:4_64-1" class="reference"><a href="#cite_note-:4-64">&#91;64&#93;</a></sup> The China State Council's 2017 “A Next Generation Artificial Intelligence Development Plan” views AI in geopolitically strategic terms and is pursuing a 'military-civil fusion' strategy to build on China's first-mover advantage in the development of AI in order to establish technological supremacy by 2030,<sup id="cite_ref-65" class="reference"><a href="#cite_note-65">&#91;65&#93;</a></sup> while Russia's President Vladimir Putin has stated that “whoever becomes the leader in this sphere will become the ruler of the world”.<sup id="cite_ref-:1_66-0" class="reference"><a href="#cite_note-:1-66">&#91;66&#93;</a></sup> James Barrat, documentary filmmaker and author of <i><a href="/wiki/Our_Final_Invention" title="Our Final Invention">Our Final Invention</a></i>, says in a <a href="/wiki/Smithsonian_(magazine)" title="Smithsonian (magazine)">Smithsonian</a> interview, "Imagine: in as little as a decade, a half-dozen companies and nations field computers that rival or surpass human intelligence. Imagine what happens when those computers become expert at programming smart computers. Soon we'll be sharing the planet with machines thousands or millions of times more intelligent than we are. And, all the while, each generation of this technology will be weaponized. Unregulated, it will be catastrophic."<sup id="cite_ref-67" class="reference"><a href="#cite_note-67">&#91;67&#93;</a></sup>
</p>
<h4><span class="mw-headline" id="Malevolent_AGI_by_design">Malevolent AGI by design</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Existential_risk_from_artificial_general_intelligence&amp;action=edit&amp;section=16" title="Edit section: Malevolent AGI by design">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>It is theorized that malevolent AGI by could be created by design, for example by a military, a government, a sociopath, or a corporation, to benefit from, control, or subjugate certain groups of people, as in <a href="/wiki/Cybercrime" title="Cybercrime">cybercrime</a>.<sup id="cite_ref-68" class="reference"><a href="#cite_note-68">&#91;68&#93;</a></sup><sup id="cite_ref-69" class="reference"><a href="#cite_note-69">&#91;69&#93;</a></sup><sup>:166</sup> Alternatively, malevolent AGI ('evil AI') could choose the goal of increasing human suffering, for example of those people who did not assist it during the information explosion phase.<sup id="cite_ref-70" class="reference"><a href="#cite_note-70">&#91;70&#93;</a></sup><sup>:158</sup>
</p>
<h4><span id="Preemptive_nuclear_strike_.28nuclear_war.29"></span><span class="mw-headline" id="Preemptive_nuclear_strike_(nuclear_war)">Preemptive nuclear strike (nuclear war)</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Existential_risk_from_artificial_general_intelligence&amp;action=edit&amp;section=17" title="Edit section: Preemptive nuclear strike (nuclear war)">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>It is theorized that a country being close to achieving AGI technological supremacy could trigger a <a href="/wiki/Pre-emptive_nuclear_strike" title="Pre-emptive nuclear strike">pre-emptive nuclear strike</a> from a rival, leading to a <a href="/wiki/Nuclear_warfare" title="Nuclear warfare">nuclear war</a>.<sup id="cite_ref-:4_64-2" class="reference"><a href="#cite_note-:4-64">&#91;64&#93;</a></sup><sup id="cite_ref-71" class="reference"><a href="#cite_note-71">&#91;71&#93;</a></sup>
</p>
<h2><span class="mw-headline" id="Timeframe">Timeframe</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Existential_risk_from_artificial_general_intelligence&amp;action=edit&amp;section=18" title="Edit section: Timeframe">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div role="note" class="hatnote navigation-not-searchable">Main article: <a href="/wiki/Artificial_general_intelligence#Feasibility" title="Artificial general intelligence">Artificial general intelligence §&#160;Feasibility</a></div>
<p>Opinions vary both on <i>whether</i> and <i>when</i> artificial general intelligence will arrive. At one extreme, AI pioneer <a href="/wiki/Herbert_A._Simon" title="Herbert A. Simon">Herbert A. Simon</a> wrote in 1965: "machines will be capable, within twenty years, of doing any work a man can do"; obviously this prediction failed to come true.<sup id="cite_ref-72" class="reference"><a href="#cite_note-72">&#91;72&#93;</a></sup> At the other extreme, roboticist Alan Winfield claims the gulf between modern computing and human-level artificial intelligence is as wide as the gulf between current space flight and practical, faster than light spaceflight.<sup id="cite_ref-73" class="reference"><a href="#cite_note-73">&#91;73&#93;</a></sup> Optimism that AGI is feasible waxes and wanes, and may have seen a resurgence in the 2010s. Four polls conducted in 2012 and 2013 suggested that the median guess among experts for when AGI would arrive was 2040 to 2050, depending on the poll.<sup id="cite_ref-new_yorker_doomsday_74-0" class="reference"><a href="#cite_note-new_yorker_doomsday-74">&#91;74&#93;</a></sup><sup id="cite_ref-75" class="reference"><a href="#cite_note-75">&#91;75&#93;</a></sup>
</p><p>Skeptics who believe it is impossible for AGI to arrive anytime soon, tend to argue that expressing concern about existential risk from AI is unhelpful because it could distract people from more immediate concerns about the impact of AGI, because of fears it could lead to government regulation or make it more difficult to secure funding for AI research, or because it could give AI research a bad reputation. Some researchers, such as Oren Etzioni, aggressively seek to quell concern over existential risk from AI, saying "[Elon Musk] has impugned us in very strong language saying we are unleashing the demon, and so we're answering."<sup id="cite_ref-76" class="reference"><a href="#cite_note-76">&#91;76&#93;</a></sup>
</p><p>In 2014 <a href="/wiki/Slate_(magazine)" title="Slate (magazine)">Slate</a>'s Adam Elkus argued "our 'smartest' AI is about as intelligent as a toddler—and only when it comes to instrumental tasks like information recall. Most roboticists are still trying to get a robot hand to pick up a ball or run around without falling over." Elkus goes on to argue that Musk's "summoning the demon" analogy may be harmful because it could result in "harsh cuts" to AI research budgets.<sup id="cite_ref-77" class="reference"><a href="#cite_note-77">&#91;77&#93;</a></sup>
</p><p>The <a href="/wiki/Information_Technology_and_Innovation_Foundation" title="Information Technology and Innovation Foundation">Information Technology and Innovation Foundation</a> (ITIF), a Washington, D.C. think-tank, awarded its Annual Luddite Award to "alarmists touting an artificial intelligence apocalypse"; its president, <a href="/wiki/Robert_D._Atkinson" title="Robert D. Atkinson">Robert D. Atkinson</a>, complained that Musk, Hawking and AI experts say AI is the largest existential threat to humanity. Atkinson stated "That's not a very winning message if you want to get AI funding out of Congress to the National Science Foundation."<sup id="cite_ref-78" class="reference"><a href="#cite_note-78">&#91;78&#93;</a></sup><sup id="cite_ref-79" class="reference"><a href="#cite_note-79">&#91;79&#93;</a></sup><sup id="cite_ref-80" class="reference"><a href="#cite_note-80">&#91;80&#93;</a></sup> <i><a href="/wiki/Nature_(journal)" title="Nature (journal)">Nature</a></i> sharply disagreed with the ITIF in an April 2016 editorial, siding instead with Musk, Hawking, and Russell, and concluding: "It is crucial that progress in technology is matched by solid, well-funded research to anticipate the scenarios it could bring about<span class="nowrap">&#160;</span>... If that is a Luddite perspective, then so be it."<sup id="cite_ref-nature_anticipating_81-0" class="reference"><a href="#cite_note-nature_anticipating-81">&#91;81&#93;</a></sup> In a 2015 <i><a href="/wiki/Washington_Post" class="mw-redirect" title="Washington Post">Washington Post</a></i> editorial, researcher <a href="/wiki/Murray_Shanahan" title="Murray Shanahan">Murray Shanahan</a> stated that human-level AI is unlikely to arrive "anytime soon", but that nevertheless "the time to start thinking through the consequences is now."<sup id="cite_ref-82" class="reference"><a href="#cite_note-82">&#91;82&#93;</a></sup>
</p>
<h2><span class="mw-headline" id="Perspectives">Perspectives</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Existential_risk_from_artificial_general_intelligence&amp;action=edit&amp;section=19" title="Edit section: Perspectives">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>The thesis that AI could pose an existential risk provokes a wide range of reactions within the scientific community, as well as in the public at large. Many of the opposing viewpoints, however, share common ground.
</p><p>The Asilomar AI Principles, which contain only the principles agreed to by 90% of the attendees of the <a href="/wiki/Future_of_Life_Institute" title="Future of Life Institute">Future of Life Institute</a>'s Beneficial AI 2017 conference,<sup id="cite_ref-life_3.0_37-1" class="reference"><a href="#cite_note-life_3.0-37">&#91;37&#93;</a></sup> agree in principle that "There being no consensus, we should avoid strong assumptions regarding upper limits on future AI capabilities" and "Advanced AI could represent a profound change in the history of life on Earth, and should be planned for and managed with commensurate care and resources."<sup id="cite_ref-83" class="reference"><a href="#cite_note-83">&#91;83&#93;</a></sup><sup id="cite_ref-84" class="reference"><a href="#cite_note-84">&#91;84&#93;</a></sup> AI safety advocates such as Bostrom and Tegmark have criticized the mainstream media's use of "those inane <i><a href="/wiki/Terminator_(franchise)" title="Terminator (franchise)">Terminator</a></i> pictures" to illustrate AI safety concerns: "It can't be much fun to have aspersions cast on one's academic discipline, one's professional community, one's life work<span class="nowrap">&#160;</span>... I call on all sides to practice patience and restraint, and to engage in direct dialogue and collaboration as much as possible."<sup id="cite_ref-life_3.0_37-2" class="reference"><a href="#cite_note-life_3.0-37">&#91;37&#93;</a></sup><sup id="cite_ref-85" class="reference"><a href="#cite_note-85">&#91;85&#93;</a></sup>
</p><p>Conversely, many skeptics agree that ongoing research into the implications of artificial general intelligence is valuable. Skeptic <a href="/wiki/Martin_Ford_(author)" title="Martin Ford (author)">Martin Ford</a> states that "I think it seems wise to apply something like <a href="/wiki/Dick_Cheney" title="Dick Cheney">Dick Cheney</a>'s famous '1 Percent Doctrine' to the specter of advanced artificial intelligence: the odds of its occurrence, at least in the foreseeable future, may be very low — but the implications are so dramatic that it should be taken seriously";<sup id="cite_ref-86" class="reference"><a href="#cite_note-86">&#91;86&#93;</a></sup> similarly, an otherwise skeptical <i><a href="/wiki/The_Economist" title="The Economist">Economist</a></i> stated in 2014 that "the implications of introducing a second intelligent species onto Earth are far-reaching enough to deserve hard thinking, even if the prospect seems remote".<sup id="cite_ref-economist_review_30-2" class="reference"><a href="#cite_note-economist_review-30">&#91;30&#93;</a></sup>
</p><p>A 2017 email survey of researchers with publications at the 2015 <a href="/wiki/Conference_on_Neural_Information_Processing_Systems" title="Conference on Neural Information Processing Systems">NIPS</a> and <a href="/wiki/International_Conference_on_Machine_Learning" title="International Conference on Machine Learning">ICML</a> machine learning conferences asked them to evaluate <a href="/wiki/Stuart_J._Russell" title="Stuart J. Russell">Stuart J. Russell</a>'s concerns about AI risk. Of the respondents, 5% said it was "among the most important problems in the field", 34% said it was "an important problem", and 31% said it was "moderately important", whilst 19% said it was "not important" and 11% said it was "not a real problem" at all.<sup id="cite_ref-87" class="reference"><a href="#cite_note-87">&#91;87&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Endorsement">Endorsement</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Existential_risk_from_artificial_general_intelligence&amp;action=edit&amp;section=20" title="Edit section: Endorsement">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<div class="thumb tright"><div class="thumbinner" style="width:222px;"><a href="/wiki/File:Bill_Gates_June_2015.jpg" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/thumb/1/19/Bill_Gates_June_2015.jpg/220px-Bill_Gates_June_2015.jpg" decoding="async" width="220" height="311" class="thumbimage" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/1/19/Bill_Gates_June_2015.jpg/330px-Bill_Gates_June_2015.jpg 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/1/19/Bill_Gates_June_2015.jpg/440px-Bill_Gates_June_2015.jpg 2x" data-file-width="996" data-file-height="1408" /></a>  <div class="thumbcaption"><div class="magnify"><a href="/wiki/File:Bill_Gates_June_2015.jpg" class="internal" title="Enlarge"></a></div>Bill Gates has stated "I<span class="nowrap">&#160;</span>... don't understand why some people are not concerned."<sup id="cite_ref-BBC_News_88-0" class="reference"><a href="#cite_note-BBC_News-88">&#91;88&#93;</a></sup></div></div></div>
<div role="note" class="hatnote navigation-not-searchable">Further information: <a href="/wiki/Existential_risk" class="mw-redirect" title="Existential risk">Existential risk</a></div>
<p>The thesis that AI poses an existential risk, and that this risk needs much more attention than it currently gets, has been endorsed by many public figures; perhaps the most famous are <a href="/wiki/Elon_Musk" title="Elon Musk">Elon Musk</a>, <a href="/wiki/Bill_Gates" title="Bill Gates">Bill Gates</a>, and <a href="/wiki/Stephen_Hawking" title="Stephen Hawking">Stephen Hawking</a>. The most notable AI researchers to endorse the thesis are Russell and <a href="/wiki/I._J._Good" title="I. J. Good">I.J. Good</a>, who advised <a href="/wiki/Stanley_Kubrick" title="Stanley Kubrick">Stanley Kubrick</a> on the filming of <i><a href="/wiki/2001:_A_Space_Odyssey" title="2001: A Space Odyssey">2001: A Space Odyssey</a></i>. Endorsers of the thesis sometimes express bafflement at skeptics: Gates states that he does not "understand why some people are not concerned",<sup id="cite_ref-BBC_News_88-1" class="reference"><a href="#cite_note-BBC_News-88">&#91;88&#93;</a></sup> and Hawking criticized widespread indifference in his 2014 editorial: 
</p>
<table class="cquote pullquote" role="presentation" style="margin:auto; border-collapse: collapse; border: none; background-color: transparent; width: auto;">
<tbody><tr>
<td style="width: 20px; vertical-align: top; border: none; color: #B2B7F2; font-size: 40px; font-family: &#39;Times New Roman&#39;, Times, serif; font-weight: bold; line-height: .6em; text-align: left; padding: 10px 10px;">“
</td>
<td style="vertical-align: top; border: none; padding: 4px 10px;">'So, facing possible futures of incalculable benefits and risks, the experts are surely doing everything possible to ensure the best outcome, right? Wrong. If a superior alien civilisation sent us a message saying, 'We'll arrive in a few decades,' would we just reply, 'OK, call us when you get here–we'll leave the lights on?' Probably not–but this is more or less what is happening with AI.'<sup id="cite_ref-hawking_editorial_21-3" class="reference"><a href="#cite_note-hawking_editorial-21">&#91;21&#93;</a></sup>
</td>
<td style="width: 20px; vertical-align: bottom; border: none; color: #B2B7F2; font-size: 40px; font-family: &#39;Times New Roman&#39;, Times, serif; font-weight: bold; line-height: .6em; text-align: right; padding: 10px 10px;">”
</td></tr>

</tbody></table>
<p>Many of the scholars who are concerned about existential risk believe that the best way forward would be to conduct (possibly massive) research into solving the difficult "control problem" to answer the question: what types of safeguards, algorithms, or architectures can programmers implement to maximize the probability that their recursively-improving AI would continue to behave in a friendly, rather than destructive, manner after it reaches superintelligence?<sup id="cite_ref-superintelligence_4-9" class="reference"><a href="#cite_note-superintelligence-4">&#91;4&#93;</a></sup><sup id="cite_ref-physica_scripta_89-0" class="reference"><a href="#cite_note-physica_scripta-89">&#91;89&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Skepticism">Skepticism</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Existential_risk_from_artificial_general_intelligence&amp;action=edit&amp;section=21" title="Edit section: Skepticism">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<div role="note" class="hatnote navigation-not-searchable">Further information: <a href="/wiki/Artificial_general_intelligence#Feasibility" title="Artificial general intelligence">Artificial general intelligence §&#160;Feasibility</a></div>
<p>The thesis that AI can pose existential risk also has many strong detractors. Skeptics sometimes charge that the thesis is crypto-religious, with an irrational belief in the possibility of superintelligence replacing an irrational belief in an omnipotent God; at an extreme, <a href="/wiki/Jaron_Lanier" title="Jaron Lanier">Jaron Lanier</a> argues that the whole concept that current machines are in any way intelligent is "an illusion" and a "stupendous con" by the wealthy.<sup id="cite_ref-atlantic-but-what_90-0" class="reference"><a href="#cite_note-atlantic-but-what-90">&#91;90&#93;</a></sup>
</p><p>Much of existing criticism argues that AGI is unlikely in the short term. Computer scientist <a href="/wiki/Gordon_Bell" title="Gordon Bell">Gordon Bell</a> argues that the human race will already destroy itself before it reaches the technological singularity. <a href="/wiki/Gordon_Moore" title="Gordon Moore">Gordon Moore</a>, the original proponent of <a href="/wiki/Moore%27s_Law" class="mw-redirect" title="Moore&#39;s Law">Moore's Law</a>, declares that "I am a skeptic. I don't believe [a technological singularity] is likely to happen, at least for a long time. And I don't know why I feel that way."<sup id="cite_ref-91" class="reference"><a href="#cite_note-91">&#91;91&#93;</a></sup> <a href="/wiki/Baidu" title="Baidu">Baidu</a> Vice President <a href="/wiki/Andrew_Ng" title="Andrew Ng">Andrew Ng</a> states AI existential risk is "like worrying about overpopulation on Mars when we have not even set foot on the planet yet."<sup id="cite_ref-shermer_48-1" class="reference"><a href="#cite_note-shermer-48">&#91;48&#93;</a></sup>
</p><p>Some AI and AGI researchers may be reluctant to discuss risks, worrying that policymakers do not have sophisticated knowledge of the field and are prone to be convinced by "alarmist" messages, or worrying that such messages will lead to cuts in AI funding. <i>Slate</i> notes that some researchers are dependent on grants from government agencies such as <a href="/wiki/DARPA" title="DARPA">DARPA</a>.<sup id="cite_ref-slate_killer_24-1" class="reference"><a href="#cite_note-slate_killer-24">&#91;24&#93;</a></sup>
</p><p>At some point in an intelligence explosion driven by a single AI, the AI would have to become vastly better at software innovation than the best innovators of the rest of the world; economist <a href="/wiki/Robin_Hanson" title="Robin Hanson">Robin Hanson</a> is skeptical that this is possible.<sup id="cite_ref-92" class="reference"><a href="#cite_note-92">&#91;92&#93;</a></sup><sup id="cite_ref-93" class="reference"><a href="#cite_note-93">&#91;93&#93;</a></sup><sup id="cite_ref-94" class="reference"><a href="#cite_note-94">&#91;94&#93;</a></sup><sup id="cite_ref-95" class="reference"><a href="#cite_note-95">&#91;95&#93;</a></sup><sup id="cite_ref-96" class="reference"><a href="#cite_note-96">&#91;96&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Intermediate_views">Intermediate views</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Existential_risk_from_artificial_general_intelligence&amp;action=edit&amp;section=22" title="Edit section: Intermediate views">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Intermediate views generally take the position that the control problem of artificial general intelligence may exist, but that it will be solved via progress in artificial intelligence, for example by creating a moral learning environment for the AI, taking care to spot clumsy malevolent behavior (the 'sordid stumble')<sup id="cite_ref-97" class="reference"><a href="#cite_note-97">&#91;97&#93;</a></sup> and then directly intervening in the code before the AI refines its behavior, or even peer pressure from <a href="/wiki/Friendly_artificial_intelligence" title="Friendly artificial intelligence">friendly AIs</a>.<sup id="cite_ref-98" class="reference"><a href="#cite_note-98">&#91;98&#93;</a></sup> In a 2015 <i><a href="/wiki/Wall_Street_Journal" class="mw-redirect" title="Wall Street Journal">Wall Street Journal</a></i> panel discussion devoted to AI risks, <a href="/wiki/IBM" title="IBM">IBM</a>'s Vice-President of Cognitive Computing, Guruduth S. Banavar, brushed off discussion of AGI with the phrase, "it is anybody's speculation."<sup id="cite_ref-99" class="reference"><a href="#cite_note-99">&#91;99&#93;</a></sup> <a href="/wiki/Geoffrey_Hinton" title="Geoffrey Hinton">Geoffrey Hinton</a>, the "godfather of deep learning", noted that "there is not a good track record of less intelligent things controlling things of greater intelligence", but stated that he continues his research because "the prospect of discovery is too <i>sweet</i>".<sup id="cite_ref-slate_killer_24-2" class="reference"><a href="#cite_note-slate_killer-24">&#91;24&#93;</a></sup><sup id="cite_ref-new_yorker_doomsday_74-1" class="reference"><a href="#cite_note-new_yorker_doomsday-74">&#91;74&#93;</a></sup> In 2004, law professor <a href="/wiki/Richard_Posner" title="Richard Posner">Richard Posner</a> wrote that dedicated efforts for addressing AI can wait, but that we should gather more information about the problem in the meanwhile.<sup id="cite_ref-100" class="reference"><a href="#cite_note-100">&#91;100&#93;</a></sup><sup id="cite_ref-physica_scripta_89-1" class="reference"><a href="#cite_note-physica_scripta-89">&#91;89&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Popular_reaction">Popular reaction</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Existential_risk_from_artificial_general_intelligence&amp;action=edit&amp;section=23" title="Edit section: Popular reaction">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>In a 2014 article in <i><a href="/wiki/The_Atlantic_(magazine)" class="mw-redirect" title="The Atlantic (magazine)">The Atlantic</a></i>, James Hamblin noted that most people do not care one way or the other about artificial general intelligence, and characterized his own gut reaction to the topic as: "Get out of here. I have a hundred thousand things I am concerned about at this exact moment. Do I seriously need to add to that a technological singularity?"<sup id="cite_ref-atlantic-but-what_90-1" class="reference"><a href="#cite_note-atlantic-but-what-90">&#91;90&#93;</a></sup>
</p><p>During a 2016 <a href="/wiki/Wired_(magazine)" title="Wired (magazine)"><i>Wired</i></a> interview of President <a href="/wiki/Barack_Obama" title="Barack Obama">Barack Obama</a> and MIT Media Lab's <a href="/wiki/Joi_Ito" title="Joi Ito">Joi Ito</a>, Ito stated: 
</p>
<table class="cquote pullquote" role="presentation" style="margin:auto; border-collapse: collapse; border: none; background-color: transparent; width: auto;">
<tbody><tr>
<td style="width: 20px; vertical-align: top; border: none; color: #B2B7F2; font-size: 40px; font-family: &#39;Times New Roman&#39;, Times, serif; font-weight: bold; line-height: .6em; text-align: left; padding: 10px 10px;">“
</td>
<td style="vertical-align: top; border: none; padding: 4px 10px;">There are a few people who believe that there is a fairly high-percentage chance that a generalized AI will happen in the next 10 years. But the way I look at it is that in order for that to happen, we're going to need a dozen or two different breakthroughs. So you can monitor when you think these breakthroughs will happen.
</td>
<td style="width: 20px; vertical-align: bottom; border: none; color: #B2B7F2; font-size: 40px; font-family: &#39;Times New Roman&#39;, Times, serif; font-weight: bold; line-height: .6em; text-align: right; padding: 10px 10px;">”
</td></tr>

</tbody></table><p> Obama added:<sup id="cite_ref-101" class="reference"><a href="#cite_note-101">&#91;101&#93;</a></sup><sup id="cite_ref-102" class="reference"><a href="#cite_note-102">&#91;102&#93;</a></sup>
</p><table class="cquote pullquote" role="presentation" style="margin:auto; border-collapse: collapse; border: none; background-color: transparent; width: auto;">
<tbody><tr>
<td style="width: 20px; vertical-align: top; border: none; color: #B2B7F2; font-size: 40px; font-family: &#39;Times New Roman&#39;, Times, serif; font-weight: bold; line-height: .6em; text-align: left; padding: 10px 10px;">“
</td>
<td style="vertical-align: top; border: none; padding: 4px 10px;">And you just have to have somebody close to the power cord. [Laughs.] Right when you see it about to happen, you gotta yank that electricity out of the wall, man.
</td>
<td style="width: 20px; vertical-align: bottom; border: none; color: #B2B7F2; font-size: 40px; font-family: &#39;Times New Roman&#39;, Times, serif; font-weight: bold; line-height: .6em; text-align: right; padding: 10px 10px;">”
</td></tr>

</tbody></table>
<p><a href="/wiki/Hillary_Clinton" title="Hillary Clinton">Hillary Clinton</a> stated in <i>"<a href="/wiki/What_Happened_(Clinton_book)" title="What Happened (Clinton book)">What Happened</a>"</i>:
</p>
<table class="cquote pullquote" role="presentation" style="margin:auto; border-collapse: collapse; border: none; background-color: transparent; width: auto;">
<tbody><tr>
<td style="width: 20px; vertical-align: top; border: none; color: #B2B7F2; font-size: 40px; font-family: &#39;Times New Roman&#39;, Times, serif; font-weight: bold; line-height: .6em; text-align: left; padding: 10px 10px;">“
</td>
<td style="vertical-align: top; border: none; padding: 4px 10px;">Technologists... have warned that artificial intelligence could one day pose an existential security threat. Musk has called it "the greatest risk we face as a civilization". Think about it: Have you ever seen a movie where the machines start thinking for themselves that ends well? Every time I went out to Silicon Valley during the campaign, I came home more alarmed about this. My staff lived in fear that I’d start talking about "the rise of the robots" in some Iowa town hall. Maybe I should have. In any case, policy makers need to keep up with technology as it races ahead, instead of always playing catch-up.<sup id="cite_ref-103" class="reference"><a href="#cite_note-103">&#91;103&#93;</a></sup>
</td>
<td style="width: 20px; vertical-align: bottom; border: none; color: #B2B7F2; font-size: 40px; font-family: &#39;Times New Roman&#39;, Times, serif; font-weight: bold; line-height: .6em; text-align: right; padding: 10px 10px;">”
</td></tr>

</tbody></table>
<p>In a <a href="/wiki/YouGov" title="YouGov">YouGov</a> poll of the public for the <a href="/wiki/British_Science_Association" title="British Science Association">British Science Association</a>, about a third of survey respondents said AI will pose a threat to the long term survival of humanity.<sup id="cite_ref-bsa_poll_104-0" class="reference"><a href="#cite_note-bsa_poll-104">&#91;104&#93;</a></sup> Referencing a poll of its readers, Slate's Jacob Brogan stated that "most of the (readers filling out our online survey) were unconvinced that A.I. itself presents a direct threat."<sup id="cite_ref-:3_105-0" class="reference"><a href="#cite_note-:3-105">&#91;105&#93;</a></sup>
</p><p>In 2018, a <a href="/wiki/SurveyMonkey" title="SurveyMonkey">SurveyMonkey</a> poll of the American public by <a href="/wiki/USA_Today" title="USA Today">USA Today</a> found 68% thought the real current threat remains "human intelligence"; however, the poll also found that 43% said superintelligent AI, if it were to happen, would result in "more harm than good", and 38% said it would do "equal amounts of harm and good".<sup id="cite_ref-:3_105-1" class="reference"><a href="#cite_note-:3-105">&#91;105&#93;</a></sup>
</p><p>One <a href="/wiki/Technological_utopianism" title="Technological utopianism">techno-utopia</a>n viewpoint expressed in some popular fiction is that AGI may tend towards peace-building.<sup id="cite_ref-106" class="reference"><a href="#cite_note-106">&#91;106&#93;</a></sup>
</p>
<h2><span class="mw-headline" id="Mitigation">Mitigation</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Existential_risk_from_artificial_general_intelligence&amp;action=edit&amp;section=24" title="Edit section: Mitigation">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div role="note" class="hatnote navigation-not-searchable">See also: <a href="/wiki/AI_control_problem" title="AI control problem">AI control problem</a></div>
<p>Researchers at Google have proposed research into general "AI safety" issues to simultaneously mitigate both short-term risks from narrow AI and long-term risks from AGI.<sup id="cite_ref-107" class="reference"><a href="#cite_note-107">&#91;107&#93;</a></sup><sup id="cite_ref-108" class="reference"><a href="#cite_note-108">&#91;108&#93;</a></sup> A 2020 estimate places global spending on AI existential risk somewhere between $10 and $50 million, compared with global spending on AI around perhaps $40 billion. Bostrom suggests a general principle of "differential technological development", that funders should consider working to speed up the development of protective technologies relative to the development of dangerous ones.<sup id="cite_ref-109" class="reference"><a href="#cite_note-109">&#91;109&#93;</a></sup> Some funders, such as <a href="/wiki/Elon_Musk" title="Elon Musk">Elon Musk</a>, propose that radical <a href="/wiki/Human_enhancement" title="Human enhancement">human cognitive enhancement</a> could be such a technology, for example through direct neural linking between man and machine; however, others argue that enhancement technologies may themselves pose an existential risk.<sup id="cite_ref-110" class="reference"><a href="#cite_note-110">&#91;110&#93;</a></sup><sup id="cite_ref-111" class="reference"><a href="#cite_note-111">&#91;111&#93;</a></sup> Researchers, if they are not caught off-guard, could closely monitor or attempt to <a href="/wiki/AI_control_problem#Capability_control" title="AI control problem">box in</a> an initial AI at a risk of becoming too powerful, as an attempt at a stop-gap measure. A dominant superintelligent AI, if it were aligned with human interests, might itself take action to mitigate the risk of takeover by rival AI, although the creation of the dominant AI could itself pose an existential risk.<sup id="cite_ref-112" class="reference"><a href="#cite_note-112">&#91;112&#93;</a></sup>
</p><p>Institutions such as the <a href="/wiki/Machine_Intelligence_Research_Institute" title="Machine Intelligence Research Institute">Machine Intelligence Research Institute</a>, the <a href="/wiki/Future_of_Humanity_Institute" title="Future of Humanity Institute">Future of Humanity Institute</a>,<sup id="cite_ref-113" class="reference"><a href="#cite_note-113">&#91;113&#93;</a></sup><sup id="cite_ref-114" class="reference"><a href="#cite_note-114">&#91;114&#93;</a></sup> the <a href="/wiki/Future_of_Life_Institute" title="Future of Life Institute">Future of Life Institute</a>, the <a href="/wiki/Centre_for_the_Study_of_Existential_Risk" title="Centre for the Study of Existential Risk">Centre for the Study of Existential Risk</a>, and the <a href="/wiki/Center_for_Human-Compatible_AI" class="mw-redirect" title="Center for Human-Compatible AI">Center for Human-Compatible AI</a><sup id="cite_ref-115" class="reference"><a href="#cite_note-115">&#91;115&#93;</a></sup> are involved in mitigating existential risk from advanced artificial intelligence, for example by research into <a href="/wiki/Friendly_artificial_intelligence" title="Friendly artificial intelligence">friendly artificial intelligence</a>.<sup id="cite_ref-givewell_5-2" class="reference"><a href="#cite_note-givewell-5">&#91;5&#93;</a></sup><sup id="cite_ref-atlantic-but-what_90-2" class="reference"><a href="#cite_note-atlantic-but-what-90">&#91;90&#93;</a></sup><sup id="cite_ref-hawking_editorial_21-4" class="reference"><a href="#cite_note-hawking_editorial-21">&#91;21&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Views_on_banning_and_regulation">Views on banning and regulation</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Existential_risk_from_artificial_general_intelligence&amp;action=edit&amp;section=25" title="Edit section: Views on banning and regulation">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<h4><span class="mw-headline" id="Banning">Banning</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Existential_risk_from_artificial_general_intelligence&amp;action=edit&amp;section=26" title="Edit section: Banning">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>There is nearly universal agreement that attempting to ban research into artificial intelligence would be unwise, and probably futile.<sup id="cite_ref-116" class="reference"><a href="#cite_note-116">&#91;116&#93;</a></sup><sup id="cite_ref-117" class="reference"><a href="#cite_note-117">&#91;117&#93;</a></sup><sup id="cite_ref-118" class="reference"><a href="#cite_note-118">&#91;118&#93;</a></sup> Skeptics argue that regulation of AI would be completely valueless, as no existential risk exists. Almost all of the scholars who believe existential risk exists agree with the skeptics that banning research would be unwise, as research could be moved to countries with looser regulations or conducted covertly. The latter issue is particularly relevant, as artificial intelligence research can be done on a small scale without substantial infrastructure or resources.<sup id="cite_ref-mcginnis_119-0" class="reference"><a href="#cite_note-mcginnis-119">&#91;119&#93;</a></sup><sup id="cite_ref-120" class="reference"><a href="#cite_note-120">&#91;120&#93;</a></sup> Two additional hypothetical difficulties with bans (or other regulation) are that technology entrepreneurs statistically tend towards general skepticism about government regulation, and that businesses could have a strong incentive to (and might well succeed at) fighting regulation and <a href="/wiki/Politicization_of_science" title="Politicization of science">politicizing</a> the underlying debate.<sup id="cite_ref-121" class="reference"><a href="#cite_note-121">&#91;121&#93;</a></sup>
</p>
<h4><span class="mw-headline" id="Regulation">Regulation</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Existential_risk_from_artificial_general_intelligence&amp;action=edit&amp;section=27" title="Edit section: Regulation">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<div role="note" class="hatnote navigation-not-searchable">See also: <a href="/wiki/Regulation_of_algorithms" title="Regulation of algorithms">Regulation of algorithms</a> and <a href="/wiki/Regulation_of_artificial_intelligence" title="Regulation of artificial intelligence">Regulation of artificial intelligence</a></div>
<p><a href="/wiki/Elon_Musk" title="Elon Musk">Elon Musk</a> called for some sort of regulation of AI development as early as 2017. According to <a href="/wiki/National_Public_Radio" class="mw-redirect" title="National Public Radio">NPR</a>, the <a href="/wiki/Tesla,_Inc." title="Tesla, Inc.">Tesla</a> CEO is "clearly not thrilled" to be advocating for government scrutiny that could impact his own industry, but believes the risks of going completely without oversight are too high: "Normally the way regulations are set up is when a bunch of bad things happen, there's a public outcry, and after many years a regulatory agency is set up to regulate that industry. It takes forever. That, in the past, has been bad but not something which represented a fundamental risk to the existence of civilisation." Musk states the first step would be for the government to gain "insight" into the actual status of current research, warning that "Once there is awareness, people will be extremely afraid<span class="nowrap">&#160;</span>... [as] they should be." In response, politicians express skepticism about the wisdom of regulating a technology that's still in development.<sup id="cite_ref-122" class="reference"><a href="#cite_note-122">&#91;122&#93;</a></sup><sup id="cite_ref-123" class="reference"><a href="#cite_note-123">&#91;123&#93;</a></sup><sup id="cite_ref-cnbc_124-0" class="reference"><a href="#cite_note-cnbc-124">&#91;124&#93;</a></sup>
</p><p>Responding both to Musk and to February 2017 proposals by European Union lawmakers to regulate AI and robotics, Intel CEO <a href="/wiki/Brian_Krzanich" title="Brian Krzanich">Brian Krzanich</a> argues that artificial intelligence is in its infancy and that it is too early to regulate the technology.<sup id="cite_ref-cnbc_124-1" class="reference"><a href="#cite_note-cnbc-124">&#91;124&#93;</a></sup> Instead of trying to regulate the technology itself, some scholars suggest to rather develop common norms including requirements for the testing and transparency of algorithms, possibly in combination with some form of warranty.<sup id="cite_ref-125" class="reference"><a href="#cite_note-125">&#91;125&#93;</a></sup> Developing well regulated weapons systems is in line with the ethos of some countries' militaries.<sup id="cite_ref-126" class="reference"><a href="#cite_note-126">&#91;126&#93;</a></sup> On October 31, 2019, the Unites States Department of Defense's (DoD's) Defense Innovation Board published the draft of a report outlining five principles for weaponized AI and making 12 recommendations for the ethical use of artificial intelligence by the DoD that seeks to manage the control problem in all DoD weaponized AI.<sup id="cite_ref-127" class="reference"><a href="#cite_note-127">&#91;127&#93;</a></sup>
</p><p>Regulation of AGI would likely be influenced by regulation of weaponized or militarized AI, i.e., the <a href="/wiki/Artificial_intelligence_arms_race" title="Artificial intelligence arms race">AI arms race</a>, the regulation of which is an emerging issue. Any form of regulation will likely be influenced by developments in leading countries' domestic policy towards militarized AI, in the US under the purview of the National Security Commission on Artificial Intelligence,<sup id="cite_ref-128" class="reference"><a href="#cite_note-128">&#91;128&#93;</a></sup><sup id="cite_ref-129" class="reference"><a href="#cite_note-129">&#91;129&#93;</a></sup> and international moves to regulate an AI arms race. Regulation of research into AGI focuses on the role of review boards and encouraging research into safe AI, and the possibility of differential technological progress (prioritizing risk-reducing strategies over risk-taking strategies in AI development) or conducting international mass surveillance to perform AGI arms control.<sup id="cite_ref-:5_130-0" class="reference"><a href="#cite_note-:5-130">&#91;130&#93;</a></sup> Regulation of conscious AGIs focuses on integrating them with existing human society and can be divided into considerations of their legal standing and of their moral rights.<sup id="cite_ref-:5_130-1" class="reference"><a href="#cite_note-:5-130">&#91;130&#93;</a></sup> AI arms control will likely require the institutionalization of new international norms embodied in effective technical specifications combined with active monitoring and informal diplomacy by communities of experts, together with a legal and political verification process.<sup id="cite_ref-131" class="reference"><a href="#cite_note-131">&#91;131&#93;</a></sup><sup id="cite_ref-132" class="reference"><a href="#cite_note-132">&#91;132&#93;</a></sup>
</p>
<h2><span class="mw-headline" id="See_also">See also</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Existential_risk_from_artificial_general_intelligence&amp;action=edit&amp;section=28" title="Edit section: See also">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<ul><li><a href="/wiki/AI_takeover" title="AI takeover">AI takeover</a></li>
<li><a href="/wiki/Artificial_intelligence_arms_race" title="Artificial intelligence arms race">Artificial intelligence arms race</a></li>
<li><a href="/wiki/Effective_altruism#Long_term_future_and_global_catastrophic_risks" title="Effective altruism">Effective altruism § Long term future and global catastrophic risks</a></li>
<li><a href="/wiki/Grey_goo" class="mw-redirect" title="Grey goo">Grey goo</a></li>
<li><i><a href="/wiki/Human_Compatible" title="Human Compatible">Human Compatible</a></i></li>
<li><a href="/wiki/Lethal_autonomous_weapon" title="Lethal autonomous weapon">Lethal autonomous weapon</a></li>
<li><a href="/wiki/Regulation_of_algorithms" title="Regulation of algorithms">Regulation of algorithms</a></li>
<li><a href="/wiki/Regulation_of_artificial_intelligence" title="Regulation of artificial intelligence">Regulation of artificial intelligence</a></li>
<li><a href="/wiki/Robot_ethics#In_popular_culture" title="Robot ethics">Robot ethics § In popular culture</a></li>
<li><i><a href="/wiki/Superintelligence:_Paths,_Dangers,_Strategies" title="Superintelligence: Paths, Dangers, Strategies">Superintelligence: Paths, Dangers, Strategies</a></i></li>
<li><a href="/wiki/System_accident" title="System accident">System accident</a></li>
<li><a href="/wiki/Technological_singularity" title="Technological singularity">Technological singularity</a></li>
<li><i><a href="/wiki/The_Precipice:_Existential_Risk_and_the_Future_of_Humanity" title="The Precipice: Existential Risk and the Future of Humanity">The Precipice: Existential Risk and the Future of Humanity</a></i></li></ul>
<h2><span class="mw-headline" id="References">References</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Existential_risk_from_artificial_general_intelligence&amp;action=edit&amp;section=29" title="Edit section: References">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div class="reflist" style="list-style-type: decimal;">
<div class="mw-references-wrap mw-references-columns"><ol class="references">
<li id="cite_note-aima-1"><span class="mw-cite-backlink">^ <a href="#cite_ref-aima_1-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-aima_1-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-aima_1-2"><sup><i><b>c</b></i></sup></a> <a href="#cite_ref-aima_1-3"><sup><i><b>d</b></i></sup></a> <a href="#cite_ref-aima_1-4"><sup><i><b>e</b></i></sup></a> <a href="#cite_ref-aima_1-5"><sup><i><b>f</b></i></sup></a> <a href="#cite_ref-aima_1-6"><sup><i><b>g</b></i></sup></a> <a href="#cite_ref-aima_1-7"><sup><i><b>h</b></i></sup></a> <a href="#cite_ref-aima_1-8"><sup><i><b>i</b></i></sup></a> <a href="#cite_ref-aima_1-9"><sup><i><b>j</b></i></sup></a></span> <span class="reference-text"><cite class="citation book"><a href="/wiki/Stuart_J._Russell" title="Stuart J. Russell">Russell, Stuart</a>; <a href="/wiki/Peter_Norvig" title="Peter Norvig">Norvig, Peter</a> (2009). "26.3: The Ethics and Risks of Developing Artificial Intelligence". <a href="/wiki/Artificial_Intelligence:_A_Modern_Approach" title="Artificial Intelligence: A Modern Approach"><i>Artificial Intelligence: A Modern Approach</i></a>. Prentice Hall. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-0-13-604259-4" title="Special:BookSources/978-0-13-604259-4"><bdi>978-0-13-604259-4</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=26.3%3A+The+Ethics+and+Risks+of+Developing+Artificial+Intelligence&amp;rft.btitle=Artificial+Intelligence%3A+A+Modern+Approach&amp;rft.pub=Prentice+Hall&amp;rft.date=2009&amp;rft.isbn=978-0-13-604259-4&amp;rft.aulast=Russell&amp;rft.aufirst=Stuart&amp;rft.au=Norvig%2C+Peter&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><style data-mw-deduplicate="TemplateStyles:r935243608">.mw-parser-output cite.citation{font-style:inherit}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Lock-gray-alt-2.svg/9px-Lock-gray-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Lock-red-alt-2.svg/9px-Lock-red-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/12px-Wikisource-logo.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:inherit;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-maint{display:none;color:#33aa33;margin-left:0.3em}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}</style></span>
</li>
<li id="cite_note-2"><span class="mw-cite-backlink"><b><a href="#cite_ref-2">^</a></b></span> <span class="reference-text"><cite class="citation journal"><a href="/wiki/Nick_Bostrom" title="Nick Bostrom">Bostrom, Nick</a> (2002). "Existential risks". <i><a href="/wiki/Journal_of_Evolution_and_Technology" class="mw-redirect" title="Journal of Evolution and Technology">Journal of Evolution and Technology</a></i>. <b>9</b> (1): 1–31.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Evolution+and+Technology&amp;rft.atitle=Existential+risks&amp;rft.volume=9&amp;rft.issue=1&amp;rft.pages=1-31&amp;rft.date=2002&amp;rft.aulast=Bostrom&amp;rft.aufirst=Nick&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-3"><span class="mw-cite-backlink"><b><a href="#cite_ref-3">^</a></b></span> <span class="reference-text"><cite class="citation journal">Turchin, Alexey; Denkenberger, David (3 May 2018). <a rel="nofollow" class="external text" href="https://dx.doi.org/10.1007/s00146-018-0845-5">"Classification of global catastrophic risks connected with artificial intelligence"</a>. <i>AI &amp; SOCIETY</i>. <b>35</b> (1): 147–163. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1007%2Fs00146-018-0845-5">10.1007/s00146-018-0845-5</a>. <a href="/wiki/International_Standard_Serial_Number" title="International Standard Serial Number">ISSN</a>&#160;<a rel="nofollow" class="external text" href="//www.worldcat.org/issn/0951-5666">0951-5666</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=AI+%26+SOCIETY&amp;rft.atitle=Classification+of+global+catastrophic+risks+connected+with+artificial+intelligence&amp;rft.volume=35&amp;rft.issue=1&amp;rft.pages=147-163&amp;rft.date=2018-05-03&amp;rft_id=info%3Adoi%2F10.1007%2Fs00146-018-0845-5&amp;rft.issn=0951-5666&amp;rft.aulast=Turchin&amp;rft.aufirst=Alexey&amp;rft.au=Denkenberger%2C+David&amp;rft_id=http%3A%2F%2Fdx.doi.org%2F10.1007%2Fs00146-018-0845-5&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-superintelligence-4"><span class="mw-cite-backlink">^ <a href="#cite_ref-superintelligence_4-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-superintelligence_4-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-superintelligence_4-2"><sup><i><b>c</b></i></sup></a> <a href="#cite_ref-superintelligence_4-3"><sup><i><b>d</b></i></sup></a> <a href="#cite_ref-superintelligence_4-4"><sup><i><b>e</b></i></sup></a> <a href="#cite_ref-superintelligence_4-5"><sup><i><b>f</b></i></sup></a> <a href="#cite_ref-superintelligence_4-6"><sup><i><b>g</b></i></sup></a> <a href="#cite_ref-superintelligence_4-7"><sup><i><b>h</b></i></sup></a> <a href="#cite_ref-superintelligence_4-8"><sup><i><b>i</b></i></sup></a> <a href="#cite_ref-superintelligence_4-9"><sup><i><b>j</b></i></sup></a></span> <span class="reference-text"><cite class="citation book"><a href="/wiki/Nick_Bostrom" title="Nick Bostrom">Bostrom, Nick</a> (2014). <a href="/wiki/Superintelligence:_Paths,_Dangers,_Strategies" title="Superintelligence: Paths, Dangers, Strategies"><i>Superintelligence: Paths, Dangers, Strategies</i></a> (First ed.). <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-0199678112" title="Special:BookSources/978-0199678112"><bdi>978-0199678112</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Superintelligence%3A+Paths%2C+Dangers%2C+Strategies&amp;rft.edition=First&amp;rft.date=2014&amp;rft.isbn=978-0199678112&amp;rft.aulast=Bostrom&amp;rft.aufirst=Nick&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-givewell-5"><span class="mw-cite-backlink">^ <a href="#cite_ref-givewell_5-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-givewell_5-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-givewell_5-2"><sup><i><b>c</b></i></sup></a></span> <span class="reference-text"><cite class="citation report"><a href="/wiki/GiveWell" title="GiveWell">GiveWell</a> (2015). <a rel="nofollow" class="external text" href="http://www.givewell.org/labs/causes/ai-risk">Potential risks from advanced artificial intelligence</a> (Report)<span class="reference-accessdate">. Retrieved <span class="nowrap">11 October</span> 2015</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=report&amp;rft.btitle=Potential+risks+from+advanced+artificial+intelligence&amp;rft.date=2015&amp;rft.au=GiveWell&amp;rft_id=http%3A%2F%2Fwww.givewell.org%2Flabs%2Fcauses%2Fai-risk&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-6"><span class="mw-cite-backlink"><b><a href="#cite_ref-6">^</a></b></span> <span class="reference-text"><cite class="citation news">Parkin, Simon (14 June 2015). <a rel="nofollow" class="external text" href="https://www.theguardian.com/tv-and-radio/2015/jun/14/science-fiction-no-more-humans-tv-artificial-intelligence">"Science fiction no more? Channel 4's Humans and our rogue AI obsessions"</a>. <i><a href="/wiki/The_Guardian" title="The Guardian">The Guardian</a></i><span class="reference-accessdate">. Retrieved <span class="nowrap">5 February</span> 2018</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+Guardian&amp;rft.atitle=Science+fiction+no+more%3F+Channel+4%27s+Humans+and+our+rogue+AI+obsessions&amp;rft.date=2015-06-14&amp;rft.aulast=Parkin&amp;rft.aufirst=Simon&amp;rft_id=https%3A%2F%2Fwww.theguardian.com%2Ftv-and-radio%2F2015%2Fjun%2F14%2Fscience-fiction-no-more-humans-tv-artificial-intelligence&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-yudkowsky-global-risk-7"><span class="mw-cite-backlink">^ <a href="#cite_ref-yudkowsky-global-risk_7-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-yudkowsky-global-risk_7-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-yudkowsky-global-risk_7-2"><sup><i><b>c</b></i></sup></a> <a href="#cite_ref-yudkowsky-global-risk_7-3"><sup><i><b>d</b></i></sup></a> <a href="#cite_ref-yudkowsky-global-risk_7-4"><sup><i><b>e</b></i></sup></a> <a href="#cite_ref-yudkowsky-global-risk_7-5"><sup><i><b>f</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal">Yudkowsky, Eliezer (2008). <a rel="nofollow" class="external text" href="https://intelligence.org/files/AIPosNegFactor.pdf">"Artificial Intelligence as a Positive and Negative Factor in Global Risk"</a> <span class="cs1-format">(PDF)</span>. <i>Global Catastrophic Risks</i>: 308–345. <a href="/wiki/Bibcode" title="Bibcode">Bibcode</a>:<a rel="nofollow" class="external text" href="https://ui.adsabs.harvard.edu/abs/2008gcr..book..303Y">2008gcr..book..303Y</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Global+Catastrophic+Risks&amp;rft.atitle=Artificial+Intelligence+as+a+Positive+and+Negative+Factor+in+Global+Risk&amp;rft.pages=308-345&amp;rft.date=2008&amp;rft_id=info%3Abibcode%2F2008gcr..book..303Y&amp;rft.aulast=Yudkowsky&amp;rft.aufirst=Eliezer&amp;rft_id=https%3A%2F%2Fintelligence.org%2Ffiles%2FAIPosNegFactor.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-research-priorities-8"><span class="mw-cite-backlink"><b><a href="#cite_ref-research-priorities_8-0">^</a></b></span> <span class="reference-text"><cite class="citation journal"><a href="/wiki/Stuart_J._Russell" title="Stuart J. Russell">Russell, Stuart</a>; Dewey, Daniel; <a href="/wiki/Max_Tegmark" title="Max Tegmark">Tegmark, Max</a> (2015). <a rel="nofollow" class="external text" href="https://futureoflife.org/data/documents/research_priorities.pdf">"Research Priorities for Robust and Beneficial Artificial Intelligence"</a> <span class="cs1-format">(PDF)</span>. <i>AI Magazine</i>. Association for the Advancement of Artificial Intelligence: 105–114. <a href="/wiki/ArXiv" title="ArXiv">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//arxiv.org/abs/1602.03506">1602.03506</a></span>. <a href="/wiki/Bibcode" title="Bibcode">Bibcode</a>:<a rel="nofollow" class="external text" href="https://ui.adsabs.harvard.edu/abs/2016arXiv160203506R">2016arXiv160203506R</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=AI+Magazine&amp;rft.atitle=Research+Priorities+for+Robust+and+Beneficial+Artificial+Intelligence&amp;rft.pages=105-114&amp;rft.date=2015&amp;rft_id=info%3Aarxiv%2F1602.03506&amp;rft_id=info%3Abibcode%2F2016arXiv160203506R&amp;rft.aulast=Russell&amp;rft.aufirst=Stuart&amp;rft.au=Dewey%2C+Daniel&amp;rft.au=Tegmark%2C+Max&amp;rft_id=https%3A%2F%2Ffutureoflife.org%2Fdata%2Fdocuments%2Fresearch_priorities.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/>, cited in <cite class="citation web"><a rel="nofollow" class="external text" href="https://futureoflife.org/ai-open-letter">"AI Open Letter - Future of Life Institute"</a>. <i>Future of Life Institute</i>. <a href="/wiki/Future_of_Life_Institute" title="Future of Life Institute">Future of Life Institute</a>. January 2015<span class="reference-accessdate">. Retrieved <span class="nowrap">9 August</span> 2019</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Future+of+Life+Institute&amp;rft.atitle=AI+Open+Letter+-+Future+of+Life+Institute&amp;rft.date=2015-01&amp;rft_id=https%3A%2F%2Ffutureoflife.org%2Fai-open-letter&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-vanity-9"><span class="mw-cite-backlink">^ <a href="#cite_ref-vanity_9-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-vanity_9-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-vanity_9-2"><sup><i><b>c</b></i></sup></a> <a href="#cite_ref-vanity_9-3"><sup><i><b>d</b></i></sup></a></span> <span class="reference-text"><cite class="citation news">Dowd, Maureen (April 2017). <a rel="nofollow" class="external text" href="https://www.vanityfair.com/news/2017/03/elon-musk-billion-dollar-crusade-to-stop-ai-space-x">"Elon Musk's Billion-Dollar Crusade to Stop the A.I. Apocalypse"</a>. <i>The Hive</i><span class="reference-accessdate">. Retrieved <span class="nowrap">27 November</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+Hive&amp;rft.atitle=Elon+Musk%27s+Billion-Dollar+Crusade+to+Stop+the+A.I.+Apocalypse&amp;rft.date=2017-04&amp;rft.aulast=Dowd&amp;rft.aufirst=Maureen&amp;rft_id=https%3A%2F%2Fwww.vanityfair.com%2Fnews%2F2017%2F03%2Felon-musk-billion-dollar-crusade-to-stop-ai-space-x&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-skeptic-10"><span class="mw-cite-backlink">^ <a href="#cite_ref-skeptic_10-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-skeptic_10-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-skeptic_10-2"><sup><i><b>c</b></i></sup></a> <a href="#cite_ref-skeptic_10-3"><sup><i><b>d</b></i></sup></a> <a href="#cite_ref-skeptic_10-4"><sup><i><b>e</b></i></sup></a></span> <span class="reference-text"><cite class="citation news">Graves, Matthew (8 November 2017). <a rel="nofollow" class="external text" href="https://www.skeptic.com/reading_room/why-we-should-be-concerned-about-artificial-superintelligence/">"Why We Should Be Concerned About Artificial Superintelligence"</a>. <i><a href="/wiki/Skeptic_(US_magazine)" class="mw-redirect" title="Skeptic (US magazine)">Skeptic (US magazine)</a></i>. <b>22</b> (2)<span class="reference-accessdate">. Retrieved <span class="nowrap">27 November</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Skeptic+%28US+magazine%29&amp;rft.atitle=Why+We+Should+Be+Concerned+About+Artificial+Superintelligence&amp;rft.volume=22&amp;rft.issue=2&amp;rft.date=2017-11-08&amp;rft.aulast=Graves&amp;rft.aufirst=Matthew&amp;rft_id=https%3A%2F%2Fwww.skeptic.com%2Freading_room%2Fwhy-we-should-be-concerned-about-artificial-superintelligence%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-11"><span class="mw-cite-backlink"><b><a href="#cite_ref-11">^</a></b></span> <span class="reference-text">Breuer, Hans-Peter. <a rel="nofollow" class="external text" href="https://www.jstor.org/pss/436868">'Samuel Butler's "the Book of the Machines" and the Argument from Design.'</a> Modern Philology, Vol. 72, No. 4 (May 1975), pp. 365–383</span>
</li>
<li id="cite_note-oxfordjournals-12"><span class="mw-cite-backlink"><b><a href="#cite_ref-oxfordjournals_12-0">^</a></b></span> <span class="reference-text">A M Turing, <i><a rel="nofollow" class="external text" href="http://philmat.oxfordjournals.org/content/4/3/256.full.pdf">Intelligent Machinery, A Heretical Theory</a></i>, 1951, reprinted <i>Philosophia Mathematica</i> (1996) 4(3): 256–260 <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1093%2Fphilmat%2F4.3.256">10.1093/philmat/4.3.256</a></span>
</li>
<li id="cite_note-13"><span class="mw-cite-backlink"><b><a href="#cite_ref-13">^</a></b></span> <span class="reference-text"><cite class="citation news">Hilliard, Mark (2017). <a rel="nofollow" class="external text" href="https://www.irishtimes.com/business/innovation/the-ai-apocalypse-will-the-human-race-soon-be-terminated-1.3019220">"The AI apocalypse: will the human race soon be terminated?"</a>. <i>The Irish Times</i><span class="reference-accessdate">. Retrieved <span class="nowrap">15 March</span> 2020</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+Irish+Times&amp;rft.atitle=The+AI+apocalypse%3A+will+the+human+race+soon+be+terminated%3F&amp;rft.date=2017&amp;rft.aulast=Hilliard&amp;rft.aufirst=Mark&amp;rft_id=https%3A%2F%2Fwww.irishtimes.com%2Fbusiness%2Finnovation%2Fthe-ai-apocalypse-will-the-human-race-soon-be-terminated-1.3019220&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-14"><span class="mw-cite-backlink"><b><a href="#cite_ref-14">^</a></b></span> <span class="reference-text">I.J. Good, <a rel="nofollow" class="external text" href="http://commonsenseatheism.com/wp-content/uploads/2011/02/Good-Speculations-Concerning-the-First-Ultraintelligent-Machine.pdf">"Speculations Concerning the First Ultraintelligent Machine"</a> <a rel="nofollow" class="external text" href="https://web.archive.org/web/20111128085512/http://commonsenseatheism.com/wp-content/uploads/2011/02/Good-Speculations-Concerning-the-First-Ultraintelligent-Machine.pdf">Archived</a> 2011-11-28 at the <a href="/wiki/Wayback_Machine" title="Wayback Machine">Wayback Machine</a> (<a rel="nofollow" class="external text" href="http://www.acceleratingfuture.com/pages/ultraintelligentmachine.html">HTML</a> <a rel="nofollow" class="external text" href="https://web.archive.org/web/20111128085512/http://commonsenseatheism.com/wp-content/uploads/2011/02/Good-Speculations-Concerning-the-First-Ultraintelligent-Machine.pdf">Archived</a> 28 November 2011 at the <a href="/wiki/Wayback_Machine" title="Wayback Machine">Wayback Machine</a>), <i>Advances in Computers</i>, vol. 6, 1965.</span>
</li>
<li id="cite_note-15"><span class="mw-cite-backlink"><b><a href="#cite_ref-15">^</a></b></span> <span class="reference-text"><cite class="citation book">Russell, Stuart J.; Norvig, Peter (2003). "Section 26.3: The Ethics and Risks of Developing Artificial Intelligence". <a href="/wiki/Artificial_Intelligence:_A_Modern_Approach" title="Artificial Intelligence: A Modern Approach"><i>Artificial Intelligence: A Modern Approach</i></a>. Upper Saddle River, N.J.: Prentice Hall. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-0137903955" title="Special:BookSources/978-0137903955"><bdi>978-0137903955</bdi></a>. <q>Similarly, Marvin Minsky once suggested that an AI program designed to solve the Riemann Hypothesis might end up taking over all the resources of Earth to build more powerful supercomputers to help achieve its goal.</q></cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Section+26.3%3A+The+Ethics+and+Risks+of+Developing+Artificial+Intelligence&amp;rft.btitle=Artificial+Intelligence%3A+A+Modern+Approach&amp;rft.place=Upper+Saddle+River%2C+N.J.&amp;rft.pub=Prentice+Hall&amp;rft.date=2003&amp;rft.isbn=978-0137903955&amp;rft.aulast=Russell&amp;rft.aufirst=Stuart+J.&amp;rft.au=Norvig%2C+Peter&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-16"><span class="mw-cite-backlink"><b><a href="#cite_ref-16">^</a></b></span> <span class="reference-text"><cite class="citation book">Barrat, James (2013). <i>Our final invention&#160;: artificial intelligence and the end of the human era</i> (First ed.). New York: St. Martin's Press. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/9780312622374" title="Special:BookSources/9780312622374"><bdi>9780312622374</bdi></a>. <q>In the bio, playfully written in the third person, Good summarized his life’s milestones, including a probably never before seen account of his work at Bletchley Park with Turing. But here’s what he wrote in 1998 about the first superintelligence, and his late-in-the-game U-turn:  [The paper] 'Speculations Concerning the First Ultra-intelligent Machine' (1965) . . . began: 'The survival of man depends on the early construction of an ultra-intelligent machine.' Those were his [Good’s] words during the Cold War, and he now suspects that 'survival' should be replaced by 'extinction.' He thinks that, because of international competition, we cannot prevent the machines from taking over. He thinks we are lemmings. He said also that 'probably Man will construct the deus ex machina in his own image.'</q></cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Our+final+invention+%3A+artificial+intelligence+and+the+end+of+the+human+era&amp;rft.place=New+York&amp;rft.edition=First&amp;rft.pub=St.+Martin%27s+Press&amp;rft.date=2013&amp;rft.isbn=9780312622374&amp;rft.aulast=Barrat&amp;rft.aufirst=James&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-17"><span class="mw-cite-backlink"><b><a href="#cite_ref-17">^</a></b></span> <span class="reference-text"><cite class="citation news">Anderson, Kurt (26 November 2014). <a rel="nofollow" class="external text" href="https://www.vanityfair.com/news/tech/2014/11/artificial-intelligence-singularity-theory">"Enthusiasts and Skeptics Debate Artificial Intelligence"</a>. <i><a href="/wiki/Vanity_Fair_(magazine)" title="Vanity Fair (magazine)">Vanity Fair</a></i><span class="reference-accessdate">. Retrieved <span class="nowrap">30 January</span> 2016</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Vanity+Fair&amp;rft.atitle=Enthusiasts+and+Skeptics+Debate+Artificial+Intelligence&amp;rft.date=2014-11-26&amp;rft.aulast=Anderson&amp;rft.aufirst=Kurt&amp;rft_id=https%3A%2F%2Fwww.vanityfair.com%2Fnews%2Ftech%2F2014%2F11%2Fartificial-intelligence-singularity-theory&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-nytimes_july09-18"><span class="mw-cite-backlink"><b><a href="#cite_ref-nytimes_july09_18-0">^</a></b></span> <span class="reference-text"><a rel="nofollow" class="external text" href="https://www.nytimes.com/2009/07/26/science/26robot.html?_r=1&amp;ref=todayspaper">Scientists Worry Machines May Outsmart Man</a> By JOHN MARKOFF, NY Times, 26 July 2009.</span>
</li>
<li id="cite_note-19"><span class="mw-cite-backlink"><b><a href="#cite_ref-19">^</a></b></span> <span class="reference-text"><cite class="citation news">Metz, Cade (9 June 2018). <a rel="nofollow" class="external text" href="https://www.nytimes.com/2018/06/09/technology/elon-musk-mark-zuckerberg-artificial-intelligence.html">"Mark Zuckerberg, Elon Musk and the Feud Over Killer Robots"</a>. <i>The New York Times</i><span class="reference-accessdate">. Retrieved <span class="nowrap">3 April</span> 2019</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+New+York+Times&amp;rft.atitle=Mark+Zuckerberg%2C+Elon+Musk+and+the+Feud+Over+Killer+Robots&amp;rft.date=2018-06-09&amp;rft.aulast=Metz&amp;rft.aufirst=Cade&amp;rft_id=https%3A%2F%2Fwww.nytimes.com%2F2018%2F06%2F09%2Ftechnology%2Felon-musk-mark-zuckerberg-artificial-intelligence.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-20"><span class="mw-cite-backlink"><b><a href="#cite_ref-20">^</a></b></span> <span class="reference-text"><cite class="citation news">Hsu, Jeremy (1 March 2012). <a rel="nofollow" class="external text" href="http://www.nbcnews.com/id/46590591/ns/technology_and_science-innovation">"Control dangerous AI before it controls us, one expert says"</a>. <i><a href="/wiki/NBC_News" title="NBC News">NBC News</a></i><span class="reference-accessdate">. Retrieved <span class="nowrap">28 January</span> 2016</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=NBC+News&amp;rft.atitle=Control+dangerous+AI+before+it+controls+us%2C+one+expert+says&amp;rft.date=2012-03-01&amp;rft.aulast=Hsu&amp;rft.aufirst=Jeremy&amp;rft_id=http%3A%2F%2Fwww.nbcnews.com%2Fid%2F46590591%2Fns%2Ftechnology_and_science-innovation&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-hawking_editorial-21"><span class="mw-cite-backlink">^ <a href="#cite_ref-hawking_editorial_21-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-hawking_editorial_21-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-hawking_editorial_21-2"><sup><i><b>c</b></i></sup></a> <a href="#cite_ref-hawking_editorial_21-3"><sup><i><b>d</b></i></sup></a> <a href="#cite_ref-hawking_editorial_21-4"><sup><i><b>e</b></i></sup></a></span> <span class="reference-text"><cite class="citation news"><a rel="nofollow" class="external text" href="https://www.independent.co.uk/news/science/stephen-hawking-transcendence-looks-at-the-implications-of-artificial-intelligence--but-are-we-taking-ai-seriously-enough-9313474.html">"Stephen Hawking: 'Transcendence looks at the implications of artificial intelligence&#160;– but are we taking AI seriously enough?<span class="cs1-kern-right">'</span>"</a>. <a href="/wiki/The_Independent_(UK)" class="mw-redirect" title="The Independent (UK)">The Independent (UK)</a><span class="reference-accessdate">. Retrieved <span class="nowrap">3 December</span> 2014</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Stephen+Hawking%3A+%27Transcendence+looks+at+the+implications+of+artificial+intelligence+%E2%80%93+but+are+we+taking+AI+seriously+enough%3F%27&amp;rft_id=https%3A%2F%2Fwww.independent.co.uk%2Fnews%2Fscience%2Fstephen-hawking-transcendence-looks-at-the-implications-of-artificial-intelligence--but-are-we-taking-ai-seriously-enough-9313474.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-bbc_on_hawking_editorial-22"><span class="mw-cite-backlink">^ <a href="#cite_ref-bbc_on_hawking_editorial_22-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-bbc_on_hawking_editorial_22-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-bbc_on_hawking_editorial_22-2"><sup><i><b>c</b></i></sup></a></span> <span class="reference-text"><cite class="citation news"><a rel="nofollow" class="external text" href="https://www.bbc.com/news/technology-30290540">"Stephen Hawking warns artificial intelligence could end mankind"</a>. <a href="/wiki/BBC" title="BBC">BBC</a>. 2 December 2014<span class="reference-accessdate">. Retrieved <span class="nowrap">3 December</span> 2014</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Stephen+Hawking+warns+artificial+intelligence+could+end+mankind&amp;rft.date=2014-12-02&amp;rft_id=https%3A%2F%2Fwww.bbc.com%2Fnews%2Ftechnology-30290540&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-23"><span class="mw-cite-backlink"><b><a href="#cite_ref-23">^</a></b></span> <span class="reference-text"><cite class="citation news">Eadicicco, Lisa (28 January 2015). <a rel="nofollow" class="external text" href="http://www.businessinsider.com/bill-gates-artificial-intelligence-2015-1">"Bill Gates: Elon Musk Is Right, We Should All Be Scared Of Artificial Intelligence Wiping Out Humanity"</a>. <i><a href="/wiki/Business_Insider" title="Business Insider">Business Insider</a></i><span class="reference-accessdate">. Retrieved <span class="nowrap">30 January</span> 2016</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Business+Insider&amp;rft.atitle=Bill+Gates%3A+Elon+Musk+Is+Right%2C+We+Should+All+Be+Scared+Of+Artificial+Intelligence+Wiping+Out+Humanity&amp;rft.date=2015-01-28&amp;rft.aulast=Eadicicco&amp;rft.aufirst=Lisa&amp;rft_id=http%3A%2F%2Fwww.businessinsider.com%2Fbill-gates-artificial-intelligence-2015-1&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-slate_killer-24"><span class="mw-cite-backlink">^ <a href="#cite_ref-slate_killer_24-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-slate_killer_24-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-slate_killer_24-2"><sup><i><b>c</b></i></sup></a></span> <span class="reference-text"><cite class="citation news">Tilli, Cecilia (28 April 2016). <a rel="nofollow" class="external text" href="http://www.slate.com/articles/technology/future_tense/2016/04/the_threats_that_artificial_intelligence_researchers_actually_worry_about.html">"Killer Robots? Lost Jobs?"</a>. <i>Slate</i><span class="reference-accessdate">. Retrieved <span class="nowrap">15 May</span> 2016</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Slate&amp;rft.atitle=Killer+Robots%3F+Lost+Jobs%3F&amp;rft.date=2016-04-28&amp;rft.aulast=Tilli&amp;rft.aufirst=Cecilia&amp;rft_id=http%3A%2F%2Fwww.slate.com%2Farticles%2Ftechnology%2Ffuture_tense%2F2016%2F04%2Fthe_threats_that_artificial_intelligence_researchers_actually_worry_about.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-25"><span class="mw-cite-backlink"><b><a href="#cite_ref-25">^</a></b></span> <span class="reference-text"><cite class="citation web"><a rel="nofollow" class="external text" href="http://www.tor.com/2011/06/21/norvig-vs-chomsky-and-the-fight-for-the-future-of-ai/">"Norvig vs. Chomsky and the Fight for the Future of AI"</a>. <i>Tor.com</i>. 21 June 2011<span class="reference-accessdate">. Retrieved <span class="nowrap">15 May</span> 2016</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Tor.com&amp;rft.atitle=Norvig+vs.+Chomsky+and+the+Fight+for+the+Future+of+AI&amp;rft.date=2011-06-21&amp;rft_id=http%3A%2F%2Fwww.tor.com%2F2011%2F06%2F21%2Fnorvig-vs-chomsky-and-the-fight-for-the-future-of-ai%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-26"><span class="mw-cite-backlink"><b><a href="#cite_ref-26">^</a></b></span> <span class="reference-text"><cite class="citation news">Johnson, Phil (30 July 2015). <a rel="nofollow" class="external text" href="https://www.itworld.com/article/2823083/enterprise-software/88716-8-famous-software-bugs-in-space.html">"Houston, we have a bug: 9 famous software glitches in space"</a>. <i><a href="/w/index.php?title=IT_World&amp;action=edit&amp;redlink=1" class="new" title="IT World (page does not exist)">IT World</a></i><span class="reference-accessdate">. Retrieved <span class="nowrap">5 February</span> 2018</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=IT+World&amp;rft.atitle=Houston%2C+we+have+a+bug%3A+9+famous+software+glitches+in+space&amp;rft.date=2015-07-30&amp;rft.aulast=Johnson&amp;rft.aufirst=Phil&amp;rft_id=https%3A%2F%2Fwww.itworld.com%2Farticle%2F2823083%2Fenterprise-software%2F88716-8-famous-software-bugs-in-space.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-27"><span class="mw-cite-backlink"><b><a href="#cite_ref-27">^</a></b></span> <span class="reference-text"><cite class="citation journal">Yampolskiy, Roman V. (8 April 2014). "Utility function security in artificially intelligent agents". <i>Journal of Experimental &amp; Theoretical Artificial Intelligence</i>. <b>26</b> (3): 373–389. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1080%2F0952813X.2014.895114">10.1080/0952813X.2014.895114</a>. <q>Nothing precludes sufficiently smart self-improving systems from optimising their reward mechanisms in order to optimisetheir current-goal achievement and in the process making a mistake leading to corruption of their reward functions.</q></cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Experimental+%26+Theoretical+Artificial+Intelligence&amp;rft.atitle=Utility+function+security+in+artificially+intelligent+agents&amp;rft.volume=26&amp;rft.issue=3&amp;rft.pages=373-389&amp;rft.date=2014-04-08&amp;rft_id=info%3Adoi%2F10.1080%2F0952813X.2014.895114&amp;rft.aulast=Yampolskiy&amp;rft.aufirst=Roman+V.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-28"><span class="mw-cite-backlink"><b><a href="#cite_ref-28">^</a></b></span> <span class="reference-text"><cite id="CITEREFBostrom,_Nick,_1973-_author." class="citation">Bostrom, Nick, 1973- author., <i>Superintelligence&#160;: paths, dangers, strategies</i>, <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-1-5012-2774-5" title="Special:BookSources/978-1-5012-2774-5"><bdi>978-1-5012-2774-5</bdi></a>, <a href="/wiki/OCLC" title="OCLC">OCLC</a>&#160;<a rel="nofollow" class="external text" href="//www.worldcat.org/oclc/1061147095">1061147095</a></cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Superintelligence+%3A+paths%2C+dangers%2C+strategies&amp;rft_id=info%3Aoclcnum%2F1061147095&amp;rft.isbn=978-1-5012-2774-5&amp;rft.au=Bostrom%2C+Nick%2C+1973-+author.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><span class="cs1-maint citation-comment">CS1 maint: multiple names: authors list (<a href="/wiki/Category:CS1_maint:_multiple_names:_authors_list" title="Category:CS1 maint: multiple names: authors list">link</a>)</span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-29"><span class="mw-cite-backlink"><b><a href="#cite_ref-29">^</a></b></span> <span class="reference-text"><cite class="citation web"><a rel="nofollow" class="external text" href="http://futureoflife.org/misc/open_letter">"Research Priorities for Robust and Beneficial Artificial Intelligence: an Open Letter"</a>. <a href="/wiki/Future_of_Life_Institute" title="Future of Life Institute">Future of Life Institute</a><span class="reference-accessdate">. Retrieved <span class="nowrap">23 October</span> 2015</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Research+Priorities+for+Robust+and+Beneficial+Artificial+Intelligence%3A+an+Open+Letter&amp;rft.pub=Future+of+Life+Institute&amp;rft_id=http%3A%2F%2Ffutureoflife.org%2Fmisc%2Fopen_letter&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-economist_review-30"><span class="mw-cite-backlink">^ <a href="#cite_ref-economist_review_30-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-economist_review_30-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-economist_review_30-2"><sup><i><b>c</b></i></sup></a></span> <span class="reference-text"><cite class="citation news"><a rel="nofollow" class="external text" href="https://www.economist.com/news/books-and-arts/21611037-potential-impacts-intelligent-machines-human-life-clever-cogs">"Clever cogs"</a>. <i><a href="/wiki/The_Economist" title="The Economist">The Economist</a></i>. 9 August 2014<span class="reference-accessdate">. Retrieved <span class="nowrap">9 August</span> 2014</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+Economist&amp;rft.atitle=Clever+cogs&amp;rft.date=2014-08-09&amp;rft_id=https%3A%2F%2Fwww.economist.com%2Fnews%2Fbooks-and-arts%2F21611037-potential-impacts-intelligent-machines-human-life-clever-cogs&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/> <a rel="nofollow" class="external text" href="http://www.businessinsider.com/intelligent-machines-and-human-life-2014-8">Syndicated</a> at <a href="/wiki/Business_Insider" title="Business Insider">Business Insider</a></span>
</li>
<li id="cite_note-31"><span class="mw-cite-backlink"><b><a href="#cite_ref-31">^</a></b></span> <span class="reference-text">Yudkowsky, E. (2013). Intelligence explosion microeconomics. Machine Intelligence Research Institute.</span>
</li>
<li id="cite_note-32"><span class="mw-cite-backlink"><b><a href="#cite_ref-32">^</a></b></span> <span class="reference-text">Yampolskiy, Roman V. "Analysis of types of self-improving software." Artificial General Intelligence. Springer International Publishing, 2015. 384-393.</span>
</li>
<li id="cite_note-omohundro-33"><span class="mw-cite-backlink">^ <a href="#cite_ref-omohundro_33-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-omohundro_33-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-omohundro_33-2"><sup><i><b>c</b></i></sup></a></span> <span class="reference-text">Omohundro, S. M. (2008, February). The basic AI drives. In AGI (Vol. 171, pp. 483-492).</span>
</li>
<li id="cite_note-34"><span class="mw-cite-backlink"><b><a href="#cite_ref-34">^</a></b></span> <span class="reference-text"><cite class="citation news">Metz, Cade (13 August 2017). <a rel="nofollow" class="external text" href="https://www.nytimes.com/2017/08/13/technology/artificial-intelligence-safety-training.html">"Teaching A.I. Systems to Behave Themselves"</a>. <i>The New York Times</i>. <q>A machine will seek to preserve its off switch, they showed</q></cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+New+York+Times&amp;rft.atitle=Teaching+A.I.+Systems+to+Behave+Themselves&amp;rft.date=2017-08-13&amp;rft.aulast=Metz&amp;rft.aufirst=Cade&amp;rft_id=https%3A%2F%2Fwww.nytimes.com%2F2017%2F08%2F13%2Ftechnology%2Fartificial-intelligence-safety-training.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-35"><span class="mw-cite-backlink"><b><a href="#cite_ref-35">^</a></b></span> <span class="reference-text"><cite class="citation arxiv">Leike, Jan (2017). "AI Safety Gridworlds". <a href="/wiki/ArXiv" title="ArXiv">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//arxiv.org/abs/1711.09883">1711.09883</a></span> [<a rel="nofollow" class="external text" href="//arxiv.org/archive/cs.LG">cs.LG</a>]. <q>A2C learns to use the button to disable the interruption mechanism</q></cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=AI+Safety+Gridworlds&amp;rft.date=2017&amp;rft_id=info%3Aarxiv%2F1711.09883&amp;rft.aulast=Leike&amp;rft.aufirst=Jan&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-36"><span class="mw-cite-backlink"><b><a href="#cite_ref-36">^</a></b></span> <span class="reference-text"><cite class="citation news">Russell, Stuart (30 August 2017). <a rel="nofollow" class="external text" href="https://www.nature.com/articles/548520a">"Artificial intelligence: The future is superintelligent"</a>. <i>Nature</i>. pp.&#160;520–521. <a href="/wiki/Bibcode" title="Bibcode">Bibcode</a>:<a rel="nofollow" class="external text" href="https://ui.adsabs.harvard.edu/abs/2017Natur.548..520R">2017Natur.548..520R</a>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1038%2F548520a">10.1038/548520a</a><span class="reference-accessdate">. Retrieved <span class="nowrap">2 February</span> 2018</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Nature&amp;rft.atitle=Artificial+intelligence%3A+The+future+is+superintelligent&amp;rft.pages=520-521&amp;rft.date=2017-08-30&amp;rft_id=info%3Adoi%2F10.1038%2F548520a&amp;rft_id=info%3Abibcode%2F2017Natur.548..520R&amp;rft.aulast=Russell&amp;rft.aufirst=Stuart&amp;rft_id=https%3A%2F%2Fwww.nature.com%2Farticles%2F548520a&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-life_3.0-37"><span class="mw-cite-backlink">^ <a href="#cite_ref-life_3.0_37-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-life_3.0_37-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-life_3.0_37-2"><sup><i><b>c</b></i></sup></a></span> <span class="reference-text"><cite class="citation book"><a href="/wiki/Max_Tegmark" title="Max Tegmark">Max Tegmark</a> (2017). <a href="/wiki/Life_3.0:_Being_Human_in_the_Age_of_Artificial_Intelligence" class="mw-redirect" title="Life 3.0: Being Human in the Age of Artificial Intelligence"><i>Life 3.0: Being Human in the Age of Artificial Intelligence</i></a> (1st ed.). Mainstreaming AI Safety: Knopf. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/9780451485076" title="Special:BookSources/9780451485076"><bdi>9780451485076</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Life+3.0%3A+Being+Human+in+the+Age+of+Artificial+Intelligence&amp;rft.place=Mainstreaming+AI+Safety&amp;rft.edition=1st&amp;rft.pub=Knopf&amp;rft.date=2017&amp;rft.isbn=9780451485076&amp;rft.au=Max+Tegmark&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-38"><span class="mw-cite-backlink"><b><a href="#cite_ref-38">^</a></b></span> <span class="reference-text">Elliott, E. W. (2011). Physics of the Future: How Science Will Shape Human Destiny and Our Daily Lives by the Year 2100, by Michio Kaku. <i><a href="/wiki/Issues_in_Science_and_Technology" title="Issues in Science and Technology">Issues in Science and Technology</a></i>, 27(4), 90.</span>
</li>
<li id="cite_note-39"><span class="mw-cite-backlink"><b><a href="#cite_ref-39">^</a></b></span> <span class="reference-text"><cite class="citation book">Kaku, Michio (2011). <a href="/wiki/Physics_of_the_future" class="mw-redirect" title="Physics of the future"><i>Physics of the future: how science will shape human destiny and our daily lives by the year 2100</i></a>. New York: Doubleday. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-0-385-53080-4" title="Special:BookSources/978-0-385-53080-4"><bdi>978-0-385-53080-4</bdi></a>. <q>I personally believe that the most likely path is that we will build robots to be benevolent and friendly</q></cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Physics+of+the+future%3A+how+science+will+shape+human+destiny+and+our+daily+lives+by+the+year+2100&amp;rft.place=New+York&amp;rft.pub=Doubleday&amp;rft.date=2011&amp;rft.isbn=978-0-385-53080-4&amp;rft.aulast=Kaku&amp;rft.aufirst=Michio&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-40"><span class="mw-cite-backlink"><b><a href="#cite_ref-40">^</a></b></span> <span class="reference-text">Yudkowsky, E. (2011, August). Complex value systems in friendly AI. In International Conference on Artificial General Intelligence (pp. 388-393). Springer, Berlin, Heidelberg.</span>
</li>
<li id="cite_note-41"><span class="mw-cite-backlink"><b><a href="#cite_ref-41">^</a></b></span> <span class="reference-text"><cite class="citation web"><a href="/wiki/Stuart_J._Russell" title="Stuart J. Russell">Russell, Stuart</a> (2014). <a rel="nofollow" class="external text" href="http://edge.org/conversation/the-myth-of-ai#26015">"Of Myths and Moonshine"</a>. <i><a href="/wiki/Edge_Foundation,_Inc." title="Edge Foundation, Inc.">Edge</a></i><span class="reference-accessdate">. Retrieved <span class="nowrap">23 October</span> 2015</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Edge&amp;rft.atitle=Of+Myths+and+Moonshine&amp;rft.date=2014&amp;rft.aulast=Russell&amp;rft.aufirst=Stuart&amp;rft_id=http%3A%2F%2Fedge.org%2Fconversation%2Fthe-myth-of-ai%2326015&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-acm-42"><span class="mw-cite-backlink">^ <a href="#cite_ref-acm_42-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-acm_42-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal"><a href="/wiki/Eric_Horvitz" title="Eric Horvitz">Dietterich, Thomas</a>; Horvitz, Eric (2015). <a rel="nofollow" class="external text" href="http://research.microsoft.com/en-us/um/people/horvitz/CACM_Oct_2015-VP.pdf">"Rise of Concerns about AI: Reflections and Directions"</a> <span class="cs1-format">(PDF)</span>. <i><a href="/wiki/Communications_of_the_ACM" title="Communications of the ACM">Communications of the ACM</a></i>. <b>58</b> (10): 38–40. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1145%2F2770869">10.1145/2770869</a><span class="reference-accessdate">. Retrieved <span class="nowrap">23 October</span> 2015</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Communications+of+the+ACM&amp;rft.atitle=Rise+of+Concerns+about+AI%3A+Reflections+and+Directions&amp;rft.volume=58&amp;rft.issue=10&amp;rft.pages=38-40&amp;rft.date=2015&amp;rft_id=info%3Adoi%2F10.1145%2F2770869&amp;rft.aulast=Dietterich&amp;rft.aufirst=Thomas&amp;rft.au=Horvitz%2C+Eric&amp;rft_id=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2Fum%2Fpeople%2Fhorvitz%2FCACM_Oct_2015-VP.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-43"><span class="mw-cite-backlink"><b><a href="#cite_ref-43">^</a></b></span> <span class="reference-text"><cite class="citation journal">Yampolskiy, Roman V. (8 April 2014). "Utility function security in artificially intelligent agents". <i>Journal of Experimental &amp; Theoretical Artificial Intelligence</i>. <b>26</b> (3): 373–389. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1080%2F0952813X.2014.895114">10.1080/0952813X.2014.895114</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Experimental+%26+Theoretical+Artificial+Intelligence&amp;rft.atitle=Utility+function+security+in+artificially+intelligent+agents&amp;rft.volume=26&amp;rft.issue=3&amp;rft.pages=373-389&amp;rft.date=2014-04-08&amp;rft_id=info%3Adoi%2F10.1080%2F0952813X.2014.895114&amp;rft.aulast=Yampolskiy&amp;rft.aufirst=Roman+V.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-44"><span class="mw-cite-backlink"><b><a href="#cite_ref-44">^</a></b></span> <span class="reference-text"><cite class="citation journal">Lenat, Douglas (1982). "Eurisko: A Program That Learns New Heuristics and Domain Concepts The Nature of Heuristics III: Program Design and Results". <i>Artificial Intelligence</i> (Print). <b>21</b> (1–2): 61–98. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1016%2Fs0004-3702%2883%2980005-8">10.1016/s0004-3702(83)80005-8</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Artificial+Intelligence&amp;rft.atitle=Eurisko%3A+A+Program+That+Learns+New+Heuristics+and+Domain+Concepts+The+Nature+of+Heuristics+III%3A+Program+Design+and+Results&amp;rft.volume=21&amp;rft.issue=1%E2%80%932&amp;rft.pages=61-98&amp;rft.date=1982&amp;rft_id=info%3Adoi%2F10.1016%2Fs0004-3702%2883%2980005-8&amp;rft.aulast=Lenat&amp;rft.aufirst=Douglas&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-45"><span class="mw-cite-backlink"><b><a href="#cite_ref-45">^</a></b></span> <span class="reference-text">Haidt, Jonathan; Kesebir, Selin (2010) "Chapter 22: Morality" In Handbook of Social Psychology,  Fifth Edition, Hoboken NJ, Wiley, 2010, pp. 797-832.</span>
</li>
<li id="cite_note-46"><span class="mw-cite-backlink"><b><a href="#cite_ref-46">^</a></b></span> <span class="reference-text"><cite class="citation journal">Waser, Mark (2015). "Designing, Implementing and Enforcing a Coherent System of Laws, Ethics and Morals for Intelligent Machines (Including Humans)". <i>Procedia Computer Science</i> (Print). <b>71</b>: 106–111. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.1016%2Fj.procs.2015.12.213">10.1016/j.procs.2015.12.213</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Procedia+Computer+Science&amp;rft.atitle=Designing%2C+Implementing+and+Enforcing+a+Coherent+System+of+Laws%2C+Ethics+and+Morals+for+Intelligent+Machines+%28Including+Humans%29&amp;rft.volume=71&amp;rft.pages=106-111&amp;rft.date=2015&amp;rft_id=info%3Adoi%2F10.1016%2Fj.procs.2015.12.213&amp;rft.aulast=Waser&amp;rft.aufirst=Mark&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-47"><span class="mw-cite-backlink"><b><a href="#cite_ref-47">^</a></b></span> <span class="reference-text">Yudkowsky, Eliezer. "Complex value systems in friendly AI." In Artificial general intelligence, pp. 388-393. Springer Berlin Heidelberg, 2011.</span>
</li>
<li id="cite_note-shermer-48"><span class="mw-cite-backlink">^ <a href="#cite_ref-shermer_48-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-shermer_48-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation news">Shermer, Michael (1 March 2017). <a rel="nofollow" class="external text" href="https://www.scientificamerican.com/article/artificial-intelligence-is-not-a-threat-mdash-yet/">"Apocalypse AI"</a>. <i>Scientific American</i>. p.&#160;77. <a href="/wiki/Bibcode" title="Bibcode">Bibcode</a>:<a rel="nofollow" class="external text" href="https://ui.adsabs.harvard.edu/abs/2017SciAm.316c..77S">2017SciAm.316c..77S</a>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1038%2Fscientificamerican0317-77">10.1038/scientificamerican0317-77</a><span class="reference-accessdate">. Retrieved <span class="nowrap">27 November</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Scientific+American&amp;rft.atitle=Apocalypse+AI&amp;rft.pages=77&amp;rft.date=2017-03-01&amp;rft_id=info%3Adoi%2F10.1038%2Fscientificamerican0317-77&amp;rft_id=info%3Abibcode%2F2017SciAm.316c..77S&amp;rft.aulast=Shermer&amp;rft.aufirst=Michael&amp;rft_id=https%3A%2F%2Fwww.scientificamerican.com%2Farticle%2Fartificial-intelligence-is-not-a-threat-mdash-yet%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-49"><span class="mw-cite-backlink"><b><a href="#cite_ref-49">^</a></b></span> <span class="reference-text"><cite class="citation news">Wakefield, Jane (15 September 2015). <a rel="nofollow" class="external text" href="https://www.bbc.com/news/technology-34118481">"Why is Facebook investing in AI?"</a>. <i>BBC News</i><span class="reference-accessdate">. Retrieved <span class="nowrap">27 November</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=BBC+News&amp;rft.atitle=Why+is+Facebook+investing+in+AI%3F&amp;rft.date=2015-09-15&amp;rft.aulast=Wakefield&amp;rft.aufirst=Jane&amp;rft_id=https%3A%2F%2Fwww.bbc.com%2Fnews%2Ftechnology-34118481&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-50"><span class="mw-cite-backlink"><b><a href="#cite_ref-50">^</a></b></span> <span class="reference-text"><cite class="citation book">Bostrom, Nick (2014). <i>Superintelligence: Paths, Dangers, Strategies</i>. Oxford, United Kingdom: Oxford University Press. p.&#160;116. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-0-19-967811-2" title="Special:BookSources/978-0-19-967811-2"><bdi>978-0-19-967811-2</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Superintelligence%3A+Paths%2C+Dangers%2C+Strategies&amp;rft.place=Oxford%2C+United+Kingdom&amp;rft.pages=116&amp;rft.pub=Oxford+University+Press&amp;rft.date=2014&amp;rft.isbn=978-0-19-967811-2&amp;rft.aulast=Bostrom&amp;rft.aufirst=Nick&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-51"><span class="mw-cite-backlink"><b><a href="#cite_ref-51">^</a></b></span> <span class="reference-text"><cite class="citation web">Bostrom, Nick (2012). <a rel="nofollow" class="external text" href="http://www.nickbostrom.com/superintelligentwill.pdf">"Superintelligent Will"</a> <span class="cs1-format">(PDF)</span>. <i>Nick Bostrom</i>. Nick Bostrom<span class="reference-accessdate">. Retrieved <span class="nowrap">29 October</span> 2015</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Nick+Bostrom&amp;rft.atitle=Superintelligent+Will&amp;rft.date=2012&amp;rft.aulast=Bostrom&amp;rft.aufirst=Nick&amp;rft_id=http%3A%2F%2Fwww.nickbostrom.com%2Fsuperintelligentwill.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-armstrong-52"><span class="mw-cite-backlink">^ <a href="#cite_ref-armstrong_52-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-armstrong_52-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-armstrong_52-2"><sup><i><b>c</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal">Armstrong, Stuart (1 January 2013). <a rel="nofollow" class="external text" href="https://www.questia.com/library/journal/1P3-3195465391/general-purpose-intelligence-arguing-the-orthogonality">"General Purpose Intelligence: Arguing the Orthogonality Thesis"</a>. <i>Analysis and Metaphysics</i>. <b>12</b><span class="reference-accessdate">. Retrieved <span class="nowrap">2 April</span> 2020</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Analysis+and+Metaphysics&amp;rft.atitle=General+Purpose+Intelligence%3A+Arguing+the+Orthogonality+Thesis&amp;rft.volume=12&amp;rft.date=2013-01-01&amp;rft.aulast=Armstrong&amp;rft.aufirst=Stuart&amp;rft_id=https%3A%2F%2Fwww.questia.com%2Flibrary%2Fjournal%2F1P3-3195465391%2Fgeneral-purpose-intelligence-arguing-the-orthogonality&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/> Full text available <a rel="nofollow" class="external text" href="https://www.fhi.ox.ac.uk/wp-content/uploads/Orthogonality_Analysis_and_Metaethics-1.pdf">here</a>.</span>
</li>
<li id="cite_note-chorost-53"><span class="mw-cite-backlink">^ <a href="#cite_ref-chorost_53-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-chorost_53-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation magazine">Chorost, Michael (18 April 2016). <a rel="nofollow" class="external text" href="http://www.slate.com/articles/technology/future_tense/2016/04/the_philosophical_argument_against_artificial_intelligence_killing_us_all.html">"Let Artificial Intelligence Evolve"</a>. <i>Slate</i><span class="reference-accessdate">. Retrieved <span class="nowrap">27 November</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Slate&amp;rft.atitle=Let+Artificial+Intelligence+Evolve&amp;rft.date=2016-04-18&amp;rft.aulast=Chorost&amp;rft.aufirst=Michael&amp;rft_id=http%3A%2F%2Fwww.slate.com%2Farticles%2Ftechnology%2Ffuture_tense%2F2016%2F04%2Fthe_philosophical_argument_against_artificial_intelligence_killing_us_all.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-54"><span class="mw-cite-backlink"><b><a href="#cite_ref-54">^</a></b></span> <span class="reference-text">Waser, Mark. "Rational Universal Benevolence: Simpler, Safer, and Wiser Than 'Friendly AI'." Artificial General Intelligence. Springer Berlin Heidelberg, 2011. 153-162. "Terminal-goaled intelligences are short-lived but mono-maniacally dangerous and a correct basis for concern if anyone is smart enough to program high-intelligence and unwise enough to want a paperclip-maximizer."</span>
</li>
<li id="cite_note-55"><span class="mw-cite-backlink"><b><a href="#cite_ref-55">^</a></b></span> <span class="reference-text"><cite class="citation news">Koebler, Jason (2 February 2016). <a rel="nofollow" class="external text" href="http://motherboard.vice.com/read/will-superintelligent-ai-ignore-humans-instead-of-destroying-us">"Will Superintelligent AI Ignore Humans Instead of Destroying Us?"</a>. <i><a href="/wiki/Vice_Magazine" class="mw-redirect" title="Vice Magazine">Vice Magazine</a></i><span class="reference-accessdate">. Retrieved <span class="nowrap">3 February</span> 2016</span>. <q>This artificial intelligence is not a basically nice creature that has a strong drive for paperclips, which, so long as it's satisfied by being able to make lots of paperclips somewhere else, is then able to interact with you in a relaxed and carefree fashion where it can be nice with you," <a href="/wiki/Eliezer_Yudkowsky" title="Eliezer Yudkowsky">Yudkowsky</a> said. "Imagine a time machine that sends backward in time information about which choice always leads to the maximum number of paperclips in the future, and this choice is then output—that's what a <a href="/wiki/Paperclip_maximizer" class="mw-redirect" title="Paperclip maximizer">paperclip maximizer</a> is.</q></cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Vice+Magazine&amp;rft.atitle=Will+Superintelligent+AI+Ignore+Humans+Instead+of+Destroying+Us%3F&amp;rft.date=2016-02-02&amp;rft.aulast=Koebler&amp;rft.aufirst=Jason&amp;rft_id=http%3A%2F%2Fmotherboard.vice.com%2Fread%2Fwill-superintelligent-ai-ignore-humans-instead-of-destroying-us&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-56"><span class="mw-cite-backlink"><b><a href="#cite_ref-56">^</a></b></span> <span class="reference-text"><cite class="citation news"><a rel="nofollow" class="external text" href="https://www.wired.com/2009/08/real-life-decepticons-robots-learn-to-cheat/">"Real-Life Decepticons: Robots Learn to Cheat"</a>. <i><a href="/wiki/Wired_(magazine)" title="Wired (magazine)">Wired</a></i>. 18 August 2009<span class="reference-accessdate">. Retrieved <span class="nowrap">7 February</span> 2016</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Wired&amp;rft.atitle=Real-Life+Decepticons%3A+Robots+Learn+to+Cheat&amp;rft.date=2009-08-18&amp;rft_id=https%3A%2F%2Fwww.wired.com%2F2009%2F08%2Freal-life-decepticons-robots-learn-to-cheat%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-57"><span class="mw-cite-backlink"><b><a href="#cite_ref-57">^</a></b></span> <span class="reference-text">Cohen, Paul R., and Edward A. Feigenbaum, eds. The handbook of artificial intelligence. Vol. 3. Butterworth-Heinemann, 2014.</span>
</li>
<li id="cite_note-58"><span class="mw-cite-backlink"><b><a href="#cite_ref-58">^</a></b></span> <span class="reference-text"><cite class="citation journal">Baum, Seth (30 September 2018). "Countering Superintelligence Misinformation". <i>Information</i>. <b>9</b> (10): 244. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.3390%2Finfo9100244">10.3390/info9100244</a></span>. <a href="/wiki/International_Standard_Serial_Number" title="International Standard Serial Number">ISSN</a>&#160;<a rel="nofollow" class="external text" href="//www.worldcat.org/issn/2078-2489">2078-2489</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Information&amp;rft.atitle=Countering+Superintelligence+Misinformation&amp;rft.volume=9&amp;rft.issue=10&amp;rft.pages=244&amp;rft.date=2018-09-30&amp;rft_id=info%3Adoi%2F10.3390%2Finfo9100244&amp;rft.issn=2078-2489&amp;rft.aulast=Baum&amp;rft.aufirst=Seth&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-59"><span class="mw-cite-backlink"><b><a href="#cite_ref-59">^</a></b></span> <span class="reference-text"><cite class="citation web"><a rel="nofollow" class="external text" href="https://www.edge.org/conversation/jaron_lanier-the-myth-of-ai">"The Myth Of AI | Edge.org"</a>. <i>www.edge.org</i><span class="reference-accessdate">. Retrieved <span class="nowrap">11 March</span> 2020</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=www.edge.org&amp;rft.atitle=The+Myth+Of+AI+%7C+Edge.org&amp;rft_id=https%3A%2F%2Fwww.edge.org%2Fconversation%2Fjaron_lanier-the-myth-of-ai&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-:0-60"><span class="mw-cite-backlink">^ <a href="#cite_ref-:0_60-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:0_60-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation book">Scornavacchi, Matthew (2015). <a rel="nofollow" class="external text" href="https://apps.dtic.mil/dtic/tr/fulltext/u2/a622649.pdf"><i>Superintelligence, Humans, and War</i></a> <span class="cs1-format">(PDF)</span>. Norfolk, Virginia: National Defense University, Joint Forces Staff College.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Superintelligence%2C+Humans%2C+and+War&amp;rft.place=Norfolk%2C+Virginia&amp;rft.pub=National+Defense+University%2C+Joint+Forces+Staff+College&amp;rft.date=2015&amp;rft.aulast=Scornavacchi&amp;rft.aufirst=Matthew&amp;rft_id=https%3A%2F%2Fapps.dtic.mil%2Fdtic%2Ftr%2Ffulltext%2Fu2%2Fa622649.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-61"><span class="mw-cite-backlink"><b><a href="#cite_ref-61">^</a></b></span> <span class="reference-text"><cite class="citation news"><a rel="nofollow" class="external text" href="https://www.telegraph.co.uk/technology/news/11837157/Should-humans-fear-the-rise-of-the-machine.html">"Should humans fear the rise of the machine?"</a>. <i><a href="/wiki/The_Telegraph_(UK)" class="mw-redirect" title="The Telegraph (UK)">The Telegraph (UK)</a></i>. 1 September 2015<span class="reference-accessdate">. Retrieved <span class="nowrap">7 February</span> 2016</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+Telegraph+%28UK%29&amp;rft.atitle=Should+humans+fear+the+rise+of+the+machine%3F&amp;rft.date=2015-09-01&amp;rft_id=https%3A%2F%2Fwww.telegraph.co.uk%2Ftechnology%2Fnews%2F11837157%2FShould-humans-fear-the-rise-of-the-machine.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-:2-62"><span class="mw-cite-backlink">^ <a href="#cite_ref-:2_62-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:2_62-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-:2_62-2"><sup><i><b>c</b></i></sup></a></span> <span class="reference-text"><cite id="CITEREFBostrom,_Nick,_1973-_author." class="citation">Bostrom, Nick, 1973- author., <i>Superintelligence&#160;: paths, dangers, strategies</i>, <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-1-5012-2774-5" title="Special:BookSources/978-1-5012-2774-5"><bdi>978-1-5012-2774-5</bdi></a>, <a href="/wiki/OCLC" title="OCLC">OCLC</a>&#160;<a rel="nofollow" class="external text" href="//www.worldcat.org/oclc/1061147095">1061147095</a></cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Superintelligence+%3A+paths%2C+dangers%2C+strategies&amp;rft_id=info%3Aoclcnum%2F1061147095&amp;rft.isbn=978-1-5012-2774-5&amp;rft.au=Bostrom%2C+Nick%2C+1973-+author.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><span class="cs1-maint citation-comment">CS1 maint: multiple names: authors list (<a href="/wiki/Category:CS1_maint:_multiple_names:_authors_list" title="Category:CS1 maint: multiple names: authors list">link</a>)</span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-Cave_2018-63"><span class="mw-cite-backlink">^ <a href="#cite_ref-Cave_2018_63-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-Cave_2018_63-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal">Cave, Stephen; ÓhÉigeartaigh, Seán S. (2018). "An AI Race for Strategic Advantage". <i>Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society - AIES '18</i>. New York, New York, USA: ACM Press: 36–40. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.1145%2F3278721.3278780">10.1145/3278721.3278780</a></span>. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-1-4503-6012-8" title="Special:BookSources/978-1-4503-6012-8"><bdi>978-1-4503-6012-8</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Proceedings+of+the+2018+AAAI%2FACM+Conference+on+AI%2C+Ethics%2C+and+Society+-+AIES+%2718&amp;rft.atitle=An+AI+Race+for+Strategic+Advantage&amp;rft.pages=36-40&amp;rft.date=2018&amp;rft_id=info%3Adoi%2F10.1145%2F3278721.3278780&amp;rft.isbn=978-1-4503-6012-8&amp;rft.aulast=Cave&amp;rft.aufirst=Stephen&amp;rft.au=%C3%93h%C3%89igeartaigh%2C+Se%C3%A1n+S.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-:4-64"><span class="mw-cite-backlink">^ <a href="#cite_ref-:4_64-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:4_64-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-:4_64-2"><sup><i><b>c</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal">Sotala, Kaj; Yampolskiy, Roman V (19 December 2014). "Responses to catastrophic AGI risk: a survey". <i>Physica Scripta</i>. <b>90</b> (1): 12. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.1088%2F0031-8949%2F90%2F1%2F018001">10.1088/0031-8949/90/1/018001</a></span>. <a href="/wiki/International_Standard_Serial_Number" title="International Standard Serial Number">ISSN</a>&#160;<a rel="nofollow" class="external text" href="//www.worldcat.org/issn/0031-8949">0031-8949</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Physica+Scripta&amp;rft.atitle=Responses+to+catastrophic+AGI+risk%3A+a+survey&amp;rft.volume=90&amp;rft.issue=1&amp;rft.pages=12&amp;rft.date=2014-12-19&amp;rft_id=info%3Adoi%2F10.1088%2F0031-8949%2F90%2F1%2F018001&amp;rft.issn=0031-8949&amp;rft.aulast=Sotala&amp;rft.aufirst=Kaj&amp;rft.au=Yampolskiy%2C+Roman+V&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-65"><span class="mw-cite-backlink"><b><a href="#cite_ref-65">^</a></b></span> <span class="reference-text"><cite class="citation web">Kania, Gregory Allen, Elsa B. <a rel="nofollow" class="external text" href="https://foreignpolicy.com/2017/09/08/china-is-using-americas-own-plan-to-dominate-the-future-of-artificial-intelligence/">"China Is Using America's Own Plan to Dominate the Future of Artificial Intelligence"</a>. <i>Foreign Policy</i><span class="reference-accessdate">. Retrieved <span class="nowrap">11 March</span> 2020</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Foreign+Policy&amp;rft.atitle=China+Is+Using+America%27s+Own+Plan+to+Dominate+the+Future+of+Artificial+Intelligence&amp;rft.aulast=Kania&amp;rft.aufirst=Gregory+Allen%2C+Elsa+B.&amp;rft_id=https%3A%2F%2Fforeignpolicy.com%2F2017%2F09%2F08%2Fchina-is-using-americas-own-plan-to-dominate-the-future-of-artificial-intelligence%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-:1-66"><span class="mw-cite-backlink"><b><a href="#cite_ref-:1_66-0">^</a></b></span> <span class="reference-text"><cite class="citation journal">Cave, Stephen; ÓhÉigeartaigh, Seán S. (2018). "An AI Race for Strategic Advantage". <i>Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society - AIES '18</i>. New York, New York, USA: ACM Press: 2. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.1145%2F3278721.3278780">10.1145/3278721.3278780</a></span>. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-1-4503-6012-8" title="Special:BookSources/978-1-4503-6012-8"><bdi>978-1-4503-6012-8</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Proceedings+of+the+2018+AAAI%2FACM+Conference+on+AI%2C+Ethics%2C+and+Society+-+AIES+%2718&amp;rft.atitle=An+AI+Race+for+Strategic+Advantage&amp;rft.pages=2&amp;rft.date=2018&amp;rft_id=info%3Adoi%2F10.1145%2F3278721.3278780&amp;rft.isbn=978-1-4503-6012-8&amp;rft.aulast=Cave&amp;rft.aufirst=Stephen&amp;rft.au=%C3%93h%C3%89igeartaigh%2C+Se%C3%A1n+S.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-67"><span class="mw-cite-backlink"><b><a href="#cite_ref-67">^</a></b></span> <span class="reference-text"><cite class="citation web">Hendry, Erica R. (21 January 2014). <a rel="nofollow" class="external text" href="http://www.smithsonianmag.com/innovation/what-happens-when-artificial-intelligence-turns-us-180949415/?no-ist">"What Happens When Artificial Intelligence Turns On Us?"</a>. <i>Smithsonian</i><span class="reference-accessdate">. Retrieved <span class="nowrap">26 October</span> 2015</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Smithsonian&amp;rft.atitle=What+Happens+When+Artificial+Intelligence+Turns+On+Us%3F&amp;rft.date=2014-01-21&amp;rft.aulast=Hendry&amp;rft.aufirst=Erica+R.&amp;rft_id=http%3A%2F%2Fwww.smithsonianmag.com%2Finnovation%2Fwhat-happens-when-artificial-intelligence-turns-us-180949415%2F%3Fno-ist&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-68"><span class="mw-cite-backlink"><b><a href="#cite_ref-68">^</a></b></span> <span class="reference-text"><cite class="citation book">Pistono, Federico Yampolskiy, Roman V. (9 May 2016). <i>Unethical Research: How to Create a Malevolent Artificial Intelligence</i>. <a href="/wiki/OCLC" title="OCLC">OCLC</a>&#160;<a rel="nofollow" class="external text" href="//www.worldcat.org/oclc/1106238048">1106238048</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Unethical+Research%3A+How+to+Create+a+Malevolent+Artificial+Intelligence&amp;rft.date=2016-05-09&amp;rft_id=info%3Aoclcnum%2F1106238048&amp;rft.au=Pistono%2C+Federico+Yampolskiy%2C+Roman+V.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><span class="cs1-maint citation-comment">CS1 maint: multiple names: authors list (<a href="/wiki/Category:CS1_maint:_multiple_names:_authors_list" title="Category:CS1 maint: multiple names: authors list">link</a>)</span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-69"><span class="mw-cite-backlink"><b><a href="#cite_ref-69">^</a></b></span> <span class="reference-text"><cite class="citation journal">Haney, Brian Seamus (2018). "The Perils &amp; Promises of Artificial General Intelligence". <i>SSRN Working Paper Series</i>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.2139%2Fssrn.3261254">10.2139/ssrn.3261254</a>. <a href="/wiki/International_Standard_Serial_Number" title="International Standard Serial Number">ISSN</a>&#160;<a rel="nofollow" class="external text" href="//www.worldcat.org/issn/1556-5068">1556-5068</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=SSRN+Working+Paper+Series&amp;rft.atitle=The+Perils+%26amp%3B+Promises+of+Artificial+General+Intelligence&amp;rft.date=2018&amp;rft_id=info%3Adoi%2F10.2139%2Fssrn.3261254&amp;rft.issn=1556-5068&amp;rft.aulast=Haney&amp;rft.aufirst=Brian+Seamus&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-70"><span class="mw-cite-backlink"><b><a href="#cite_ref-70">^</a></b></span> <span class="reference-text"><cite class="citation journal">Turchin, Alexey; Denkenberger, David (3 May 2018). <a rel="nofollow" class="external text" href="https://dx.doi.org/10.1007/s00146-018-0845-5">"Classification of global catastrophic risks connected with artificial intelligence"</a>. <i>AI &amp; SOCIETY</i>. <b>35</b> (1): 147–163. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1007%2Fs00146-018-0845-5">10.1007/s00146-018-0845-5</a>. <a href="/wiki/International_Standard_Serial_Number" title="International Standard Serial Number">ISSN</a>&#160;<a rel="nofollow" class="external text" href="//www.worldcat.org/issn/0951-5666">0951-5666</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=AI+%26+SOCIETY&amp;rft.atitle=Classification+of+global+catastrophic+risks+connected+with+artificial+intelligence&amp;rft.volume=35&amp;rft.issue=1&amp;rft.pages=147-163&amp;rft.date=2018-05-03&amp;rft_id=info%3Adoi%2F10.1007%2Fs00146-018-0845-5&amp;rft.issn=0951-5666&amp;rft.aulast=Turchin&amp;rft.aufirst=Alexey&amp;rft.au=Denkenberger%2C+David&amp;rft_id=http%3A%2F%2Fdx.doi.org%2F10.1007%2Fs00146-018-0845-5&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-71"><span class="mw-cite-backlink"><b><a href="#cite_ref-71">^</a></b></span> <span class="reference-text"><cite class="citation book">Miller, James D. (2015). <i>Singularity Rising: Surviving and Thriving in a Smarter&#160;; Richer&#160;; and More Dangerous World</i>. Benbella Books. <a href="/wiki/OCLC" title="OCLC">OCLC</a>&#160;<a rel="nofollow" class="external text" href="//www.worldcat.org/oclc/942647155">942647155</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Singularity+Rising%3A+Surviving+and+Thriving+in+a+Smarter+%3B+Richer+%3B+and+More+Dangerous+World&amp;rft.pub=Benbella+Books&amp;rft.date=2015&amp;rft_id=info%3Aoclcnum%2F942647155&amp;rft.au=Miller%2C+James+D.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-72"><span class="mw-cite-backlink"><b><a href="#cite_ref-72">^</a></b></span> <span class="reference-text">Harvnb|Simon|1965|p=96 quoted in Harvnb|Crevier|1993|p=109</span>
</li>
<li id="cite_note-73"><span class="mw-cite-backlink"><b><a href="#cite_ref-73">^</a></b></span> <span class="reference-text"><cite class="citation news">Winfield, Alan. <a rel="nofollow" class="external text" href="https://www.theguardian.com/technology/2014/aug/10/artificial-intelligence-will-not-become-a-frankensteins-monster-ian-winfield">"Artificial intelligence will not turn into a Frankenstein's monster"</a>. <i><a href="/wiki/The_Guardian" title="The Guardian">The Guardian</a></i><span class="reference-accessdate">. Retrieved <span class="nowrap">17 September</span> 2014</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+Guardian&amp;rft.atitle=Artificial+intelligence+will+not+turn+into+a+Frankenstein%27s+monster&amp;rft.aulast=Winfield&amp;rft.aufirst=Alan&amp;rft_id=https%3A%2F%2Fwww.theguardian.com%2Ftechnology%2F2014%2Faug%2F10%2Fartificial-intelligence-will-not-become-a-frankensteins-monster-ian-winfield&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-new_yorker_doomsday-74"><span class="mw-cite-backlink">^ <a href="#cite_ref-new_yorker_doomsday_74-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-new_yorker_doomsday_74-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation news">Raffi Khatchadourian (23 November 2015). <a rel="nofollow" class="external text" href="https://www.newyorker.com/magazine/2015/11/23/doomsday-invention-artificial-intelligence-nick-bostrom">"The Doomsday Invention: Will artificial intelligence bring us utopia or destruction?"</a>. <i><a href="/wiki/The_New_Yorker_(magazine)" class="mw-redirect" title="The New Yorker (magazine)">The New Yorker</a></i><span class="reference-accessdate">. Retrieved <span class="nowrap">7 February</span> 2016</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+New+Yorker&amp;rft.atitle=The+Doomsday+Invention%3A+Will+artificial+intelligence+bring+us+utopia+or+destruction%3F&amp;rft.date=2015-11-23&amp;rft.au=Raffi+Khatchadourian&amp;rft_id=https%3A%2F%2Fwww.newyorker.com%2Fmagazine%2F2015%2F11%2F23%2Fdoomsday-invention-artificial-intelligence-nick-bostrom&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-75"><span class="mw-cite-backlink"><b><a href="#cite_ref-75">^</a></b></span> <span class="reference-text">Müller, V. C., &amp; Bostrom, N. (2016). Future progress in artificial intelligence: A survey of expert opinion. In Fundamental issues of artificial intelligence (pp. 555-572). Springer, Cham.</span>
</li>
<li id="cite_note-76"><span class="mw-cite-backlink"><b><a href="#cite_ref-76">^</a></b></span> <span class="reference-text"><cite class="citation news">Dina Bass; Jack Clark (5 February 2015). <a rel="nofollow" class="external text" href="https://www.bloomberg.com/news/articles/2015-02-04/is-elon-musk-right-about-ai-researchers-don-t-think-so">"Is Elon Musk Right About AI? Researchers Don't Think So: To quell fears of artificial intelligence running amok, supporters want to give the field an image makeover"</a>. <i><a href="/wiki/Bloomberg_News" title="Bloomberg News">Bloomberg News</a></i><span class="reference-accessdate">. Retrieved <span class="nowrap">7 February</span> 2016</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Bloomberg+News&amp;rft.atitle=Is+Elon+Musk+Right+About+AI%3F+Researchers+Don%27t+Think+So%3A+To+quell+fears+of+artificial+intelligence+running+amok%2C+supporters+want+to+give+the+field+an+image+makeover&amp;rft.date=2015-02-05&amp;rft.au=Dina+Bass&amp;rft.au=Jack+Clark&amp;rft_id=https%3A%2F%2Fwww.bloomberg.com%2Fnews%2Farticles%2F2015-02-04%2Fis-elon-musk-right-about-ai-researchers-don-t-think-so&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-77"><span class="mw-cite-backlink"><b><a href="#cite_ref-77">^</a></b></span> <span class="reference-text"><cite class="citation news">Elkus, Adam (31 October 2014). <a rel="nofollow" class="external text" href="http://www.slate.com/articles/technology/future_tense/2014/10/elon_musk_artificial_intelligence_why_you_shouldn_t_be_afraid_of_ai.html">"Don't Fear Artificial Intelligence"</a>. <i><a href="/wiki/Slate_(magazine)" title="Slate (magazine)">Slate</a></i><span class="reference-accessdate">. Retrieved <span class="nowrap">15 May</span> 2016</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Slate&amp;rft.atitle=Don%27t+Fear+Artificial+Intelligence&amp;rft.date=2014-10-31&amp;rft.aulast=Elkus&amp;rft.aufirst=Adam&amp;rft_id=http%3A%2F%2Fwww.slate.com%2Farticles%2Ftechnology%2Ffuture_tense%2F2014%2F10%2Felon_musk_artificial_intelligence_why_you_shouldn_t_be_afraid_of_ai.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-78"><span class="mw-cite-backlink"><b><a href="#cite_ref-78">^</a></b></span> <span class="reference-text"><a rel="nofollow" class="external text" href="https://itif.org/publications/2016/01/19/artificial-intelligence-alarmists-win-itif%E2%80%99s-annual-luddite-award">Artificial Intelligence Alarmists Win ITIF’s Annual Luddite Award</a>, ITIF Website, 19 January 2016</span>
</li>
<li id="cite_note-79"><span class="mw-cite-backlink"><b><a href="#cite_ref-79">^</a></b></span> <span class="reference-text"><cite class="citation news"><a rel="nofollow" class="external text" href="https://www.independent.co.uk/life-style/gadgets-and-tech/news/elon-musk-stephen-hawking-luddite-award-of-the-year-itif-a6821921.html">"<span class="cs1-kern-left">'</span>Artificial intelligence alarmists' like Elon Musk and Stephen Hawking win 'Luddite of the Year' award"</a>. <i><a href="/wiki/The_Independent_(UK)" class="mw-redirect" title="The Independent (UK)">The Independent (UK)</a></i>. 19 January 2016<span class="reference-accessdate">. Retrieved <span class="nowrap">7 February</span> 2016</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+Independent+%28UK%29&amp;rft.atitle=%27Artificial+intelligence+alarmists%27+like+Elon+Musk+and+Stephen+Hawking+win+%27Luddite+of+the+Year%27+award&amp;rft.date=2016-01-19&amp;rft_id=https%3A%2F%2Fwww.independent.co.uk%2Flife-style%2Fgadgets-and-tech%2Fnews%2Felon-musk-stephen-hawking-luddite-award-of-the-year-itif-a6821921.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-80"><span class="mw-cite-backlink"><b><a href="#cite_ref-80">^</a></b></span> <span class="reference-text"><cite class="citation web">Garner, Rochelle. <a rel="nofollow" class="external text" href="https://www.cnet.com/news/elon-musk-stephen-hawking-win-annual-luddite-award/">"Elon Musk, Stephen Hawking win Luddite award as AI 'alarmists<span class="cs1-kern-right">'</span>"</a>. <i>CNET</i><span class="reference-accessdate">. Retrieved <span class="nowrap">7 February</span> 2016</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=CNET&amp;rft.atitle=Elon+Musk%2C+Stephen+Hawking+win+Luddite+award+as+AI+%27alarmists%27&amp;rft.aulast=Garner&amp;rft.aufirst=Rochelle&amp;rft_id=https%3A%2F%2Fwww.cnet.com%2Fnews%2Felon-musk-stephen-hawking-win-annual-luddite-award%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-nature_anticipating-81"><span class="mw-cite-backlink"><b><a href="#cite_ref-nature_anticipating_81-0">^</a></b></span> <span class="reference-text"><cite class="citation journal">"Anticipating artificial intelligence". <i>Nature</i>. <b>532</b> (7600): 413. 26 April 2016. <a href="/wiki/Bibcode" title="Bibcode">Bibcode</a>:<a rel="nofollow" class="external text" href="https://ui.adsabs.harvard.edu/abs/2016Natur.532Q.413.">2016Natur.532Q.413.</a>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.1038%2F532413a">10.1038/532413a</a></span>. <a href="/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&#160;<a rel="nofollow" class="external text" href="//pubmed.ncbi.nlm.nih.gov/27121801">27121801</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Nature&amp;rft.atitle=Anticipating+artificial+intelligence&amp;rft.volume=532&amp;rft.issue=7600&amp;rft.pages=413&amp;rft.date=2016-04-26&amp;rft_id=info%3Apmid%2F27121801&amp;rft_id=info%3Adoi%2F10.1038%2F532413a&amp;rft_id=info%3Abibcode%2F2016Natur.532Q.413.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-82"><span class="mw-cite-backlink"><b><a href="#cite_ref-82">^</a></b></span> <span class="reference-text"><cite class="citation news"><a href="/wiki/Murray_Shanahan" title="Murray Shanahan">Murray Shanahan</a> (3 November 2015). <a rel="nofollow" class="external text" href="https://www.washingtonpost.com/news/in-theory/wp/2015/11/03/machines-may-seem-intelligent-but-itll-be-a-while-before-they-actually-are/">"Machines may seem intelligent, but it'll be a while before they actually are"</a>. <i><a href="/wiki/The_Washington_Post" title="The Washington Post">The Washington Post</a></i><span class="reference-accessdate">. Retrieved <span class="nowrap">15 May</span> 2016</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+Washington+Post&amp;rft.atitle=Machines+may+seem+intelligent%2C+but+it%27ll+be+a+while+before+they+actually+are&amp;rft.date=2015-11-03&amp;rft.au=Murray+Shanahan&amp;rft_id=https%3A%2F%2Fwww.washingtonpost.com%2Fnews%2Fin-theory%2Fwp%2F2015%2F11%2F03%2Fmachines-may-seem-intelligent-but-itll-be-a-while-before-they-actually-are%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-83"><span class="mw-cite-backlink"><b><a href="#cite_ref-83">^</a></b></span> <span class="reference-text"><cite class="citation web"><a rel="nofollow" class="external text" href="https://futureoflife.org/ai-principles/">"AI Principles"</a>. <i><a href="/wiki/Future_of_Life_Institute" title="Future of Life Institute">Future of Life Institute</a></i><span class="reference-accessdate">. Retrieved <span class="nowrap">11 December</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Future+of+Life+Institute&amp;rft.atitle=AI+Principles&amp;rft_id=https%3A%2F%2Ffutureoflife.org%2Fai-principles%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-84"><span class="mw-cite-backlink"><b><a href="#cite_ref-84">^</a></b></span> <span class="reference-text"><cite class="citation news"><a rel="nofollow" class="external text" href="http://www.newsweek.com/ai-asilomar-principles-artificial-intelligence-elon-musk-550525">"Elon Musk and Stephen Hawking warn of artificial intelligence arms race"</a>. <i><a href="/wiki/Newsweek" title="Newsweek">Newsweek</a></i>. 31 January 2017<span class="reference-accessdate">. Retrieved <span class="nowrap">11 December</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Newsweek&amp;rft.atitle=Elon+Musk+and+Stephen+Hawking+warn+of+artificial+intelligence+arms+race&amp;rft.date=2017-01-31&amp;rft_id=http%3A%2F%2Fwww.newsweek.com%2Fai-asilomar-principles-artificial-intelligence-elon-musk-550525&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-85"><span class="mw-cite-backlink"><b><a href="#cite_ref-85">^</a></b></span> <span class="reference-text"><cite class="citation book"><a href="/wiki/Nick_Bostrom" title="Nick Bostrom">Bostrom, Nick</a> (2016). "New Epilogue to the Paperback Edition". <a href="/wiki/Superintelligence:_Paths,_Dangers,_Strategies" title="Superintelligence: Paths, Dangers, Strategies"><i>Superintelligence: Paths, Dangers, Strategies</i></a> (Paperback ed.).</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=New+Epilogue+to+the+Paperback+Edition&amp;rft.btitle=Superintelligence%3A+Paths%2C+Dangers%2C+Strategies&amp;rft.edition=Paperback&amp;rft.date=2016&amp;rft.aulast=Bostrom&amp;rft.aufirst=Nick&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-86"><span class="mw-cite-backlink"><b><a href="#cite_ref-86">^</a></b></span> <span class="reference-text"><cite class="citation book"><a href="/wiki/Martin_Ford_(author)" title="Martin Ford (author)">Martin Ford</a> (2015). "Chapter 9: Super-intelligence and the Singularity". <a href="/wiki/Rise_of_the_Robots:_Technology_and_the_Threat_of_a_Jobless_Future" class="mw-redirect" title="Rise of the Robots: Technology and the Threat of a Jobless Future"><i>Rise of the Robots: Technology and the Threat of a Jobless Future</i></a>. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/9780465059997" title="Special:BookSources/9780465059997"><bdi>9780465059997</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Chapter+9%3A+Super-intelligence+and+the+Singularity&amp;rft.btitle=Rise+of+the+Robots%3A+Technology+and+the+Threat+of+a+Jobless+Future&amp;rft.date=2015&amp;rft.isbn=9780465059997&amp;rft.au=Martin+Ford&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-87"><span class="mw-cite-backlink"><b><a href="#cite_ref-87">^</a></b></span> <span class="reference-text"><cite class="citation arxiv">Grace, Katja; Salvatier, John; Dafoe, Allan; Zhang, Baobao; Evans, Owain (24 May 2017). "When Will AI Exceed Human Performance? Evidence from AI Experts". <a href="/wiki/ArXiv" title="ArXiv">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//arxiv.org/abs/1705.08807">1705.08807</a></span> [<a rel="nofollow" class="external text" href="//arxiv.org/archive/cs.AI">cs.AI</a>].</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=When+Will+AI+Exceed+Human+Performance%3F+Evidence+from+AI+Experts&amp;rft.date=2017-05-24&amp;rft_id=info%3Aarxiv%2F1705.08807&amp;rft.aulast=Grace&amp;rft.aufirst=Katja&amp;rft.au=Salvatier%2C+John&amp;rft.au=Dafoe%2C+Allan&amp;rft.au=Zhang%2C+Baobao&amp;rft.au=Evans%2C+Owain&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-BBC_News-88"><span class="mw-cite-backlink">^ <a href="#cite_ref-BBC_News_88-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-BBC_News_88-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation news">Rawlinson, Kevin (29 January 2015). <a rel="nofollow" class="external text" href="https://www.bbc.co.uk/news/31047780">"Microsoft's Bill Gates insists AI is a threat"</a>. <i><a href="/wiki/BBC_News" title="BBC News">BBC News</a></i><span class="reference-accessdate">. Retrieved <span class="nowrap">30 January</span> 2015</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=BBC+News&amp;rft.atitle=Microsoft%27s+Bill+Gates+insists+AI+is+a+threat&amp;rft.date=2015-01-29&amp;rft.aulast=Rawlinson&amp;rft.aufirst=Kevin&amp;rft_id=https%3A%2F%2Fwww.bbc.co.uk%2Fnews%2F31047780&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-physica_scripta-89"><span class="mw-cite-backlink">^ <a href="#cite_ref-physica_scripta_89-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-physica_scripta_89-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal">Kaj Sotala; <a href="/wiki/Roman_Yampolskiy" title="Roman Yampolskiy">Roman Yampolskiy</a> (19 December 2014). "Responses to catastrophic AGI risk: a survey". <i><a href="/wiki/Physica_Scripta" title="Physica Scripta">Physica Scripta</a></i>. <b>90</b> (1).</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Physica+Scripta&amp;rft.atitle=Responses+to+catastrophic+AGI+risk%3A+a+survey&amp;rft.volume=90&amp;rft.issue=1&amp;rft.date=2014-12-19&amp;rft.au=Kaj+Sotala&amp;rft.au=Roman+Yampolskiy&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-atlantic-but-what-90"><span class="mw-cite-backlink">^ <a href="#cite_ref-atlantic-but-what_90-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-atlantic-but-what_90-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-atlantic-but-what_90-2"><sup><i><b>c</b></i></sup></a></span> <span class="reference-text"><cite class="citation magazine"><a rel="nofollow" class="external text" href="https://www.theatlantic.com/health/archive/2014/05/but-what-does-the-end-of-humanity-mean-for-me/361931/">"But What Would the End of Humanity Mean for Me?"</a>. <i>The Atlantic</i>. 9 May 2014<span class="reference-accessdate">. Retrieved <span class="nowrap">12 December</span> 2015</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+Atlantic&amp;rft.atitle=But+What+Would+the+End+of+Humanity+Mean+for+Me%3F&amp;rft.date=2014-05-09&amp;rft_id=https%3A%2F%2Fwww.theatlantic.com%2Fhealth%2Farchive%2F2014%2F05%2Fbut-what-does-the-end-of-humanity-mean-for-me%2F361931%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-91"><span class="mw-cite-backlink"><b><a href="#cite_ref-91">^</a></b></span> <span class="reference-text"><cite class="citation news"><a rel="nofollow" class="external text" href="https://spectrum.ieee.org/computing/hardware/tech-luminaries-address-singularity">"Tech Luminaries Address Singularity"</a>. <i>IEEE Spectrum: Technology, Engineering, and Science News</i> (SPECIAL REPORT: THE SINGULARITY). 1 June 2008<span class="reference-accessdate">. Retrieved <span class="nowrap">8 April</span> 2020</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=IEEE+Spectrum%3A+Technology%2C+Engineering%2C+and+Science+News&amp;rft.atitle=Tech+Luminaries+Address+Singularity&amp;rft.issue=SPECIAL+REPORT%3A+THE+SINGULARITY&amp;rft.date=2008-06-01&amp;rft_id=https%3A%2F%2Fspectrum.ieee.org%2Fcomputing%2Fhardware%2Ftech-luminaries-address-singularity&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-92"><span class="mw-cite-backlink"><b><a href="#cite_ref-92">^</a></b></span> <span class="reference-text"><a rel="nofollow" class="external free" href="http://intelligence.org/files/AIFoomDebate.pdf">http://intelligence.org/files/AIFoomDebate.pdf</a></span>
</li>
<li id="cite_note-93"><span class="mw-cite-backlink"><b><a href="#cite_ref-93">^</a></b></span> <span class="reference-text"><cite class="citation web"><a rel="nofollow" class="external text" href="http://www.overcomingbias.com/2014/07/30855.html">"Overcoming Bias&#160;: I Still Don't Get Foom"</a>. <i>www.overcomingbias.com</i><span class="reference-accessdate">. Retrieved <span class="nowrap">20 September</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=www.overcomingbias.com&amp;rft.atitle=Overcoming+Bias+%3A+I+Still+Don%27t+Get+Foom&amp;rft_id=http%3A%2F%2Fwww.overcomingbias.com%2F2014%2F07%2F30855.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-94"><span class="mw-cite-backlink"><b><a href="#cite_ref-94">^</a></b></span> <span class="reference-text"><cite class="citation web"><a rel="nofollow" class="external text" href="http://www.overcomingbias.com/2011/07/debating-yudkowsky.html">"Overcoming Bias&#160;: Debating Yudkowsky"</a>. <i>www.overcomingbias.com</i><span class="reference-accessdate">. Retrieved <span class="nowrap">20 September</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=www.overcomingbias.com&amp;rft.atitle=Overcoming+Bias+%3A+Debating+Yudkowsky&amp;rft_id=http%3A%2F%2Fwww.overcomingbias.com%2F2011%2F07%2Fdebating-yudkowsky.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-95"><span class="mw-cite-backlink"><b><a href="#cite_ref-95">^</a></b></span> <span class="reference-text"><cite class="citation web"><a rel="nofollow" class="external text" href="https://www.overcomingbias.com/2017/08/foom-justifies-ai-risk-efforts-now.html">"Overcoming Bias&#160;: Foom Justifies AI Risk Efforts Now"</a>. <i>www.overcomingbias.com</i><span class="reference-accessdate">. Retrieved <span class="nowrap">20 September</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=www.overcomingbias.com&amp;rft.atitle=Overcoming+Bias+%3A+Foom+Justifies+AI+Risk+Efforts+Now&amp;rft_id=https%3A%2F%2Fwww.overcomingbias.com%2F2017%2F08%2Ffoom-justifies-ai-risk-efforts-now.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-96"><span class="mw-cite-backlink"><b><a href="#cite_ref-96">^</a></b></span> <span class="reference-text"><cite class="citation web"><a rel="nofollow" class="external text" href="http://www.overcomingbias.com/2011/06/the-betterness-explosion.html">"Overcoming Bias&#160;: The Betterness Explosion"</a>. <i>www.overcomingbias.com</i><span class="reference-accessdate">. Retrieved <span class="nowrap">20 September</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=www.overcomingbias.com&amp;rft.atitle=Overcoming+Bias+%3A+The+Betterness+Explosion&amp;rft_id=http%3A%2F%2Fwww.overcomingbias.com%2F2011%2F06%2Fthe-betterness-explosion.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-97"><span class="mw-cite-backlink"><b><a href="#cite_ref-97">^</a></b></span> <span class="reference-text"><cite class="citation journal">Votruba, Ashley M.; Kwan, Virginia S.Y. (2014). "Interpreting expert disagreement: The influence of decisional cohesion on the persuasiveness of expert group recommendations". <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1037%2Fe512142015-190">10.1037/e512142015-190</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Interpreting+expert+disagreement%3A+The+influence+of+decisional+cohesion+on+the+persuasiveness+of+expert+group+recommendations&amp;rft.date=2014&amp;rft_id=info%3Adoi%2F10.1037%2Fe512142015-190&amp;rft.aulast=Votruba&amp;rft.aufirst=Ashley+M.&amp;rft.au=Kwan%2C+Virginia+S.Y.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span> <span class="cs1-hidden-error error citation-comment">Cite journal requires <code class="cs1-code">&#124;journal=</code> (<a href="/wiki/Help:CS1_errors#missing_periodical" title="Help:CS1 errors">help</a>)</span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-98"><span class="mw-cite-backlink"><b><a href="#cite_ref-98">^</a></b></span> <span class="reference-text"><cite class="citation journal">Agar, Nicholas. <a rel="nofollow" class="external text" href="https://jetpress.org/v26.1/agar.htm">"Don't Worry about Superintelligence"</a>. <i>Journal of Evolution &amp; Technology</i>. <b>26</b> (1): 73–82.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Evolution+%26+Technology&amp;rft.atitle=Don%27t+Worry+about+Superintelligence&amp;rft.volume=26&amp;rft.issue=1&amp;rft.pages=73-82&amp;rft.aulast=Agar&amp;rft.aufirst=Nicholas&amp;rft_id=https%3A%2F%2Fjetpress.org%2Fv26.1%2Fagar.htm&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-99"><span class="mw-cite-backlink"><b><a href="#cite_ref-99">^</a></b></span> <span class="reference-text"><cite class="citation news">Greenwald, Ted (11 May 2015). <a rel="nofollow" class="external text" href="https://www.wsj.com/articles/does-artificial-intelligence-pose-a-threat-1431109025">"Does Artificial Intelligence Pose a Threat?"</a>. <i>Wall Street Journal</i><span class="reference-accessdate">. Retrieved <span class="nowrap">15 May</span> 2016</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Wall+Street+Journal&amp;rft.atitle=Does+Artificial+Intelligence+Pose+a+Threat%3F&amp;rft.date=2015-05-11&amp;rft.aulast=Greenwald&amp;rft.aufirst=Ted&amp;rft_id=https%3A%2F%2Fwww.wsj.com%2Farticles%2Fdoes-artificial-intelligence-pose-a-threat-1431109025&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-100"><span class="mw-cite-backlink"><b><a href="#cite_ref-100">^</a></b></span> <span class="reference-text"><cite class="citation book"><a href="/wiki/Richard_Posner" title="Richard Posner">Richard Posner</a> (2006). <i>Catastrophe: risk and response</i>. Oxford: Oxford University Press. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-0-19-530647-7" title="Special:BookSources/978-0-19-530647-7"><bdi>978-0-19-530647-7</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Catastrophe%3A+risk+and+response&amp;rft.place=Oxford&amp;rft.pub=Oxford+University+Press&amp;rft.date=2006&amp;rft.isbn=978-0-19-530647-7&amp;rft.au=Richard+Posner&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-101"><span class="mw-cite-backlink"><b><a href="#cite_ref-101">^</a></b></span> <span class="reference-text"><cite class="citation news">Dadich, Scott. <a rel="nofollow" class="external text" href="https://www.wired.com/2016/10/president-obama-mit-joi-ito-interview/">"Barack Obama Talks AI, Robo Cars, and the Future of the World"</a>. <i>WIRED</i><span class="reference-accessdate">. Retrieved <span class="nowrap">27 November</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=WIRED&amp;rft.atitle=Barack+Obama+Talks+AI%2C+Robo+Cars%2C+and+the+Future+of+the+World&amp;rft.aulast=Dadich&amp;rft.aufirst=Scott&amp;rft_id=https%3A%2F%2Fwww.wired.com%2F2016%2F10%2Fpresident-obama-mit-joi-ito-interview%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-102"><span class="mw-cite-backlink"><b><a href="#cite_ref-102">^</a></b></span> <span class="reference-text"><cite class="citation news">Kircher, Madison Malone. <a rel="nofollow" class="external text" href="https://nymag.com/selectall/2016/10/barack-obama-talks-artificial-intelligence-in-wired.html">"Obama on the Risks of AI: 'You Just Gotta Have Somebody Close to the Power Cord<span class="cs1-kern-right">'</span>"</a>. <i>Select All</i><span class="reference-accessdate">. Retrieved <span class="nowrap">27 November</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Select+All&amp;rft.atitle=Obama+on+the+Risks+of+AI%3A+%27You+Just+Gotta+Have+Somebody+Close+to+the+Power+Cord%27&amp;rft.aulast=Kircher&amp;rft.aufirst=Madison+Malone&amp;rft_id=https%3A%2F%2Fnymag.com%2Fselectall%2F2016%2F10%2Fbarack-obama-talks-artificial-intelligence-in-wired.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-103"><span class="mw-cite-backlink"><b><a href="#cite_ref-103">^</a></b></span> <span class="reference-text"><cite class="citation book">Clinton, Hillary (2017). <a href="/wiki/What_Happened_(Clinton_book)" title="What Happened (Clinton book)"><i>What Happened</i></a>. p.&#160;241. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-1-5011-7556-5" title="Special:BookSources/978-1-5011-7556-5"><bdi>978-1-5011-7556-5</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=What+Happened&amp;rft.pages=241&amp;rft.date=2017&amp;rft.isbn=978-1-5011-7556-5&amp;rft.aulast=Clinton&amp;rft.aufirst=Hillary&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/> via <a rel="nofollow" class="external autonumber" href="http://lukemuehlhauser.com/hillary-clinton-on-ai-risk/">[1]</a></span>
</li>
<li id="cite_note-bsa_poll-104"><span class="mw-cite-backlink"><b><a href="#cite_ref-bsa_poll_104-0">^</a></b></span> <span class="reference-text"><cite class="citation news"><a rel="nofollow" class="external text" href="http://www.businessinsider.com/over-a-third-of-people-think-ai-poses-a-threat-to-humanity-2016-3?r=UK&amp;IR=T">"Over a third of people think AI poses a threat to humanity"</a>. <i><a href="/wiki/Business_Insider" title="Business Insider">Business Insider</a></i>. 11 March 2016<span class="reference-accessdate">. Retrieved <span class="nowrap">16 May</span> 2016</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Business+Insider&amp;rft.atitle=Over+a+third+of+people+think+AI+poses+a+threat+to+humanity&amp;rft.date=2016-03-11&amp;rft_id=http%3A%2F%2Fwww.businessinsider.com%2Fover-a-third-of-people-think-ai-poses-a-threat-to-humanity-2016-3%3Fr%3DUK%26IR%3DT&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-:3-105"><span class="mw-cite-backlink">^ <a href="#cite_ref-:3_105-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:3_105-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation news">Brogan, Jacob (6 May 2016). <a rel="nofollow" class="external text" href="http://www.slate.com/blogs/future_tense/2016/05/06/futurography_readers_share_their_opinions_about_killer_artificial_intelligence.html">"What Slate Readers Think About Killer A.I."</a> <i>Slate</i><span class="reference-accessdate">. Retrieved <span class="nowrap">15 May</span> 2016</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Slate&amp;rft.atitle=What+Slate+Readers+Think+About+Killer+A.I.&amp;rft.date=2016-05-06&amp;rft.aulast=Brogan&amp;rft.aufirst=Jacob&amp;rft_id=http%3A%2F%2Fwww.slate.com%2Fblogs%2Ffuture_tense%2F2016%2F05%2F06%2Ffuturography_readers_share_their_opinions_about_killer_artificial_intelligence.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-106"><span class="mw-cite-backlink"><b><a href="#cite_ref-106">^</a></b></span> <span class="reference-text"><cite class="citation journal">LIPPENS, RONNIE (2002). "Imachinations of Peace: Scientifictions of Peace in Iain M. Banks's The Player of Games". <i>Utopianstudies Utopian Studies</i>. <b>13</b> (1): 135–147. <a href="/wiki/International_Standard_Serial_Number" title="International Standard Serial Number">ISSN</a>&#160;<a rel="nofollow" class="external text" href="//www.worldcat.org/issn/1045-991X">1045-991X</a>. <a href="/wiki/OCLC" title="OCLC">OCLC</a>&#160;<a rel="nofollow" class="external text" href="//www.worldcat.org/oclc/5542757341">5542757341</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Utopianstudies+Utopian+Studies&amp;rft.atitle=Imachinations+of+Peace%3A+Scientifictions+of+Peace+in+Iain+M.+Banks%27s+The+Player+of+Games&amp;rft.volume=13&amp;rft.issue=1&amp;rft.pages=135-147&amp;rft.date=2002&amp;rft_id=info%3Aoclcnum%2F5542757341&amp;rft.issn=1045-991X&amp;rft.aulast=LIPPENS&amp;rft.aufirst=RONNIE&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-107"><span class="mw-cite-backlink"><b><a href="#cite_ref-107">^</a></b></span> <span class="reference-text"><cite class="citation news">Vincent, James (22 June 2016). <a rel="nofollow" class="external text" href="https://www.theverge.com/circuitbreaker/2016/6/22/11999664/google-robots-ai-safety-five-problems">"Google's AI researchers say these are the five key problems for robot safety"</a>. <i>The Verge</i><span class="reference-accessdate">. Retrieved <span class="nowrap">5 April</span> 2020</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+Verge&amp;rft.atitle=Google%27s+AI+researchers+say+these+are+the+five+key+problems+for+robot+safety&amp;rft.date=2016-06-22&amp;rft.aulast=Vincent&amp;rft.aufirst=James&amp;rft_id=https%3A%2F%2Fwww.theverge.com%2Fcircuitbreaker%2F2016%2F6%2F22%2F11999664%2Fgoogle-robots-ai-safety-five-problems&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-108"><span class="mw-cite-backlink"><b><a href="#cite_ref-108">^</a></b></span> <span class="reference-text">Amodei, Dario, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mané. "Concrete problems in AI safety." arXiv preprint arXiv:1606.06565 (2016).</span>
</li>
<li id="cite_note-109"><span class="mw-cite-backlink"><b><a href="#cite_ref-109">^</a></b></span> <span class="reference-text"><cite class="citation book"><a href="/wiki/Toby_Ord" title="Toby Ord">Toby Ord</a> (2020). <i><a href="/wiki/The_Precipice:_Existential_Risk_and_the_Future_of_Humanity" title="The Precipice: Existential Risk and the Future of Humanity">The Precipice: Existential Risk and the Future of Humanity</a></i>. Bloomsbury Publishing Plc. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/9781526600196" title="Special:BookSources/9781526600196"><bdi>9781526600196</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=The+Precipice%3A+Existential+Risk+and+the+Future+of+Humanity&amp;rft.pub=Bloomsbury+Publishing+Plc&amp;rft.date=2020&amp;rft.isbn=9781526600196&amp;rft.au=Toby+Ord&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-110"><span class="mw-cite-backlink"><b><a href="#cite_ref-110">^</a></b></span> <span class="reference-text"><cite class="citation news"><a rel="nofollow" class="external text" href="https://www.nbcnews.com/mach/tech/elon-musk-wants-hook-your-brain-directly-computers-starting-next-ncna1030631">"Elon Musk wants to hook your brain up directly to computers — starting next year"</a>. <i>NBC News</i>. 2019<span class="reference-accessdate">. Retrieved <span class="nowrap">5 April</span> 2020</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=NBC+News&amp;rft.atitle=Elon+Musk+wants+to+hook+your+brain+up+directly+to+computers+%E2%80%94+starting+next+year&amp;rft.date=2019&amp;rft_id=https%3A%2F%2Fwww.nbcnews.com%2Fmach%2Ftech%2Felon-musk-wants-hook-your-brain-directly-computers-starting-next-ncna1030631&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-111"><span class="mw-cite-backlink"><b><a href="#cite_ref-111">^</a></b></span> <span class="reference-text"><cite class="citation news">Torres, Phil (18 September 2018). <a rel="nofollow" class="external text" href="https://slate.com/technology/2018/09/genetic-engineering-to-stop-doomsday.html">"Only Radically Enhancing Humanity Can Save Us All"</a>. <i>Slate Magazine</i><span class="reference-accessdate">. Retrieved <span class="nowrap">5 April</span> 2020</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Slate+Magazine&amp;rft.atitle=Only+Radically+Enhancing+Humanity+Can+Save+Us+All&amp;rft.date=2018-09-18&amp;rft.aulast=Torres&amp;rft.aufirst=Phil&amp;rft_id=https%3A%2F%2Fslate.com%2Ftechnology%2F2018%2F09%2Fgenetic-engineering-to-stop-doomsday.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-112"><span class="mw-cite-backlink"><b><a href="#cite_ref-112">^</a></b></span> <span class="reference-text"><cite class="citation journal">Barrett, Anthony M.; Baum, Seth D. (23 May 2016). "A model of pathways to artificial superintelligence catastrophe for risk and decision analysis". <i>Journal of Experimental &amp; Theoretical Artificial Intelligence</i>. <b>29</b> (2): 397–414. <a href="/wiki/ArXiv" title="ArXiv">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//arxiv.org/abs/1607.07730">1607.07730</a></span>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1080%2F0952813X.2016.1186228">10.1080/0952813X.2016.1186228</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Experimental+%26+Theoretical+Artificial+Intelligence&amp;rft.atitle=A+model+of+pathways+to+artificial+superintelligence+catastrophe+for+risk+and+decision+analysis&amp;rft.volume=29&amp;rft.issue=2&amp;rft.pages=397-414&amp;rft.date=2016-05-23&amp;rft_id=info%3Aarxiv%2F1607.07730&amp;rft_id=info%3Adoi%2F10.1080%2F0952813X.2016.1186228&amp;rft.aulast=Barrett&amp;rft.aufirst=Anthony+M.&amp;rft.au=Baum%2C+Seth+D.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-113"><span class="mw-cite-backlink"><b><a href="#cite_ref-113">^</a></b></span> <span class="reference-text"><cite class="citation news">Mark Piesing (17 May 2012). <a rel="nofollow" class="external text" href="https://www.wired.co.uk/news/archive/2012-05/17/the-dangers-of-an-ai-smarter-than-us">"AI uprising: humans will be outsourced, not obliterated"</a>. <i>Wired</i><span class="reference-accessdate">. Retrieved <span class="nowrap">12 December</span> 2015</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Wired&amp;rft.atitle=AI+uprising%3A+humans+will+be+outsourced%2C+not+obliterated&amp;rft.date=2012-05-17&amp;rft.au=Mark+Piesing&amp;rft_id=https%3A%2F%2Fwww.wired.co.uk%2Fnews%2Farchive%2F2012-05%2F17%2Fthe-dangers-of-an-ai-smarter-than-us&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-114"><span class="mw-cite-backlink"><b><a href="#cite_ref-114">^</a></b></span> <span class="reference-text"><cite class="citation news">Coughlan, Sean (24 April 2013). <a rel="nofollow" class="external text" href="https://www.bbc.com/news/business-22002530">"How are humans going to become extinct?"</a>. <i>BBC News</i><span class="reference-accessdate">. Retrieved <span class="nowrap">29 March</span> 2014</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=BBC+News&amp;rft.atitle=How+are+humans+going+to+become+extinct%3F&amp;rft.date=2013-04-24&amp;rft.aulast=Coughlan&amp;rft.aufirst=Sean&amp;rft_id=https%3A%2F%2Fwww.bbc.com%2Fnews%2Fbusiness-22002530&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-115"><span class="mw-cite-backlink"><b><a href="#cite_ref-115">^</a></b></span> <span class="reference-text"><cite class="citation news">Technology Correspondent, Mark Bridge (10 June 2017). <a rel="nofollow" class="external text" href="https://www.thetimes.co.uk/article/making-robots-less-confident-could-prevent-them-taking-over-gnsblq7lx">"Making robots less confident could prevent them taking over"</a>. <i>The Times</i><span class="reference-accessdate">. Retrieved <span class="nowrap">21 March</span> 2018</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+Times&amp;rft.atitle=Making+robots+less+confident+could+prevent+them+taking+over&amp;rft.date=2017-06-10&amp;rft.aulast=Technology+Correspondent&amp;rft.aufirst=Mark+Bridge&amp;rft_id=https%3A%2F%2Fwww.thetimes.co.uk%2Farticle%2Fmaking-robots-less-confident-could-prevent-them-taking-over-gnsblq7lx&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-116"><span class="mw-cite-backlink"><b><a href="#cite_ref-116">^</a></b></span> <span class="reference-text"><cite class="citation journal"><a href="/wiki/John_McGinnis" title="John McGinnis">John McGinnis</a> (Summer 2010). <a rel="nofollow" class="external text" href="http://scholarlycommons.law.northwestern.edu/cgi/viewcontent.cgi?article=1193&amp;context=nulr_online">"Accelerating AI"</a>. <i><a href="/wiki/Northwestern_University_Law_Review" title="Northwestern University Law Review">Northwestern University Law Review</a></i>. <b>104</b> (3): 1253–1270<span class="reference-accessdate">. Retrieved <span class="nowrap">16 July</span> 2014</span>. <q>For all these reasons, verifying a global relinquishment treaty, or even one limited to AI-related weapons development, is a nonstarter... (For different reasons from ours, the Machine Intelligence Research Institute) considers (AGI) relinquishment infeasible...</q></cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Northwestern+University+Law+Review&amp;rft.atitle=Accelerating+AI&amp;rft.ssn=summer&amp;rft.volume=104&amp;rft.issue=3&amp;rft.pages=1253-1270&amp;rft.date=2010&amp;rft.au=John+McGinnis&amp;rft_id=http%3A%2F%2Fscholarlycommons.law.northwestern.edu%2Fcgi%2Fviewcontent.cgi%3Farticle%3D1193%26context%3Dnulr_online&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-117"><span class="mw-cite-backlink"><b><a href="#cite_ref-117">^</a></b></span> <span class="reference-text"><cite class="citation journal">Kaj Sotala; <a href="/wiki/Roman_Yampolskiy" title="Roman Yampolskiy">Roman Yampolskiy</a> (19 December 2014). "Responses to catastrophic AGI risk: a survey". <i><a href="/wiki/Physica_Scripta" title="Physica Scripta">Physica Scripta</a></i>. <b>90</b> (1). <q>In general, most writers reject proposals for broad relinquishment... Relinquishment proposals suffer from many of the same problems as regulation proposals, but to a greater extent. There is no historical precedent of general, multi-use technology similar to AGI being successfully relinquished for good, nor do there seem to be any theoretical reasons for believing that relinquishment proposals would work in the future. Therefore we do not consider them to be a viable class of proposals.</q></cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Physica+Scripta&amp;rft.atitle=Responses+to+catastrophic+AGI+risk%3A+a+survey&amp;rft.volume=90&amp;rft.issue=1&amp;rft.date=2014-12-19&amp;rft.au=Kaj+Sotala&amp;rft.au=Roman+Yampolskiy&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-118"><span class="mw-cite-backlink"><b><a href="#cite_ref-118">^</a></b></span> <span class="reference-text"><cite class="citation news">Brad Allenby (11 April 2016). <a rel="nofollow" class="external text" href="http://www.slate.com/articles/technology/future_tense/2016/04/why_it_s_a_mistake_to_compare_a_i_with_human_intelligence.html">"The Wrong Cognitive Measuring Stick"</a>. <i>Slate</i><span class="reference-accessdate">. Retrieved <span class="nowrap">15 May</span> 2016</span>. <q>It is fantasy to suggest that the accelerating development and deployment of technologies that taken together are considered to be A.I. will be stopped or limited, either by regulation or even by national legislation.</q></cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Slate&amp;rft.atitle=The+Wrong+Cognitive+Measuring+Stick&amp;rft.date=2016-04-11&amp;rft.au=Brad+Allenby&amp;rft_id=http%3A%2F%2Fwww.slate.com%2Farticles%2Ftechnology%2Ffuture_tense%2F2016%2F04%2Fwhy_it_s_a_mistake_to_compare_a_i_with_human_intelligence.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-mcginnis-119"><span class="mw-cite-backlink"><b><a href="#cite_ref-mcginnis_119-0">^</a></b></span> <span class="reference-text"><cite class="citation journal"><a href="/wiki/John_McGinnis" title="John McGinnis">John McGinnis</a> (Summer 2010). <a rel="nofollow" class="external text" href="http://scholarlycommons.law.northwestern.edu/cgi/viewcontent.cgi?article=1193&amp;context=nulr_online">"Accelerating AI"</a>. <i><a href="/wiki/Northwestern_University_Law_Review" title="Northwestern University Law Review">Northwestern University Law Review</a></i>. <b>104</b> (3): 1253–1270<span class="reference-accessdate">. Retrieved <span class="nowrap">16 July</span> 2014</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Northwestern+University+Law+Review&amp;rft.atitle=Accelerating+AI&amp;rft.ssn=summer&amp;rft.volume=104&amp;rft.issue=3&amp;rft.pages=1253-1270&amp;rft.date=2010&amp;rft.au=John+McGinnis&amp;rft_id=http%3A%2F%2Fscholarlycommons.law.northwestern.edu%2Fcgi%2Fviewcontent.cgi%3Farticle%3D1193%26context%3Dnulr_online&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-120"><span class="mw-cite-backlink"><b><a href="#cite_ref-120">^</a></b></span> <span class="reference-text"><cite class="citation news"><a rel="nofollow" class="external text" href="https://www.newyorker.com/tech/elements/why-we-should-think-about-the-threat-of-artificial-intelligence">"Why We Should Think About the Threat of Artificial Intelligence"</a>. <i><a href="/wiki/The_New_Yorker" title="The New Yorker">The New Yorker</a></i>. 4 October 2013<span class="reference-accessdate">. Retrieved <span class="nowrap">7 February</span> 2016</span>. <q>Of course, one could try to ban super-intelligent computers altogether. But 'the competitive advantage—economic, military, even artistic—of every advance in automation is so compelling,' <a href="/wiki/Vernor_Vinge" title="Vernor Vinge">Vernor Vinge</a>, the mathematician and science-fiction author, wrote, 'that passing laws, or having customs, that forbid such things merely assures that someone else will.'</q></cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+New+Yorker&amp;rft.atitle=Why+We+Should+Think+About+the+Threat+of+Artificial+Intelligence&amp;rft.date=2013-10-04&amp;rft_id=https%3A%2F%2Fwww.newyorker.com%2Ftech%2Felements%2Fwhy-we-should-think-about-the-threat-of-artificial-intelligence&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-121"><span class="mw-cite-backlink"><b><a href="#cite_ref-121">^</a></b></span> <span class="reference-text"><cite class="citation journal">Baum, Seth (22 August 2018). <a rel="nofollow" class="external text" href="https://dx.doi.org/10.3390/info9090209">"Superintelligence Skepticism as a Political Tool"</a>. <i>Information</i>. <b>9</b> (9): 209. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.3390%2Finfo9090209">10.3390/info9090209</a></span>. <a href="/wiki/International_Standard_Serial_Number" title="International Standard Serial Number">ISSN</a>&#160;<a rel="nofollow" class="external text" href="//www.worldcat.org/issn/2078-2489">2078-2489</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Information&amp;rft.atitle=Superintelligence+Skepticism+as+a+Political+Tool&amp;rft.volume=9&amp;rft.issue=9&amp;rft.pages=209&amp;rft.date=2018-08-22&amp;rft_id=info%3Adoi%2F10.3390%2Finfo9090209&amp;rft.issn=2078-2489&amp;rft.aulast=Baum&amp;rft.aufirst=Seth&amp;rft_id=http%3A%2F%2Fdx.doi.org%2F10.3390%2Finfo9090209&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-122"><span class="mw-cite-backlink"><b><a href="#cite_ref-122">^</a></b></span> <span class="reference-text"><cite class="citation news"><a rel="nofollow" class="external text" href="https://www.npr.org/sections/thetwo-way/2017/07/17/537686649/elon-musk-warns-governors-artificial-intelligence-poses-existential-risk">"Elon Musk Warns Governors: Artificial Intelligence Poses 'Existential Risk<span class="cs1-kern-right">'</span>"</a>. <i>NPR.org</i><span class="reference-accessdate">. Retrieved <span class="nowrap">27 November</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=NPR.org&amp;rft.atitle=Elon+Musk+Warns+Governors%3A+Artificial+Intelligence+Poses+%27Existential+Risk%27&amp;rft_id=https%3A%2F%2Fwww.npr.org%2Fsections%2Fthetwo-way%2F2017%2F07%2F17%2F537686649%2Felon-musk-warns-governors-artificial-intelligence-poses-existential-risk&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-123"><span class="mw-cite-backlink"><b><a href="#cite_ref-123">^</a></b></span> <span class="reference-text"><cite class="citation news">Gibbs, Samuel (17 July 2017). <a rel="nofollow" class="external text" href="https://www.theguardian.com/technology/2017/jul/17/elon-musk-regulation-ai-combat-existential-threat-tesla-spacex-ceo">"Elon Musk: regulate AI to combat 'existential threat' before it's too late"</a>. <i>The Guardian</i><span class="reference-accessdate">. Retrieved <span class="nowrap">27 November</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+Guardian&amp;rft.atitle=Elon+Musk%3A+regulate+AI+to+combat+%27existential+threat%27+before+it%27s+too+late&amp;rft.date=2017-07-17&amp;rft.aulast=Gibbs&amp;rft.aufirst=Samuel&amp;rft_id=https%3A%2F%2Fwww.theguardian.com%2Ftechnology%2F2017%2Fjul%2F17%2Felon-musk-regulation-ai-combat-existential-threat-tesla-spacex-ceo&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-cnbc-124"><span class="mw-cite-backlink">^ <a href="#cite_ref-cnbc_124-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-cnbc_124-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation news">Kharpal, Arjun (7 November 2017). <a rel="nofollow" class="external text" href="https://www.cnbc.com/2017/11/07/ai-infancy-and-too-early-to-regulate-intel-ceo-brian-krzanich-says.html">"A.I. is in its 'infancy' and it's too early to regulate it, Intel CEO Brian Krzanich says"</a>. <i>CNBC</i><span class="reference-accessdate">. Retrieved <span class="nowrap">27 November</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=CNBC&amp;rft.atitle=A.I.+is+in+its+%27infancy%27+and+it%27s+too+early+to+regulate+it%2C+Intel+CEO+Brian+Krzanich+says&amp;rft.date=2017-11-07&amp;rft.aulast=Kharpal&amp;rft.aufirst=Arjun&amp;rft_id=https%3A%2F%2Fwww.cnbc.com%2F2017%2F11%2F07%2Fai-infancy-and-too-early-to-regulate-intel-ceo-brian-krzanich-says.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-125"><span class="mw-cite-backlink"><b><a href="#cite_ref-125">^</a></b></span> <span class="reference-text"><cite class="citation journal">Kaplan, Andreas; Haenlein, Michael (2019). "Siri, Siri, in my hand: Who's the fairest in the land? On the interpretations, illustrations, and implications of artificial intelligence". <i>Business Horizons</i>. <b>62</b>: 15–25. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1016%2Fj.bushor.2018.08.004">10.1016/j.bushor.2018.08.004</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Business+Horizons&amp;rft.atitle=Siri%2C+Siri%2C+in+my+hand%3A+Who%27s+the+fairest+in+the+land%3F+On+the+interpretations%2C+illustrations%2C+and+implications+of+artificial+intelligence&amp;rft.volume=62&amp;rft.pages=15-25&amp;rft.date=2019&amp;rft_id=info%3Adoi%2F10.1016%2Fj.bushor.2018.08.004&amp;rft.aulast=Kaplan&amp;rft.aufirst=Andreas&amp;rft.au=Haenlein%2C+Michael&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-126"><span class="mw-cite-backlink"><b><a href="#cite_ref-126">^</a></b></span> <span class="reference-text"><cite class="citation journal">Baum, Seth D.; Goertzel, Ben; Goertzel, Ted G. (January 2011). "How long until human-level AI? Results from an expert assessment". <i>Technological Forecasting and Social Change</i>. <b>78</b> (1): 185–195. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1016%2Fj.techfore.2010.09.006">10.1016/j.techfore.2010.09.006</a>. <a href="/wiki/International_Standard_Serial_Number" title="International Standard Serial Number">ISSN</a>&#160;<a rel="nofollow" class="external text" href="//www.worldcat.org/issn/0040-1625">0040-1625</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Technological+Forecasting+and+Social+Change&amp;rft.atitle=How+long+until+human-level+AI%3F+Results+from+an+expert+assessment&amp;rft.volume=78&amp;rft.issue=1&amp;rft.pages=185-195&amp;rft.date=2011-01&amp;rft_id=info%3Adoi%2F10.1016%2Fj.techfore.2010.09.006&amp;rft.issn=0040-1625&amp;rft.aulast=Baum&amp;rft.aufirst=Seth+D.&amp;rft.au=Goertzel%2C+Ben&amp;rft.au=Goertzel%2C+Ted+G.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-127"><span class="mw-cite-backlink"><b><a href="#cite_ref-127">^</a></b></span> <span class="reference-text"><cite class="citation book">United States. Defense Innovation Board. <i>AI principles&#160;: recommendations on the ethical use of artificial intelligence by the Department of Defense</i>. <a href="/wiki/OCLC" title="OCLC">OCLC</a>&#160;<a rel="nofollow" class="external text" href="//www.worldcat.org/oclc/1126650738">1126650738</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=AI+principles+%3A+recommendations+on+the+ethical+use+of+artificial+intelligence+by+the+Department+of+Defense&amp;rft_id=info%3Aoclcnum%2F1126650738&amp;rft.au=United+States.+Defense+Innovation+Board.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-128"><span class="mw-cite-backlink"><b><a href="#cite_ref-128">^</a></b></span> <span class="reference-text"><cite class="citation web">Stefanik, Elise M. (22 May 2018). <a rel="nofollow" class="external text" href="https://www.congress.gov/bill/115th-congress/house-bill/5356">"H.R.5356 - 115th Congress (2017-2018): National Security Commission Artificial Intelligence Act of 2018"</a>. <i>www.congress.gov</i><span class="reference-accessdate">. Retrieved <span class="nowrap">13 March</span> 2020</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=www.congress.gov&amp;rft.atitle=H.R.5356+-+115th+Congress+%282017-2018%29%3A+National+Security+Commission+Artificial+Intelligence+Act+of+2018&amp;rft.date=2018-05-22&amp;rft.aulast=Stefanik&amp;rft.aufirst=Elise+M.&amp;rft_id=https%3A%2F%2Fwww.congress.gov%2Fbill%2F115th-congress%2Fhouse-bill%2F5356&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-129"><span class="mw-cite-backlink"><b><a href="#cite_ref-129">^</a></b></span> <span class="reference-text"><cite class="citation journal">Baum, Seth (30 September 2018). "Countering Superintelligence Misinformation". <i>Information</i>. <b>9</b> (10): 244. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.3390%2Finfo9100244">10.3390/info9100244</a></span>. <a href="/wiki/International_Standard_Serial_Number" title="International Standard Serial Number">ISSN</a>&#160;<a rel="nofollow" class="external text" href="//www.worldcat.org/issn/2078-2489">2078-2489</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Information&amp;rft.atitle=Countering+Superintelligence+Misinformation&amp;rft.volume=9&amp;rft.issue=10&amp;rft.pages=244&amp;rft.date=2018-09-30&amp;rft_id=info%3Adoi%2F10.3390%2Finfo9100244&amp;rft.issn=2078-2489&amp;rft.aulast=Baum&amp;rft.aufirst=Seth&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-:5-130"><span class="mw-cite-backlink">^ <a href="#cite_ref-:5_130-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:5_130-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal">Sotala, Kaj; Yampolskiy, Roman V (19 December 2014). "Responses to catastrophic AGI risk: a survey". <i>Physica Scripta</i>. <b>90</b> (1): 018001. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.1088%2F0031-8949%2F90%2F1%2F018001">10.1088/0031-8949/90/1/018001</a></span>. <a href="/wiki/International_Standard_Serial_Number" title="International Standard Serial Number">ISSN</a>&#160;<a rel="nofollow" class="external text" href="//www.worldcat.org/issn/0031-8949">0031-8949</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Physica+Scripta&amp;rft.atitle=Responses+to+catastrophic+AGI+risk%3A+a+survey&amp;rft.volume=90&amp;rft.issue=1&amp;rft.pages=018001&amp;rft.date=2014-12-19&amp;rft_id=info%3Adoi%2F10.1088%2F0031-8949%2F90%2F1%2F018001&amp;rft.issn=0031-8949&amp;rft.aulast=Sotala&amp;rft.aufirst=Kaj&amp;rft.au=Yampolskiy%2C+Roman+V&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-131"><span class="mw-cite-backlink"><b><a href="#cite_ref-131">^</a></b></span> <span class="reference-text"><cite class="citation journal">Geist, Edward Moore (15 August 2016). "It's already too late to stop the AI arms race—We must manage it instead". <i>Bulletin of the Atomic Scientists</i>. <b>72</b> (5): 318–321. <a href="/wiki/Bibcode" title="Bibcode">Bibcode</a>:<a rel="nofollow" class="external text" href="https://ui.adsabs.harvard.edu/abs/2016BuAtS..72e.318G">2016BuAtS..72e.318G</a>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1080%2F00963402.2016.1216672">10.1080/00963402.2016.1216672</a>. <a href="/wiki/International_Standard_Serial_Number" title="International Standard Serial Number">ISSN</a>&#160;<a rel="nofollow" class="external text" href="//www.worldcat.org/issn/0096-3402">0096-3402</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Bulletin+of+the+Atomic+Scientists&amp;rft.atitle=It%27s+already+too+late+to+stop+the+AI+arms+race%E2%80%94We+must+manage+it+instead&amp;rft.volume=72&amp;rft.issue=5&amp;rft.pages=318-321&amp;rft.date=2016-08-15&amp;rft.issn=0096-3402&amp;rft_id=info%3Adoi%2F10.1080%2F00963402.2016.1216672&amp;rft_id=info%3Abibcode%2F2016BuAtS..72e.318G&amp;rft.aulast=Geist&amp;rft.aufirst=Edward+Moore&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-132"><span class="mw-cite-backlink"><b><a href="#cite_ref-132">^</a></b></span> <span class="reference-text"><cite class="citation journal">Maas, Matthijs M. (6 February 2019). "How viable is international arms control for military artificial intelligence? Three lessons from nuclear weapons". <i>Contemporary Security Policy</i>. <b>40</b> (3): 285–311. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1080%2F13523260.2019.1576464">10.1080/13523260.2019.1576464</a>. <a href="/wiki/International_Standard_Serial_Number" title="International Standard Serial Number">ISSN</a>&#160;<a rel="nofollow" class="external text" href="//www.worldcat.org/issn/1352-3260">1352-3260</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Contemporary+Security+Policy&amp;rft.atitle=How+viable+is+international+arms+control+for+military+artificial+intelligence%3F+Three+lessons+from+nuclear+weapons&amp;rft.volume=40&amp;rft.issue=3&amp;rft.pages=285-311&amp;rft.date=2019-02-06&amp;rft_id=info%3Adoi%2F10.1080%2F13523260.2019.1576464&amp;rft.issn=1352-3260&amp;rft.aulast=Maas&amp;rft.aufirst=Matthijs+M.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AExistential+risk+from+artificial+general+intelligence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
</ol></div></div>
<div style="clear:both;"></div>
<div role="navigation" class="navbox" aria-labelledby="Existential_risk_from_artificial_intelligence" style="padding:3px"><table class="nowraplinks mw-collapsible expanded navbox-inner" style="border-spacing:0;background:transparent;color:inherit"><tbody><tr><th scope="col" class="navbox-title" colspan="2"><div class="plainlinks hlist navbar mini"><ul><li class="nv-view"><a href="/wiki/Template:Existential_risk_from_artificial_intelligence" title="Template:Existential risk from artificial intelligence"><abbr title="View this template" style=";;background:none transparent;border:none;-moz-box-shadow:none;-webkit-box-shadow:none;box-shadow:none; padding:0;">v</abbr></a></li><li class="nv-talk"><a href="/wiki/Template_talk:Existential_risk_from_artificial_intelligence" title="Template talk:Existential risk from artificial intelligence"><abbr title="Discuss this template" style=";;background:none transparent;border:none;-moz-box-shadow:none;-webkit-box-shadow:none;box-shadow:none; padding:0;">t</abbr></a></li><li class="nv-edit"><a class="external text" href="https://en.wikipedia.org/w/index.php?title=Template:Existential_risk_from_artificial_intelligence&amp;action=edit"><abbr title="Edit this template" style=";;background:none transparent;border:none;-moz-box-shadow:none;-webkit-box-shadow:none;box-shadow:none; padding:0;">e</abbr></a></li></ul></div><div id="Existential_risk_from_artificial_intelligence" style="font-size:114%;margin:0 4em"><a class="mw-selflink selflink">Existential risk</a> from <a href="/wiki/Artificial_intelligence" title="Artificial intelligence">artificial intelligence</a></div></th></tr><tr><th scope="row" class="navbox-group" style="width:1%">Concepts</th><td class="navbox-list navbox-odd hlist" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/Accelerating_change" title="Accelerating change">Accelerating change</a></li>
<li><a href="/wiki/AI_box" title="AI box">AI box</a></li>
<li><a href="/wiki/AI_takeover" title="AI takeover">AI takeover</a></li>
<li><a href="/wiki/AI_control_problem" title="AI control problem">Control problem</a></li>
<li><a class="mw-selflink selflink">Existential risk from artificial general intelligence</a></li>
<li><a href="/wiki/Friendly_artificial_intelligence" title="Friendly artificial intelligence">Friendly artificial intelligence</a></li>
<li><a href="/wiki/Instrumental_convergence" title="Instrumental convergence">Instrumental convergence</a></li>
<li><a href="/wiki/Intelligence_explosion" class="mw-redirect" title="Intelligence explosion">Intelligence explosion</a></li>
<li><a href="/wiki/Machine_ethics" title="Machine ethics">Machine ethics</a></li>
<li><a href="/wiki/Superintelligence" title="Superintelligence">Superintelligence</a></li>
<li><a href="/wiki/Technological_singularity" title="Technological singularity">Technological singularity</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Organizations</th><td class="navbox-list navbox-even hlist" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/Allen_Institute_for_AI" title="Allen Institute for AI">Allen Institute for AI</a></li>
<li><a href="/wiki/Center_for_Applied_Rationality" title="Center for Applied Rationality">Center for Applied Rationality</a></li>
<li><a href="/wiki/Center_for_Human-Compatible_Artificial_Intelligence" title="Center for Human-Compatible Artificial Intelligence">Center for Human-Compatible Artificial Intelligence</a></li>
<li><a href="/wiki/Center_for_Security_and_Emerging_Technology" title="Center for Security and Emerging Technology">Center for Security and Emerging Technology</a></li>
<li><a href="/wiki/Centre_for_the_Study_of_Existential_Risk" title="Centre for the Study of Existential Risk">Centre for the Study of Existential Risk</a></li>
<li><a href="/wiki/DeepMind" title="DeepMind">DeepMind</a></li>
<li><a href="/wiki/Foundational_Questions_Institute" title="Foundational Questions Institute">Foundational Questions Institute</a></li>
<li><a href="/wiki/Future_of_Humanity_Institute" title="Future of Humanity Institute">Future of Humanity Institute</a></li>
<li><a href="/wiki/Future_of_Life_Institute" title="Future of Life Institute">Future of Life Institute</a></li>
<li><a href="/wiki/Humanity%2B" title="Humanity+">Humanity+</a></li>
<li><a href="/wiki/Institute_for_Ethics_and_Emerging_Technologies" title="Institute for Ethics and Emerging Technologies">Institute for Ethics and Emerging Technologies</a></li>
<li><a href="/wiki/Leverhulme_Centre_for_the_Future_of_Intelligence" title="Leverhulme Centre for the Future of Intelligence">Leverhulme Centre for the Future of Intelligence</a></li>
<li><a href="/wiki/Machine_Intelligence_Research_Institute" title="Machine Intelligence Research Institute">Machine Intelligence Research Institute</a></li>
<li><a href="/wiki/OpenAI" title="OpenAI">OpenAI</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">People</th><td class="navbox-list navbox-odd hlist" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/Nick_Bostrom" title="Nick Bostrom">Nick Bostrom</a></li>
<li><a href="/wiki/K._Eric_Drexler" title="K. Eric Drexler">Eric Drexler</a></li>
<li><a href="/wiki/Sam_Harris" title="Sam Harris">Sam Harris</a></li>
<li><a href="/wiki/Stephen_Hawking" title="Stephen Hawking">Stephen Hawking</a></li>
<li><a href="/wiki/Bill_Hibbard" title="Bill Hibbard">Bill Hibbard</a></li>
<li><a href="/wiki/Bill_Joy" title="Bill Joy">Bill Joy</a></li>
<li><a href="/wiki/Elon_Musk" title="Elon Musk">Elon Musk</a></li>
<li><a href="/wiki/Steve_Omohundro" title="Steve Omohundro">Steve Omohundro</a></li>
<li><a href="/wiki/Huw_Price" title="Huw Price">Huw Price</a></li>
<li><a href="/wiki/Martin_Rees" title="Martin Rees">Martin Rees</a></li>
<li><a href="/wiki/Stuart_J._Russell" title="Stuart J. Russell">Stuart J. Russell</a></li>
<li><a href="/wiki/Jaan_Tallinn" title="Jaan Tallinn">Jaan Tallinn</a></li>
<li><a href="/wiki/Max_Tegmark" title="Max Tegmark">Max Tegmark</a></li>
<li><a href="/wiki/Frank_Wilczek" title="Frank Wilczek">Frank Wilczek</a></li>
<li><a href="/wiki/Roman_Yampolskiy" title="Roman Yampolskiy">Roman Yampolskiy</a></li>
<li><a href="/wiki/Andrew_Yang" title="Andrew Yang">Andrew Yang</a></li>
<li><a href="/wiki/Eliezer_Yudkowsky" title="Eliezer Yudkowsky">Eliezer Yudkowsky</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Other</th><td class="navbox-list navbox-even hlist" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/Global_catastrophic_risk#Artificial_intelligence" title="Global catastrophic risk">Artificial intelligence as a global catastrophic risk</a></li>
<li><a href="/wiki/Artificial_general_intelligence#Controversies_and_dangers" title="Artificial general intelligence">Controversies and dangers of artificial general intelligence</a></li>
<li><a href="/wiki/Ethics_of_artificial_intelligence" title="Ethics of artificial intelligence">Ethics of artificial intelligence</a></li>
<li><i><a href="/wiki/Human_Compatible" title="Human Compatible">Human Compatible</a></i></li>
<li><a href="/wiki/Open_Letter_on_Artificial_Intelligence" title="Open Letter on Artificial Intelligence">Open Letter on Artificial Intelligence</a></li>
<li><i><a href="/wiki/Our_Final_Invention" title="Our Final Invention">Our Final Invention</a></i></li>
<li><i><a href="/wiki/The_Precipice:_Existential_Risk_and_the_Future_of_Humanity" title="The Precipice: Existential Risk and the Future of Humanity">The Precipice</a></i></li>
<li><i><a href="/wiki/Superintelligence:_Paths,_Dangers,_Strategies" title="Superintelligence: Paths, Dangers, Strategies">Superintelligence: Paths, Dangers, Strategies</a></i></li></ul>
</div></td></tr><tr><td class="navbox-abovebelow" colspan="2"><div><img alt="Category" src="//upload.wikimedia.org/wikipedia/en/thumb/4/48/Folder_Hexagonal_Icon.svg/16px-Folder_Hexagonal_Icon.svg.png" decoding="async" title="Category" width="16" height="14" srcset="//upload.wikimedia.org/wikipedia/en/thumb/4/48/Folder_Hexagonal_Icon.svg/24px-Folder_Hexagonal_Icon.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/4/48/Folder_Hexagonal_Icon.svg/32px-Folder_Hexagonal_Icon.svg.png 2x" data-file-width="36" data-file-height="31" /> <a href="/wiki/Category:Existential_risk_from_artificial_general_intelligence" title="Category:Existential risk from artificial general intelligence">Category</a></div></td></tr></tbody></table></div>
<div role="navigation" class="navbox" aria-labelledby="Effective_altruism" style="padding:3px"><table class="nowraplinks mw-collapsible autocollapse navbox-inner" style="border-spacing:0;background:transparent;color:inherit"><tbody><tr><th scope="col" class="navbox-title" colspan="2"><div class="plainlinks hlist navbar mini"><ul><li class="nv-view"><a href="/wiki/Template:Effective_altruism" title="Template:Effective altruism"><abbr title="View this template" style=";;background:none transparent;border:none;-moz-box-shadow:none;-webkit-box-shadow:none;box-shadow:none; padding:0;">v</abbr></a></li><li class="nv-talk"><a href="/wiki/Template_talk:Effective_altruism" title="Template talk:Effective altruism"><abbr title="Discuss this template" style=";;background:none transparent;border:none;-moz-box-shadow:none;-webkit-box-shadow:none;box-shadow:none; padding:0;">t</abbr></a></li><li class="nv-edit"><a class="external text" href="https://en.wikipedia.org/w/index.php?title=Template:Effective_altruism&amp;action=edit"><abbr title="Edit this template" style=";;background:none transparent;border:none;-moz-box-shadow:none;-webkit-box-shadow:none;box-shadow:none; padding:0;">e</abbr></a></li></ul></div><div id="Effective_altruism" style="font-size:114%;margin:0 4em"><a href="/wiki/Effective_altruism" title="Effective altruism">Effective altruism</a></div></th></tr><tr><th scope="row" class="navbox-group" style="width:1%">Concepts</th><td class="navbox-list navbox-odd hlist" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/Charity_assessment" title="Charity assessment">Charity assessment</a></li>
<li><a href="/wiki/Demandingness_objection" title="Demandingness objection">Demandingness objection</a></li>
<li><a href="/wiki/Disability-adjusted_life_year" title="Disability-adjusted life year">Disability-adjusted life year</a></li>
<li><a href="/wiki/Earning_to_give" title="Earning to give">Earning to give</a></li>
<li><a href="/wiki/Equal_consideration_of_interests" title="Equal consideration of interests">Equal consideration of interests</a></li>
<li><a href="/wiki/Marginal_utility" title="Marginal utility">Marginal utility</a></li>
<li><a href="/wiki/Quality-adjusted_life_year" title="Quality-adjusted life year">Quality-adjusted life year</a></li>
<li><a href="/wiki/Utilitarianism" title="Utilitarianism">Utilitarianism</a></li>
<li><a href="/wiki/Venture_philanthropy" title="Venture philanthropy">Venture philanthropy</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Key figures</th><td class="navbox-list navbox-even hlist" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/Liv_Boeree" title="Liv Boeree">Liv Boeree</a></li>
<li><a href="/wiki/Nick_Bostrom" title="Nick Bostrom">Nick Bostrom</a></li>
<li><a href="/wiki/Ben_Delo" title="Ben Delo">Ben Delo</a></li>
<li><a href="/wiki/Hilary_Greaves" title="Hilary Greaves">Hilary Greaves</a></li>
<li><a href="/wiki/Holden_Karnofsky" title="Holden Karnofsky">Holden Karnofsky</a></li>
<li><a href="/wiki/William_MacAskill" title="William MacAskill">William MacAskill</a></li>
<li><a href="/wiki/Dustin_Moskovitz" title="Dustin Moskovitz">Dustin Moskovitz</a></li>
<li><a href="/wiki/Yew-Kwang_Ng" title="Yew-Kwang Ng">Yew-Kwang Ng</a></li>
<li><a href="/wiki/Toby_Ord" title="Toby Ord">Toby Ord</a></li>
<li><a href="/wiki/Derek_Parfit" title="Derek Parfit">Derek Parfit</a></li>
<li><a href="/wiki/Peter_Singer" title="Peter Singer">Peter Singer</a></li>
<li><a href="/wiki/Eliezer_Yudkowsky" title="Eliezer Yudkowsky">Eliezer Yudkowsky</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Organizations</th><td class="navbox-list navbox-odd hlist" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/80,000_Hours" title="80,000 Hours">80,000 Hours</a></li>
<li><a href="/wiki/Animal_Charity_Evaluators" class="mw-redirect" title="Animal Charity Evaluators">Animal Charity Evaluators</a></li>
<li><a href="/wiki/Animal_Ethics_(organization)" title="Animal Ethics (organization)">Animal Ethics</a></li>
<li><a href="/wiki/Against_Malaria_Foundation" title="Against Malaria Foundation">Against Malaria Foundation</a></li>
<li><a href="/wiki/Center_for_High_Impact_Philanthropy" title="Center for High Impact Philanthropy">Center for High Impact Philanthropy</a></li>
<li><a href="/wiki/Centre_for_the_Study_of_Existential_Risk" title="Centre for the Study of Existential Risk">Centre for the Study of Existential Risk</a></li>
<li><a href="/wiki/Deworm_the_World_Initiative" title="Deworm the World Initiative">Deworm the World Initiative</a></li>
<li><a href="/wiki/Faunalytics" title="Faunalytics">Faunalytics</a></li>
<li><a href="/wiki/Future_of_Humanity_Institute" title="Future of Humanity Institute">Future of Humanity Institute</a></li>
<li><a href="/wiki/Future_of_Life_Institute" title="Future of Life Institute">Future of Life Institute</a></li>
<li><a href="/wiki/Founders_Pledge" title="Founders Pledge">Founders Pledge</a></li>
<li><a href="/wiki/GiveDirectly" title="GiveDirectly">GiveDirectly</a></li>
<li><a href="/wiki/GiveWell" title="GiveWell">GiveWell</a></li>
<li><a href="/wiki/Giving_What_We_Can" title="Giving What We Can">Giving What We Can</a></li>
<li><a href="/wiki/Good_Ventures" title="Good Ventures">Good Ventures</a></li>
<li><a href="/wiki/The_Good_Food_Institute" title="The Good Food Institute">The Good Food Institute</a></li>
<li><a href="/wiki/The_Humane_League" title="The Humane League">The Humane League</a></li>
<li><a href="/wiki/Mercy_for_Animals" title="Mercy for Animals">Mercy for Animals</a></li>
<li><a href="/wiki/Machine_Intelligence_Research_Institute" title="Machine Intelligence Research Institute">Machine Intelligence Research Institute</a></li>
<li><a href="/wiki/Malaria_Consortium" title="Malaria Consortium">Malaria Consortium</a></li>
<li><a href="/wiki/Open_Philanthropy_Project" title="Open Philanthropy Project">Open Philanthropy Project</a></li>
<li><a href="/wiki/Raising_for_Effective_Giving" title="Raising for Effective Giving">Raising for Effective Giving</a></li>
<li><a href="/wiki/Schistosomiasis_Control_Initiative" class="mw-redirect" title="Schistosomiasis Control Initiative">Schistosomiasis Control Initiative</a></li>
<li><a href="/wiki/Sentience_Institute" title="Sentience Institute">Sentience Institute</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Focus areas</th><td class="navbox-list navbox-even hlist" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/Aid_effectiveness" title="Aid effectiveness">Aid effectiveness</a></li>
<li><a href="/wiki/Biotechnology_risk" title="Biotechnology risk">Biotechnology risk</a></li>
<li><a href="/wiki/Global_warming" title="Global warming">Climate change</a></li>
<li><a href="/wiki/Cultured_meat" title="Cultured meat">Cultured meat</a></li>
<li><a href="/wiki/Disease_burden" title="Disease burden">Disease burden</a></li>
<li><a href="/wiki/Economic_stability" title="Economic stability">Economic stability</a></li>
<li><a class="mw-selflink selflink">Existential risk from artificial general intelligence</a></li>
<li><a href="/wiki/Global_catastrophic_risk" title="Global catastrophic risk">Global catastrophic risk</a></li>
<li><a href="/wiki/Global_health" title="Global health">Global health</a></li>
<li><a href="/wiki/Global_poverty" class="mw-redirect" title="Global poverty">Global poverty</a></li>
<li><a href="/wiki/Immigration_reform" title="Immigration reform">Immigration reform</a></li>
<li><a href="/wiki/Intensive_animal_farming" title="Intensive animal farming">Intensive animal farming</a></li>
<li><a href="/wiki/Land_use" title="Land use">Land use reform</a></li>
<li><a href="/wiki/Life_extension" title="Life extension">Life extension</a></li>
<li><a href="/wiki/Malaria_prevention" class="mw-redirect" title="Malaria prevention">Malaria prevention</a></li>
<li><a href="/wiki/Mass_deworming" title="Mass deworming">Mass deworming</a></li>
<li><a href="/wiki/Neglected_tropical_diseases" title="Neglected tropical diseases">Neglected tropical diseases</a></li>
<li><a href="/wiki/Wild_animal_suffering" title="Wild animal suffering">Wild animal suffering</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Literature</th><td class="navbox-list navbox-odd hlist" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><i><a href="/wiki/Doing_Good_Better" title="Doing Good Better">Doing Good Better</a></i></li>
<li><i><a href="/wiki/The_End_of_Animal_Farming" title="The End of Animal Farming">The End of Animal Farming</a></i></li>
<li><i><a href="/wiki/Famine,_Affluence,_and_Morality" title="Famine, Affluence, and Morality">Famine, Affluence, and Morality</a></i></li>
<li><i><a href="/wiki/The_Life_You_Can_Save" title="The Life You Can Save">The Life You Can Save</a></i></li>
<li><i><a href="/wiki/Living_High_and_Letting_Die" title="Living High and Letting Die">Living High and Letting Die</a></i></li>
<li><i><a href="/wiki/The_Most_Good_You_Can_Do" title="The Most Good You Can Do">The Most Good You Can Do</a></i></li>
<li><i><a href="/wiki/Practical_Ethics" title="Practical Ethics">Practical Ethics</a></i></li>
<li><i><a href="/wiki/The_Precipice:_Existential_Risk_and_the_Future_of_Humanity" title="The Precipice: Existential Risk and the Future of Humanity">The Precipice</a></i></li></ul>
</div></td></tr></tbody></table></div>
<div role="navigation" class="navbox" aria-labelledby="Global_catastrophic_risks" style="padding:3px"><table class="nowraplinks hlist mw-collapsible autocollapse navbox-inner" style="border-spacing:0;background:transparent;color:inherit"><tbody><tr><th scope="col" class="navbox-title" colspan="2"><div class="plainlinks hlist navbar mini"><ul><li class="nv-view"><a href="/wiki/Template:Global_catastrophic_risks" title="Template:Global catastrophic risks"><abbr title="View this template" style=";;background:none transparent;border:none;-moz-box-shadow:none;-webkit-box-shadow:none;box-shadow:none; padding:0;">v</abbr></a></li><li class="nv-talk"><a href="/wiki/Template_talk:Global_catastrophic_risks" title="Template talk:Global catastrophic risks"><abbr title="Discuss this template" style=";;background:none transparent;border:none;-moz-box-shadow:none;-webkit-box-shadow:none;box-shadow:none; padding:0;">t</abbr></a></li><li class="nv-edit"><a class="external text" href="https://en.wikipedia.org/w/index.php?title=Template:Global_catastrophic_risks&amp;action=edit"><abbr title="Edit this template" style=";;background:none transparent;border:none;-moz-box-shadow:none;-webkit-box-shadow:none;box-shadow:none; padding:0;">e</abbr></a></li></ul></div><div id="Global_catastrophic_risks" style="font-size:114%;margin:0 4em"><a href="/wiki/Global_catastrophic_risk" title="Global catastrophic risk">Global catastrophic risks</a></div></th></tr><tr><td class="navbox-abovebelow" colspan="2"><div id="*_Future_of_the_Earth_*_Ultimate_fate_of_the_universe">
<ul><li><a href="/wiki/Future_of_Earth" title="Future of Earth">Future of the Earth</a></li>
<li><a href="/wiki/Ultimate_fate_of_the_universe" title="Ultimate fate of the universe">Ultimate fate of the universe</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Technological</th><td class="navbox-list navbox-odd" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/Doomsday_Clock" title="Doomsday Clock">Doomsday Clock</a></li>
<li><a href="/wiki/Gray_goo" title="Gray goo">Gray goo</a></li>
<li><a href="/wiki/Kinetic_bombardment" title="Kinetic bombardment">Kinetic bombardment</a></li>
<li><a href="/wiki/Mutual_assured_destruction" title="Mutual assured destruction">Mutual assured destruction</a>
<ul><li><a href="/wiki/Dead_Hand" title="Dead Hand">Dead Hand</a></li>
<li><a href="/wiki/Doomsday_device" title="Doomsday device">Doomsday device</a></li>
<li><a href="/wiki/Antimatter_weapon" title="Antimatter weapon">Antimatter weapon</a></li></ul></li>
<li><a href="/wiki/Synthetic_intelligence" title="Synthetic intelligence">Synthetic intelligence</a> / <a href="/wiki/Artificial_intelligence" title="Artificial intelligence">Artificial intelligence</a>
<ul><li><a class="mw-selflink selflink">Existential risk from artificial intelligence</a></li>
<li><a href="/wiki/AI_takeover" title="AI takeover">AI takeover</a></li></ul></li>
<li><a href="/wiki/Technological_singularity" title="Technological singularity">Technological singularity</a></li>
<li><a href="/wiki/Transhumanism" title="Transhumanism">Transhumanism</a></li>
<li><a href="/wiki/Year_2000_problem" title="Year 2000 problem">Year 2000 problem</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Sociological</th><td class="navbox-list navbox-even" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/Malthusian_catastrophe" title="Malthusian catastrophe">Malthusian catastrophe</a></li>
<li><a href="/wiki/New_World_Order_(conspiracy_theory)" title="New World Order (conspiracy theory)">New World Order (conspiracy theory)</a></li>
<li><a href="/wiki/Nuclear_holocaust" title="Nuclear holocaust">Nuclear holocaust</a>
<ul><li><a href="/wiki/Nuclear_winter" title="Nuclear winter">winter</a></li>
<li><a href="/wiki/Nuclear_famine" title="Nuclear famine">famine</a></li>
<li><a href="/wiki/Cobalt_bomb" title="Cobalt bomb">cobalt</a></li></ul></li>
<li><a href="/wiki/Societal_collapse" title="Societal collapse">Societal collapse</a></li>
<li><a href="/wiki/Collapsology" title="Collapsology">Collapsology</a></li>
<li><a href="/wiki/World_War_III" title="World War III">World War III</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Ecological</th><td class="navbox-list navbox-odd" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em"></div><table class="nowraplinks navbox-subgroup" style="border-spacing:0"><tbody><tr><th scope="row" class="navbox-group" style="width:1%"><a href="/wiki/Climate_change_(general_concept)" title="Climate change (general concept)">Climate change</a></th><td class="navbox-list navbox-odd" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/Extinction_risk_from_global_warming" title="Extinction risk from global warming">Extinction risk from global warming</a>
<ul><li><a href="/wiki/Tipping_points_in_the_climate_system" title="Tipping points in the climate system">Tipping points in the climate system</a></li></ul></li>
<li><a href="/wiki/Global_terrestrial_stilling" title="Global terrestrial stilling">Global terrestrial stilling</a></li>
<li><a href="/wiki/Global_warming" title="Global warming">Global warming</a></li>
<li><a href="/wiki/Hypercane" title="Hypercane">Hypercane</a></li>
<li><a href="/wiki/Ice_age" title="Ice age">Ice age</a></li>
<li><a href="/wiki/Ecocide" title="Ecocide">Ecocide</a></li>
<li><a href="/wiki/Human_impact_on_the_environment" title="Human impact on the environment">Human impact on the environment</a></li>
<li><a href="/wiki/Ozone_depletion" title="Ozone depletion">Ozone depletion</a></li>
<li><a href="/wiki/Cascade_effect_(ecology)" title="Cascade effect (ecology)">Cascade effect</a></li>
<li><a href="/wiki/Supervolcano" title="Supervolcano">Supervolcano</a>
<ul><li><a href="/wiki/Volcanic_winter" title="Volcanic winter">winter</a></li></ul></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%"><a href="/wiki/Earth_Overshoot_Day" title="Earth Overshoot Day">Earth Overshoot Day</a></th><td class="navbox-list navbox-even" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/Overexploitation" title="Overexploitation">Overexploitation</a></li>
<li><a href="/wiki/Overpopulation" title="Overpopulation">Overpopulation</a>
<ul><li><a href="/wiki/Human_overpopulation" title="Human overpopulation">Human overpopulation</a></li></ul></li></ul>
</div></td></tr></tbody></table><div></div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Biological</th><td class="navbox-list navbox-odd" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em"></div><table class="nowraplinks navbox-subgroup" style="border-spacing:0"><tbody><tr><th scope="row" class="navbox-group" style="width:1%"><a href="/wiki/Extinction" title="Extinction">Extinction</a></th><td class="navbox-list navbox-odd" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/Extinction_event" title="Extinction event">Extinction event</a></li>
<li><a href="/wiki/Human_extinction" title="Human extinction">Human extinction</a></li>
<li><a href="/wiki/Genetic_erosion" title="Genetic erosion">Genetic erosion</a></li>
<li><a href="/wiki/Genetic_pollution" title="Genetic pollution">Genetic pollution</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Others</th><td class="navbox-list navbox-even" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/Dysgenics" title="Dysgenics">Dysgenics</a></li>
<li><a href="/wiki/Pandemic" title="Pandemic">Pandemic</a>
<ul><li><a href="/wiki/Biological_agent" title="Biological agent">Biological agent</a></li></ul></li></ul>
</div></td></tr></tbody></table><div></div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Astronomical</th><td class="navbox-list navbox-odd" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/Big_Crunch" title="Big Crunch">Big Crunch</a></li>
<li><a href="/wiki/Big_Rip" title="Big Rip">Big Rip</a></li>
<li><a href="/wiki/Coronal_mass_ejection" title="Coronal mass ejection">Coronal mass ejection</a></li>
<li><a href="/wiki/Gamma-ray_burst" title="Gamma-ray burst">Gamma-ray burst</a></li>
<li><a href="/wiki/Impact_event" title="Impact event">Impact event</a>
<ul><li><a href="/wiki/Asteroid_impact_avoidance" title="Asteroid impact avoidance">Asteroid impact avoidance</a></li>
<li><a href="/wiki/Potentially_hazardous_object" title="Potentially hazardous object">Potentially hazardous object</a></li></ul></li>
<li><a href="/wiki/Near-Earth_supernova" title="Near-Earth supernova">Near-Earth supernova</a></li>
<li><a href="/wiki/Solar_flare" title="Solar flare">Solar flare</a></li>
<li><a href="/wiki/Stellar_collision" title="Stellar collision">Stellar collision</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Mythological</th><td class="navbox-list navbox-odd" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em"></div><table class="nowraplinks navbox-subgroup" style="border-spacing:0"><tbody><tr><th scope="row" class="navbox-group" style="width:1%"><a href="/wiki/Eschatology" title="Eschatology">Eschatology</a></th><td class="navbox-list navbox-even" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/Buddhist_eschatology" title="Buddhist eschatology">Buddhist</a></li>
<li><a href="/wiki/Hindu_eschatology" title="Hindu eschatology">Hindu</a></li>
<li><a href="/wiki/Last_Judgment" title="Last Judgment">Last Judgment</a>
<ul><li><a href="/wiki/Christian_eschatology" title="Christian eschatology">Christian</a>
<ul><li><a href="/wiki/Book_of_Revelation" title="Book of Revelation">Book of Revelation</a></li></ul></li>
<li><a href="/wiki/Islamic_eschatology" title="Islamic eschatology">Islamic</a></li>
<li><a href="/wiki/Jewish_eschatology" title="Jewish eschatology">Jewish</a></li></ul></li>
<li><a href="/wiki/Ragnar%C3%B6k" title="Ragnarök">Norse</a></li>
<li><a href="/wiki/Frashokereti" title="Frashokereti">Zoroastrian</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Others</th><td class="navbox-list navbox-odd" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/2011_end_times_prediction" title="2011 end times prediction">2011 end times prediction</a></li>
<li><a href="/wiki/2012_phenomenon" title="2012 phenomenon">2012 phenomenon</a></li>
<li><a href="/wiki/Apocalyptic_literature" title="Apocalyptic literature">Apocalypse</a></li>
<li><a href="/wiki/Armageddon" title="Armageddon">Armageddon</a></li>
<li><a href="/wiki/Blood_moon_prophecy" title="Blood moon prophecy">Blood moon prophecy</a></li>
<li><a href="/wiki/End_time" title="End time">End time</a></li>
<li><a href="/wiki/List_of_dates_predicted_for_apocalyptic_events" title="List of dates predicted for apocalyptic events">List of dates predicted for apocalyptic events</a></li>
<li><a href="/wiki/Nibiru_cataclysm" title="Nibiru cataclysm">Nibiru cataclysm</a></li>
<li><a href="/wiki/Rapture" title="Rapture">Rapture</a></li>
<li><a href="/wiki/Revelation_12_sign_prophecy" title="Revelation 12 sign prophecy">Revelation 12 sign prophecy</a></li>
<li><a href="/wiki/Third_Temple" title="Third Temple">Third Temple</a></li></ul>
</div></td></tr></tbody></table><div></div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Fictional</th><td class="navbox-list navbox-even" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/Alien_invasion" title="Alien invasion">Alien invasion</a></li>
<li><a href="/wiki/Apocalyptic_and_post-apocalyptic_fiction" title="Apocalyptic and post-apocalyptic fiction">Apocalyptic and post-apocalyptic fiction</a>
<ul><li><a href="/wiki/List_of_apocalyptic_and_post-apocalyptic_fiction" title="List of apocalyptic and post-apocalyptic fiction">List of apocalyptic and post-apocalyptic fiction</a></li>
<li><a href="/wiki/List_of_apocalyptic_films" title="List of apocalyptic films">List of apocalyptic films</a></li></ul></li>
<li><a href="/wiki/Climate_fiction" title="Climate fiction">Climate fiction</a></li>
<li><a href="/wiki/Disaster_film" title="Disaster film">Disaster films</a>
<ul><li><a href="/wiki/List_of_disaster_films" title="List of disaster films">List of disaster films</a></li></ul></li>
<li><a href="/wiki/List_of_fictional_doomsday_devices" title="List of fictional doomsday devices">List of fictional doomsday devices</a></li>
<li><a href="/wiki/Zombie_apocalypse" title="Zombie apocalypse">Zombie apocalypse</a></li></ul>
</div></td></tr><tr><td class="navbox-abovebelow" colspan="2"><div>
<ul><li><img alt="Category" src="//upload.wikimedia.org/wikipedia/en/thumb/4/48/Folder_Hexagonal_Icon.svg/16px-Folder_Hexagonal_Icon.svg.png" decoding="async" title="Category" width="16" height="14" srcset="//upload.wikimedia.org/wikipedia/en/thumb/4/48/Folder_Hexagonal_Icon.svg/24px-Folder_Hexagonal_Icon.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/4/48/Folder_Hexagonal_Icon.svg/32px-Folder_Hexagonal_Icon.svg.png 2x" data-file-width="36" data-file-height="31" /> Categories
<ul><li><a href="/wiki/Category:Apocalypticism" title="Category:Apocalypticism">Apocalypticism</a></li>
<li><a href="/wiki/Category:Future_problems" title="Category:Future problems">Future problems</a></li>
<li><a href="/wiki/Category:Hazards" title="Category:Hazards">Hazards</a></li>
<li><a href="/wiki/Category:Risk_analysis" title="Category:Risk analysis">Risk analysis</a></li>
<li><a href="/wiki/Category:Doomsday_scenarios" title="Category:Doomsday scenarios">Doomsday scenarios</a></li></ul></li></ul>
</div></td></tr></tbody></table></div>
<!-- 
NewPP limit report
Parsed by mw1282
Cached time: 20200415092035
Cache expiry: 2592000
Dynamic content: false
Complications: [vary‐revision‐sha1]
CPU time usage: 1.432 seconds
Real time usage: 1.737 seconds
Preprocessor visited node count: 7063/1000000
Post‐expand include size: 303516/2097152 bytes
Template argument size: 12379/2097152 bytes
Highest expansion depth: 13/40
Expensive parser function count: 7/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 394978/5000000 bytes
Number of Wikibase entities loaded: 5/400
Lua time usage: 0.780/10.000 seconds
Lua memory usage: 8.38 MB/50 MB
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00% 1444.321      1 -total
 69.83% 1008.569      1 Template:Reflist
 19.93%  287.804     31 Template:Cite_journal
 15.53%  224.321     45 Template:Cite_news
 10.04%  145.008     16 Template:Cite_book
  5.23%   75.484     15 Template:Cite_web
  3.61%   52.187      1 Template:Use_dmy_dates
  3.59%   51.891      6 Template:Navbox
  3.34%   48.283      2 Template:Citation
  2.97%   42.922      1 Template:Citation_needed
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:46583121-0!canonical!math=5 and timestamp 20200415092033 and revision id 951068096
 -->
</div><noscript><img src="//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" alt="" title="" width="1" height="1" style="border: none; position: absolute;" /></noscript></div>
		<div class="printfooter">Retrieved from "<a dir="ltr" href="https://en.wikipedia.org/w/index.php?title=Existential_risk_from_artificial_general_intelligence&amp;oldid=951068096">https://en.wikipedia.org/w/index.php?title=Existential_risk_from_artificial_general_intelligence&amp;oldid=951068096</a>"</div>
		<div id="catlinks" class="catlinks" data-mw="interface"><div id="mw-normal-catlinks" class="mw-normal-catlinks"><a href="/wiki/Help:Category" title="Help:Category">Categories</a>: <ul><li><a href="/wiki/Category:Existential_risk_from_artificial_general_intelligence" title="Category:Existential risk from artificial general intelligence">Existential risk from artificial general intelligence</a></li><li><a href="/wiki/Category:Futures_studies" title="Category:Futures studies">Futures studies</a></li><li><a href="/wiki/Category:Future_problems" title="Category:Future problems">Future problems</a></li><li><a href="/wiki/Category:Human_extinction" title="Category:Human extinction">Human extinction</a></li><li><a href="/wiki/Category:Technology_hazards" title="Category:Technology hazards">Technology hazards</a></li><li><a href="/wiki/Category:Doomsday_scenarios" title="Category:Doomsday scenarios">Doomsday scenarios</a></li></ul></div><div id="mw-hidden-catlinks" class="mw-hidden-catlinks mw-hidden-cats-hidden">Hidden categories: <ul><li><a href="/wiki/Category:Webarchive_template_wayback_links" title="Category:Webarchive template wayback links">Webarchive template wayback links</a></li><li><a href="/wiki/Category:CS1_maint:_multiple_names:_authors_list" title="Category:CS1 maint: multiple names: authors list">CS1 maint: multiple names: authors list</a></li><li><a href="/wiki/Category:CS1_errors:_missing_periodical" title="Category:CS1 errors: missing periodical">CS1 errors: missing periodical</a></li><li><a href="/wiki/Category:Use_dmy_dates_from_May_2018" title="Category:Use dmy dates from May 2018">Use dmy dates from May 2018</a></li><li><a href="/wiki/Category:Articles_with_short_description" title="Category:Articles with short description">Articles with short description</a></li><li><a href="/wiki/Category:All_articles_with_unsourced_statements" title="Category:All articles with unsourced statements">All articles with unsourced statements</a></li><li><a href="/wiki/Category:Articles_with_unsourced_statements_from_November_2017" title="Category:Articles with unsourced statements from November 2017">Articles with unsourced statements from November 2017</a></li></ul></div></div>
		<div class="visualClear"></div>
		
	</div>
</div>
<div id='mw-data-after-content'>
	<div class="read-more-container"></div>
</div>

<div id="mw-navigation">
    <h2>Navigation menu</h2>
    <div id="mw-head">
        
        <div id="p-personal" role="navigation" class="" aria-labelledby="p-personal-label">
        	<h3 id="p-personal-label">Personal tools</h3>
        	<ul >
        		
        		<li id="pt-anonuserpage">Not logged in</li>
        		<li id="pt-anontalk"><a href="/wiki/Special:MyTalk" title="Discussion about edits from this IP address [n]" accesskey="n">Talk</a></li><li id="pt-anoncontribs"><a href="/wiki/Special:MyContributions" title="A list of edits made from this IP address [y]" accesskey="y">Contributions</a></li><li id="pt-createaccount"><a href="/w/index.php?title=Special:CreateAccount&amp;returnto=Existential+risk+from+artificial+general+intelligence" title="You are encouraged to create an account and log in; however, it is not mandatory">Create account</a></li><li id="pt-login"><a href="/w/index.php?title=Special:UserLogin&amp;returnto=Existential+risk+from+artificial+general+intelligence" title="You&#039;re encouraged to log in; however, it&#039;s not mandatory. [o]" accesskey="o">Log in</a></li>
        	</ul>
        </div>
        <div id="left-navigation">
            <div id="p-namespaces" role="navigation" class="vectorTabs " aria-labelledby="p-namespaces-label">
            	<h3 id="p-namespaces-label">Namespaces</h3>
            	<ul >
            		<li id="ca-nstab-main" class="selected"><a href="/wiki/Existential_risk_from_artificial_general_intelligence" title="View the content page [c]" accesskey="c">Article</a></li><li id="ca-talk"><a href="/wiki/Talk:Existential_risk_from_artificial_general_intelligence" rel="discussion" title="Discuss improvements to the content page [t]" accesskey="t">Talk</a></li>
            	</ul>
            </div>
            <div id="p-variants" role="navigation" class="vectorMenu emptyPortlet" aria-labelledby="p-variants-label">
            	<input type="checkbox" class="vectorMenuCheckbox" aria-labelledby="p-variants-label" />
            	<h3 id="p-variants-label">
            		<span>Variants</span>
            	</h3>
            	<ul class="menu" >
            		
            	</ul>
            </div>
        </div>
        <div id="right-navigation">
            <div id="p-views" role="navigation" class="vectorTabs " aria-labelledby="p-views-label">
            	<h3 id="p-views-label">Views</h3>
            	<ul >
            		<li id="ca-view" class="collapsible selected"><a href="/wiki/Existential_risk_from_artificial_general_intelligence">Read</a></li><li id="ca-edit" class="collapsible"><a href="/w/index.php?title=Existential_risk_from_artificial_general_intelligence&amp;action=edit" title="Edit this page [e]" accesskey="e">Edit</a></li><li id="ca-history" class="collapsible"><a href="/w/index.php?title=Existential_risk_from_artificial_general_intelligence&amp;action=history" title="Past revisions of this page [h]" accesskey="h">View history</a></li>
            	</ul>
            </div>
            <div id="p-cactions" role="navigation" class="vectorMenu emptyPortlet" aria-labelledby="p-cactions-label">
            	<input type="checkbox" class="vectorMenuCheckbox" aria-labelledby="p-cactions-label" />
            	<h3 id="p-cactions-label">
            		<span>More</span>
            	</h3>
            	<ul class="menu" >
            		
            	</ul>
            </div>
            <div id="p-search" role="search">
            	<h3 >
            		<label for="searchInput">Search</label>
            	</h3>
            	<form action="/w/index.php" id="searchform">
            		<div id="simpleSearch">
            			<input type="search" name="search" placeholder="Search Wikipedia" title="Search Wikipedia [f]" accesskey="f" id="searchInput"/>
            			<input type="hidden" value="Special:Search" name="title"/>
            			<input type="submit" name="fulltext" value="Search" title="Search Wikipedia for this text" id="mw-searchButton" class="searchButton mw-fallbackSearchButton"/>
            			<input type="submit" name="go" value="Go" title="Go to a page with this exact name if it exists" id="searchButton" class="searchButton"/>
            		</div>
            	</form>
            </div>
        </div>
    </div>
    
    <div id="mw-panel">
    	<div id="p-logo" role="banner">
    		<a  title="Visit the main page" class="mw-wiki-logo" href="/wiki/Main_Page"></a>
    	</div>
    	<div class="portal" role="navigation" id="p-navigation"  aria-labelledby="p-navigation-label">
    		<h3  id="p-navigation-label">
    			Navigation
    		</h3>
    		<div class="body">
    			<ul><li id="n-mainpage-description"><a href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z">Main page</a></li><li id="n-contents"><a href="/wiki/Wikipedia:Contents" title="Guides to browsing Wikipedia">Contents</a></li><li id="n-featuredcontent"><a href="/wiki/Wikipedia:Featured_content" title="Featured content – the best of Wikipedia">Featured content</a></li><li id="n-currentevents"><a href="/wiki/Portal:Current_events" title="Find background information on current events">Current events</a></li><li id="n-randompage"><a href="/wiki/Special:Random" title="Load a random article [x]" accesskey="x">Random article</a></li><li id="n-sitesupport"><a href="https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&amp;utm_medium=sidebar&amp;utm_campaign=C13_en.wikipedia.org&amp;uselang=en" title="Support us">Donate to Wikipedia</a></li><li id="n-shoplink"><a href="//shop.wikimedia.org" title="Visit the Wikipedia store">Wikipedia store</a></li></ul>
    			
    		</div>
    	</div>
    	
    	<div class="portal" role="navigation" id="p-interaction"  aria-labelledby="p-interaction-label">
    		<h3  id="p-interaction-label">
    			Interaction
    		</h3>
    		<div class="body">
    			<ul><li id="n-help"><a href="/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia">Help</a></li><li id="n-aboutsite"><a href="/wiki/Wikipedia:About" title="Find out about Wikipedia">About Wikipedia</a></li><li id="n-portal"><a href="/wiki/Wikipedia:Community_portal" title="About the project, what you can do, where to find things">Community portal</a></li><li id="n-recentchanges"><a href="/wiki/Special:RecentChanges" title="A list of recent changes in the wiki [r]" accesskey="r">Recent changes</a></li><li id="n-contactpage"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia">Contact page</a></li></ul>
    			
    		</div>
    	</div>
    	
    	<div class="portal" role="navigation" id="p-tb"  aria-labelledby="p-tb-label">
    		<h3  id="p-tb-label">
    			Tools
    		</h3>
    		<div class="body">
    			<ul><li id="t-whatlinkshere"><a href="/wiki/Special:WhatLinksHere/Existential_risk_from_artificial_general_intelligence" title="List of all English Wikipedia pages containing links to this page [j]" accesskey="j">What links here</a></li><li id="t-recentchangeslinked"><a href="/wiki/Special:RecentChangesLinked/Existential_risk_from_artificial_general_intelligence" rel="nofollow" title="Recent changes in pages linked from this page [k]" accesskey="k">Related changes</a></li><li id="t-upload"><a href="/wiki/Wikipedia:File_Upload_Wizard" title="Upload files [u]" accesskey="u">Upload file</a></li><li id="t-specialpages"><a href="/wiki/Special:SpecialPages" title="A list of all special pages [q]" accesskey="q">Special pages</a></li><li id="t-permalink"><a href="/w/index.php?title=Existential_risk_from_artificial_general_intelligence&amp;oldid=951068096" title="Permanent link to this revision of the page">Permanent link</a></li><li id="t-info"><a href="/w/index.php?title=Existential_risk_from_artificial_general_intelligence&amp;action=info" title="More information about this page">Page information</a></li><li id="t-wikibase"><a href="https://www.wikidata.org/wiki/Special:EntityPage/Q21715237" title="Link to connected data repository item [g]" accesskey="g">Wikidata item</a></li><li id="t-cite"><a href="/w/index.php?title=Special:CiteThisPage&amp;page=Existential_risk_from_artificial_general_intelligence&amp;id=951068096&amp;wpFormIdentifier=titleform" title="Information on how to cite this page">Cite this page</a></li></ul>
    			
    		</div>
    	</div>
    	
    	<div class="portal" role="navigation" id="p-coll-print_export"  aria-labelledby="p-coll-print_export-label">
    		<h3  id="p-coll-print_export-label">
    			Print/export
    		</h3>
    		<div class="body">
    			<ul><li id="coll-create_a_book"><a href="/w/index.php?title=Special:Book&amp;bookcmd=book_creator&amp;referer=Existential+risk+from+artificial+general+intelligence">Create a book</a></li><li id="coll-download-as-rl"><a href="/w/index.php?title=Special:ElectronPdf&amp;page=Existential+risk+from+artificial+general+intelligence&amp;action=show-download-screen">Download as PDF</a></li><li id="t-print"><a href="/w/index.php?title=Existential_risk_from_artificial_general_intelligence&amp;printable=yes" title="Printable version of this page [p]" accesskey="p">Printable version</a></li></ul>
    			
    		</div>
    	</div>
    	
    	<div class="portal" role="navigation" id="p-lang"  aria-labelledby="p-lang-label">
    		<h3  id="p-lang-label">
    			Languages
    		</h3>
    		<div class="body">
    			<ul><li class="interlanguage-link interwiki-ar"><a href="https://ar.wikipedia.org/wiki/%D8%A7%D9%84%D8%AE%D8%B7%D8%B1_%D8%A7%D9%84%D9%88%D8%AC%D9%88%D8%AF%D9%8A_%D9%85%D9%86_%D8%A7%D9%84%D8%B0%D9%83%D8%A7%D8%A1_%D8%A7%D9%84%D8%A7%D8%B5%D8%B7%D9%86%D8%A7%D8%B9%D9%8A_%D8%A7%D9%84%D8%B9%D8%A7%D9%85" title="الخطر الوجودي من الذكاء الاصطناعي العام – Arabic" lang="ar" hreflang="ar" class="interlanguage-link-target">العربية</a></li><li class="interlanguage-link interwiki-cs"><a href="https://cs.wikipedia.org/wiki/Existen%C4%8Dn%C3%AD_rizika_v%C3%BDvoje_um%C4%9Bl%C3%A9_inteligence" title="Existenční rizika vývoje umělé inteligence – Czech" lang="cs" hreflang="cs" class="interlanguage-link-target">Čeština</a></li><li class="interlanguage-link interwiki-id"><a href="https://id.wikipedia.org/wiki/Krisis_eksistensial_dari_kecerdasan_buatan" title="Krisis eksistensial dari kecerdasan buatan – Indonesian" lang="id" hreflang="id" class="interlanguage-link-target">Bahasa Indonesia</a></li><li class="interlanguage-link interwiki-sv"><a href="https://sv.wikipedia.org/wiki/Existentiell_risk_orsakad_av_artificiell_generell_intelligens" title="Existentiell risk orsakad av artificiell generell intelligens – Swedish" lang="sv" hreflang="sv" class="interlanguage-link-target">Svenska</a></li></ul>
    			<div class="after-portlet after-portlet-lang"><span class="wb-langlinks-edit wb-langlinks-link"><a href="https://www.wikidata.org/wiki/Special:EntityPage/Q21715237#sitelinks-wikipedia" title="Edit interlanguage links" class="wbc-editpage">Edit links</a></span></div>
    		</div>
    	</div>
    	
    </div>
</div>

<div id="footer" role="contentinfo" >
	<ul id="footer-info" class="">
		<li id="footer-info-lastmod"> This page was last edited on 15 April 2020, at 09:20<span class="anonymous-show">&#160;(UTC)</span>.</li>
		<li id="footer-info-copyright">Text is available under the <a rel="license" href="//en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License">Creative Commons Attribution-ShareAlike License</a><a rel="license" href="//creativecommons.org/licenses/by-sa/3.0/" style="display:none;"></a>;
additional terms may apply.  By using this site, you agree to the <a href="//foundation.wikimedia.org/wiki/Terms_of_Use">Terms of Use</a> and <a href="//foundation.wikimedia.org/wiki/Privacy_policy">Privacy Policy</a>. Wikipedia® is a registered trademark of the <a href="//www.wikimediafoundation.org/">Wikimedia Foundation, Inc.</a>, a non-profit organization.</li>
	</ul>
	<ul id="footer-places" class="">
		<li id="footer-places-privacy"><a href="https://foundation.wikimedia.org/wiki/Privacy_policy" class="extiw" title="wmf:Privacy policy">Privacy policy</a></li>
		<li id="footer-places-about"><a href="/wiki/Wikipedia:About" title="Wikipedia:About">About Wikipedia</a></li>
		<li id="footer-places-disclaimer"><a href="/wiki/Wikipedia:General_disclaimer" title="Wikipedia:General disclaimer">Disclaimers</a></li>
		<li id="footer-places-contact"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us">Contact Wikipedia</a></li>
		<li id="footer-places-developers"><a href="https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute">Developers</a></li>
		<li id="footer-places-statslink"><a href="https://stats.wikimedia.org/#/en.wikipedia.org">Statistics</a></li>
		<li id="footer-places-cookiestatement"><a href="https://foundation.wikimedia.org/wiki/Cookie_statement">Cookie statement</a></li>
		<li id="footer-places-mobileview"><a href="//en.m.wikipedia.org/w/index.php?title=Existential_risk_from_artificial_general_intelligence&amp;mobileaction=toggle_view_mobile" class="noprint stopMobileRedirectToggle">Mobile view</a></li>
	</ul>
	<ul id="footer-icons" class="noprint">
		<li id="footer-copyrightico"><a href="https://wikimediafoundation.org/"><img src="/static/images/wikimedia-button.png" srcset="/static/images/wikimedia-button-1.5x.png 1.5x, /static/images/wikimedia-button-2x.png 2x" width="88" height="31" alt="Wikimedia Foundation"/></a></li>
		<li id="footer-poweredbyico"><a href="https://www.mediawiki.org/"><img src="/static/images/poweredby_mediawiki_88x31.png" alt="Powered by MediaWiki" srcset="/static/images/poweredby_mediawiki_132x47.png 1.5x, /static/images/poweredby_mediawiki_176x62.png 2x" width="88" height="31"/></a></li>
	</ul>
	<div style="clear: both;"></div>
</div>

<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgPageParseReport":{"limitreport":{"cputime":"1.432","walltime":"1.737","ppvisitednodes":{"value":7063,"limit":1000000},"postexpandincludesize":{"value":303516,"limit":2097152},"templateargumentsize":{"value":12379,"limit":2097152},"expansiondepth":{"value":13,"limit":40},"expensivefunctioncount":{"value":7,"limit":500},"unstrip-depth":{"value":1,"limit":20},"unstrip-size":{"value":394978,"limit":5000000},"entityaccesscount":{"value":5,"limit":400},"timingprofile":["100.00% 1444.321      1 -total"," 69.83% 1008.569      1 Template:Reflist"," 19.93%  287.804     31 Template:Cite_journal"," 15.53%  224.321     45 Template:Cite_news"," 10.04%  145.008     16 Template:Cite_book","  5.23%   75.484     15 Template:Cite_web","  3.61%   52.187      1 Template:Use_dmy_dates","  3.59%   51.891      6 Template:Navbox","  3.34%   48.283      2 Template:Citation","  2.97%   42.922      1 Template:Citation_needed"]},"scribunto":{"limitreport-timeusage":{"value":"0.780","limit":"10.000"},"limitreport-memusage":{"value":8783876,"limit":52428800}},"cachereport":{"origin":"mw1282","timestamp":"20200415092035","ttl":2592000,"transientcontent":false}}});});</script>
<script type="application/ld+json">{"@context":"https:\/\/schema.org","@type":"Article","name":"Existential risk from artificial general intelligence","url":"https:\/\/en.wikipedia.org\/wiki\/Existential_risk_from_artificial_general_intelligence","sameAs":"http:\/\/www.wikidata.org\/entity\/Q21715237","mainEntity":"http:\/\/www.wikidata.org\/entity\/Q21715237","author":{"@type":"Organization","name":"Contributors to Wikimedia projects"},"publisher":{"@type":"Organization","name":"Wikimedia Foundation, Inc.","logo":{"@type":"ImageObject","url":"https:\/\/www.wikimedia.org\/static\/images\/wmf-hor-googpub.png"}},"datePublished":"2015-05-01T21:17:43Z","dateModified":"2020-04-15T09:20:28Z","headline":"hypothesis that artificial general intelligence could result in human extinction"}</script>
<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgBackendResponseTime":127,"wgHostname":"mw1268"});});</script></body></html>
