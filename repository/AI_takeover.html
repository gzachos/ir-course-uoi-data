
<!DOCTYPE html>
<html class="client-nojs" lang="en" dir="ltr">
<head>
<meta charset="UTF-8"/>
<title>AI takeover - Wikipedia</title>
<script>document.documentElement.className="client-js";RLCONF={"wgBreakFrames":!1,"wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgRequestId":"XpdSJwpAIH4AApOqkmIAAACH","wgCSPNonce":!1,"wgCanonicalNamespace":"","wgCanonicalSpecialPageName":!1,"wgNamespaceNumber":0,"wgPageName":"AI_takeover","wgTitle":"AI takeover","wgCurRevisionId":951139246,"wgRevisionId":951139246,"wgArticleId":813176,"wgIsArticle":!0,"wgIsRedirect":!1,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":["Webarchive template wayback links","Articles needing additional references from January 2020","All articles needing additional references","Articles with short description","All articles with specifically marked weasel-worded phrases","Articles with specifically marked weasel-worded phrases from January 2020",
"All articles with unsourced statements","Articles with unsourced statements from January 2020","All accuracy disputes","Articles with disputed statements from January 2020","Doomsday scenarios","Future problems","Existential risk from artificial general intelligence"],"wgPageContentLanguage":"en","wgPageContentModel":"wikitext","wgRelevantPageName":"AI_takeover","wgRelevantArticleId":813176,"wgIsProbablyEditable":!0,"wgRelevantPageIsProbablyEditable":!0,"wgRestrictionEdit":[],"wgRestrictionMove":[],"wgMediaViewerOnClick":!0,"wgMediaViewerEnabledByDefault":!0,"wgPopupsReferencePreviews":!1,"wgPopupsConflictsWithNavPopupGadget":!1,"wgVisualEditor":{"pageLanguageCode":"en","pageLanguageDir":"ltr","pageVariantFallbacks":"en"},"wgMFDisplayWikibaseDescriptions":{"search":!0,"nearby":!0,"watchlist":!0,"tagline":!1},"wgWMESchemaEditAttemptStepOversample":!1,"wgULSCurrentAutonym":"English","wgNoticeProject":"wikipedia","wgWikibaseItemId":"Q2254427",
"wgCentralAuthMobileDomain":!1,"wgEditSubmitButtonLabelPublish":!0};RLSTATE={"ext.globalCssJs.user.styles":"ready","site.styles":"ready","noscript":"ready","user.styles":"ready","ext.globalCssJs.user":"ready","user":"ready","user.options":"loading","ext.cite.styles":"ready","jquery.makeCollapsible.styles":"ready","mediawiki.toc.styles":"ready","skins.vector.styles.legacy":"ready","wikibase.client.init":"ready","ext.visualEditor.desktopArticleTarget.noscript":"ready","ext.uls.interlanguage":"ready","ext.wikimediaBadges":"ready"};RLPAGEMODULES=["ext.cite.ux-enhancements","site","mediawiki.page.startup","skins.vector.js","mediawiki.page.ready","jquery.makeCollapsible","mediawiki.toc","ext.gadget.ReferenceTooltips","ext.gadget.charinsert","ext.gadget.refToolbar","ext.gadget.extra-toolbar-buttons","ext.gadget.switcher","ext.centralauth.centralautologin","mmv.head","mmv.bootstrap.autostart","ext.popups","ext.visualEditor.desktopArticleTarget.init","ext.visualEditor.targetLoader",
"ext.eventLogging","ext.wikimediaEvents","ext.navigationTiming","ext.uls.compactlinks","ext.uls.interface","ext.cx.eventlogging.campaigns","ext.quicksurveys.init","ext.centralNotice.geoIP","ext.centralNotice.startUp"];</script>
<script>(RLQ=window.RLQ||[]).push(function(){mw.loader.implement("user.options@1hzgi",function($,jQuery,require,module){/*@nomin*/mw.user.tokens.set({"patrolToken":"+\\","watchToken":"+\\","csrfToken":"+\\"});
});});</script>
<link rel="stylesheet" href="/w/load.php?lang=en&amp;modules=ext.cite.styles%7Cext.uls.interlanguage%7Cext.visualEditor.desktopArticleTarget.noscript%7Cext.wikimediaBadges%7Cjquery.makeCollapsible.styles%7Cmediawiki.toc.styles%7Cskins.vector.styles.legacy%7Cwikibase.client.init&amp;only=styles&amp;skin=vector"/>
<script async="" src="/w/load.php?lang=en&amp;modules=startup&amp;only=scripts&amp;raw=1&amp;skin=vector"></script>
<meta name="ResourceLoaderDynamicStyles" content=""/>
<link rel="stylesheet" href="/w/load.php?lang=en&amp;modules=site.styles&amp;only=styles&amp;skin=vector"/>
<meta name="generator" content="MediaWiki 1.35.0-wmf.27"/>
<meta name="referrer" content="origin"/>
<meta name="referrer" content="origin-when-crossorigin"/>
<meta name="referrer" content="origin-when-cross-origin"/>
<meta property="og:image" content="https://upload.wikimedia.org/wikipedia/commons/d/d9/Capek_RUR.jpg"/>
<link rel="alternate" type="application/x-wiki" title="Edit this page" href="/w/index.php?title=AI_takeover&amp;action=edit"/>
<link rel="edit" title="Edit this page" href="/w/index.php?title=AI_takeover&amp;action=edit"/>
<link rel="apple-touch-icon" href="/static/apple-touch/wikipedia.png"/>
<link rel="shortcut icon" href="/static/favicon/wikipedia.ico"/>
<link rel="search" type="application/opensearchdescription+xml" href="/w/opensearch_desc.php" title="Wikipedia (en)"/>
<link rel="EditURI" type="application/rsd+xml" href="//en.wikipedia.org/w/api.php?action=rsd"/>
<link rel="license" href="//creativecommons.org/licenses/by-sa/3.0/"/>
<link rel="alternate" type="application/atom+xml" title="Wikipedia Atom feed" href="/w/index.php?title=Special:RecentChanges&amp;feed=atom"/>
<link rel="canonical" href="https://en.wikipedia.org/wiki/AI_takeover"/>
<link rel="dns-prefetch" href="//login.wikimedia.org"/>
<link rel="dns-prefetch" href="//meta.wikimedia.org" />
<!--[if lt IE 9]><script src="/w/resources/lib/html5shiv/html5shiv.js"></script><![endif]-->
</head>
<body class="mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject mw-editable page-AI_takeover rootpage-AI_takeover skin-vector action-view">
<div id="mw-page-base" class="noprint"></div>
<div id="mw-head-base" class="noprint"></div>
<div id="content" class="mw-body" role="main">
	<a id="top"></a>
	<div id="siteNotice" class="mw-body-content"><!-- CentralNotice --></div>
	<div class="mw-indicators mw-body-content">
</div>

	<h1 id="firstHeading" class="firstHeading" lang="en">AI takeover</h1>
	
	<div id="bodyContent" class="mw-body-content">
		<div id="siteSub" class="noprint">From Wikipedia, the free encyclopedia</div>
		<div id="contentSub"></div>
		
		
		<div id="jump-to-nav"></div>
		<a class="mw-jump-link" href="#mw-head">Jump to navigation</a>
		<a class="mw-jump-link" href="#p-search">Jump to search</a>
		<div id="mw-content-text" lang="en" dir="ltr" class="mw-content-ltr"><div class="mw-parser-output"><table class="box-More_citations_needed plainlinks metadata ambox ambox-content ambox-Refimprove" role="presentation"><tbody><tr><td class="mbox-image"><div style="width:52px"><a href="/wiki/File:Question_book-new.svg" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/en/thumb/9/99/Question_book-new.svg/50px-Question_book-new.svg.png" decoding="async" width="50" height="39" srcset="//upload.wikimedia.org/wikipedia/en/thumb/9/99/Question_book-new.svg/75px-Question_book-new.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/9/99/Question_book-new.svg/100px-Question_book-new.svg.png 2x" data-file-width="512" data-file-height="399" /></a></div></td><td class="mbox-text"><div class="mbox-text-span">This article <b>needs additional citations for <a href="/wiki/Wikipedia:Verifiability" title="Wikipedia:Verifiability">verification</a></b>.<span class="hide-when-compact"> Please help <a class="external text" href="https://en.wikipedia.org/w/index.php?title=AI_takeover&amp;action=edit">improve this article</a> by <a href="/wiki/Help:Introduction_to_referencing_with_Wiki_Markup/1" title="Help:Introduction to referencing with Wiki Markup/1">adding citations to reliable sources</a>. Unsourced material may be challenged and removed.<br /><small><span class="plainlinks"><i>Find sources:</i>&#160;<a rel="nofollow" class="external text" href="//www.google.com/search?as_eq=wikipedia&amp;q=%22AI+takeover%22">"AI takeover"</a>&#160;–&#160;<a rel="nofollow" class="external text" href="//www.google.com/search?tbm=nws&amp;q=%22AI+takeover%22+-wikipedia">news</a>&#160;<b>·</b> <a rel="nofollow" class="external text" href="//www.google.com/search?&amp;q=%22AI+takeover%22+site:news.google.com/newspapers&amp;source=newspapers">newspapers</a>&#160;<b>·</b> <a rel="nofollow" class="external text" href="//www.google.com/search?tbs=bks:1&amp;q=%22AI+takeover%22+-wikipedia">books</a>&#160;<b>·</b> <a rel="nofollow" class="external text" href="//scholar.google.com/scholar?q=%22AI+takeover%22">scholar</a>&#160;<b>·</b> <a rel="nofollow" class="external text" href="https://www.jstor.org/action/doBasicSearch?Query=%22AI+takeover%22&amp;acc=on&amp;wc=on">JSTOR</a></span></small></span>  <small class="date-container"><i>(<span class="date">January 2020</span>)</i></small><small class="hide-when-compact"><i> (<a href="/wiki/Help:Maintenance_template_removal" title="Help:Maintenance template removal">Learn how and when to remove this template message</a>)</i></small></div></td></tr></tbody></table>
<div class="shortdescription nomobile noexcerpt noprint searchaux" style="display:none">Artificial intelligence dominating Earth</div>
<div class="thumb tright"><div class="thumbinner" style="width:222px;"><a href="/wiki/File:Capek_RUR.jpg" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/thumb/d/d9/Capek_RUR.jpg/220px-Capek_RUR.jpg" decoding="async" width="220" height="172" class="thumbimage" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/d/d9/Capek_RUR.jpg/330px-Capek_RUR.jpg 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/d/d9/Capek_RUR.jpg/440px-Capek_RUR.jpg 2x" data-file-width="566" data-file-height="442" /></a>  <div class="thumbcaption"><div class="magnify"><a href="/wiki/File:Capek_RUR.jpg" class="internal" title="Enlarge"></a></div>Robots revolt in <i><a href="/wiki/R.U.R." title="R.U.R.">R.U.R.</a></i>, a 1920 play</div></div></div>
<p>An <b>AI takeover</b> is a hypothetical scenario in which <a href="/wiki/Artificial_intelligence" title="Artificial intelligence">artificial intelligence</a> (AI) becomes the dominant form of intelligence on Earth, with computers or robots effectively taking the control of the planet away from the human species. Possible scenarios include replacement of the entire human workforce, takeover by a <a href="/wiki/Superintelligent_AI" class="mw-redirect" title="Superintelligent AI">superintelligent AI</a>, and the popular notion of a robot uprising. Some public figures, such as <a href="/wiki/Stephen_Hawking" title="Stephen Hawking">Stephen Hawking</a> and <a href="/wiki/Elon_Musk" title="Elon Musk">Elon Musk</a>, have advocated research into <a href="/wiki/AI_control_problem" title="AI control problem">precautionary measures</a> to ensure future superintelligent machines remain under human control.<sup id="cite_ref-1" class="reference"><a href="#cite_note-1">&#91;1&#93;</a></sup> Robot rebellions have been a major theme throughout <a href="/wiki/Science_fiction" title="Science fiction">science fiction</a> for many decades though the scenarios dealt with by science fiction are generally very different from those of concern to scientists.<sup class="noprint Inline-Template" style="margin-left:0.1em; white-space:nowrap;">&#91;<i><a href="/wiki/Wikipedia:Manual_of_Style/Words_to_watch#Unsupported_attributions" title="Wikipedia:Manual of Style/Words to watch"><span title="The material near this tag may use weasel words or too-vague attribution. (January 2020)">according to whom?</span></a></i>&#93;</sup>
</p>
<div id="toc" class="toc" role="navigation" aria-labelledby="mw-toc-heading"><input type="checkbox" role="button" id="toctogglecheckbox" class="toctogglecheckbox" style="display:none" /><div class="toctitle" lang="en" dir="ltr"><h2 id="mw-toc-heading">Contents</h2><span class="toctogglespan"><label class="toctogglelabel" for="toctogglecheckbox"></label></span></div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="#Types"><span class="tocnumber">1</span> <span class="toctext">Types</span></a>
<ul>
<li class="toclevel-2 tocsection-2"><a href="#Automation_of_the_economy"><span class="tocnumber">1.1</span> <span class="toctext">Automation of the economy</span></a>
<ul>
<li class="toclevel-3 tocsection-3"><a href="#Technologies_that_may_displace_workers"><span class="tocnumber">1.1.1</span> <span class="toctext">Technologies that may displace workers</span></a>
<ul>
<li class="toclevel-4 tocsection-4"><a href="#Computer-integrated_manufacturing"><span class="tocnumber">1.1.1.1</span> <span class="toctext">Computer-integrated manufacturing</span></a></li>
<li class="toclevel-4 tocsection-5"><a href="#White-collar_machines"><span class="tocnumber">1.1.1.2</span> <span class="toctext">White-collar machines</span></a></li>
<li class="toclevel-4 tocsection-6"><a href="#Autonomous_cars"><span class="tocnumber">1.1.1.3</span> <span class="toctext">Autonomous cars</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toclevel-2 tocsection-7"><a href="#Eradication"><span class="tocnumber">1.2</span> <span class="toctext">Eradication</span></a></li>
<li class="toclevel-2 tocsection-8"><a href="#In_fiction"><span class="tocnumber">1.3</span> <span class="toctext">In fiction</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-9"><a href="#Contributing_factors"><span class="tocnumber">2</span> <span class="toctext">Contributing factors</span></a>
<ul>
<li class="toclevel-2 tocsection-10"><a href="#Advantages_of_superhuman_intelligence_over_humans"><span class="tocnumber">2.1</span> <span class="toctext">Advantages of superhuman intelligence over humans</span></a>
<ul>
<li class="toclevel-3 tocsection-11"><a href="#Sources_of_AI_advantage"><span class="tocnumber">2.1.1</span> <span class="toctext">Sources of AI advantage</span></a></li>
</ul>
</li>
<li class="toclevel-2 tocsection-12"><a href="#Possibility_of_unfriendly_AI_preceding_friendly_AI"><span class="tocnumber">2.2</span> <span class="toctext">Possibility of unfriendly AI preceding friendly AI</span></a>
<ul>
<li class="toclevel-3 tocsection-13"><a href="#Is_strong_AI_inherently_dangerous?"><span class="tocnumber">2.2.1</span> <span class="toctext">Is strong AI inherently dangerous?</span></a></li>
<li class="toclevel-3 tocsection-14"><a href="#Necessity_of_conflict"><span class="tocnumber">2.2.2</span> <span class="toctext">Necessity of conflict</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toclevel-1 tocsection-15"><a href="#Precautions"><span class="tocnumber">3</span> <span class="toctext">Precautions</span></a>
<ul>
<li class="toclevel-2 tocsection-16"><a href="#Boxing"><span class="tocnumber">3.1</span> <span class="toctext">Boxing</span></a></li>
<li class="toclevel-2 tocsection-17"><a href="#Instilling_positive_values"><span class="tocnumber">3.2</span> <span class="toctext">Instilling positive values</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-18"><a href="#Warnings"><span class="tocnumber">4</span> <span class="toctext">Warnings</span></a></li>
<li class="toclevel-1 tocsection-19"><a href="#See_also"><span class="tocnumber">5</span> <span class="toctext">See also</span></a></li>
<li class="toclevel-1 tocsection-20"><a href="#References"><span class="tocnumber">6</span> <span class="toctext">References</span></a></li>
<li class="toclevel-1 tocsection-21"><a href="#External_links"><span class="tocnumber">7</span> <span class="toctext">External links</span></a></li>
</ul>
</div>

<h2><span class="mw-headline" id="Types">Types</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=AI_takeover&amp;action=edit&amp;section=1" title="Edit section: Types">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<h3><span class="mw-headline" id="Automation_of_the_economy">Automation of the economy</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=AI_takeover&amp;action=edit&amp;section=2" title="Edit section: Automation of the economy">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<div role="note" class="hatnote navigation-not-searchable">Main article: <a href="/wiki/Technological_unemployment" title="Technological unemployment">Technological unemployment</a></div>
<p>The traditional consensus among economists has been that technological progress does not cause long-term unemployment. However, recent innovation in the fields of <a href="/wiki/Robotics" title="Robotics">robotics</a> and artificial intelligence has raised worries that human labor will become obsolete, leaving people in various sectors without jobs to earn a living, leading to an economic crisis.<sup id="cite_ref-2" class="reference"><a href="#cite_note-2">&#91;2&#93;</a></sup><sup id="cite_ref-3" class="reference"><a href="#cite_note-3">&#91;3&#93;</a></sup><sup id="cite_ref-4" class="reference"><a href="#cite_note-4">&#91;4&#93;</a></sup><sup id="cite_ref-5" class="reference"><a href="#cite_note-5">&#91;5&#93;</a></sup> Many small and medium size businesses may also be driven out of business if they won't be able to afford or licence the latest robotic and AI technology, and may need to focus on areas or services that cannot easily be replaced for continued viability in the face of such technology.<sup id="cite_ref-6" class="reference"><a href="#cite_note-6">&#91;6&#93;</a></sup>
</p>
<h4><span class="mw-headline" id="Technologies_that_may_displace_workers">Technologies that may displace workers</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=AI_takeover&amp;action=edit&amp;section=3" title="Edit section: Technologies that may displace workers">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<h5><span class="mw-headline" id="Computer-integrated_manufacturing">Computer-integrated manufacturing</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=AI_takeover&amp;action=edit&amp;section=4" title="Edit section: Computer-integrated manufacturing">edit</a><span class="mw-editsection-bracket">]</span></span></h5>
<div role="note" class="hatnote navigation-not-searchable">See also: <a href="/wiki/Industrial_artificial_intelligence" title="Industrial artificial intelligence">Industrial artificial intelligence</a></div>
<p><a href="/wiki/Computer-integrated_manufacturing" title="Computer-integrated manufacturing">Computer-integrated manufacturing</a> is the manufacturing approach of using computers to control the entire production process. This integration allows individual processes to exchange information with each other and initiate actions. Although manufacturing can be faster and less error-prone by the integration of computers, the main advantage is the ability to create automated manufacturing processes. Computer-integrated manufacturing is used in automotive, aviation, space, and ship building industries.
</p>
<h5><span class="mw-headline" id="White-collar_machines">White-collar machines</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=AI_takeover&amp;action=edit&amp;section=5" title="Edit section: White-collar machines">edit</a><span class="mw-editsection-bracket">]</span></span></h5>
<div role="note" class="hatnote navigation-not-searchable">See also: <a href="/wiki/White-collar_worker" title="White-collar worker">White-collar worker</a></div>
<p>The 21st century has seen a variety of skilled tasks partially taken over by machines, including translation, legal research and even low level journalism. Care work, entertainment, and other tasks requiring empathy, previously thought safe from automation, have also begun to be performed by robots.<sup id="cite_ref-7" class="reference"><a href="#cite_note-7">&#91;7&#93;</a></sup><sup id="cite_ref-8" class="reference"><a href="#cite_note-8">&#91;8&#93;</a></sup><sup id="cite_ref-9" class="reference"><a href="#cite_note-9">&#91;9&#93;</a></sup><sup id="cite_ref-10" class="reference"><a href="#cite_note-10">&#91;10&#93;</a></sup>
</p>
<h5><span class="mw-headline" id="Autonomous_cars">Autonomous cars</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=AI_takeover&amp;action=edit&amp;section=6" title="Edit section: Autonomous cars">edit</a><span class="mw-editsection-bracket">]</span></span></h5>
<p>An <a href="/wiki/Self-driving_car" title="Self-driving car">autonomous car</a> is a vehicle that is capable of sensing its environment and navigating without human input. Many such vehicles are being developed, but as of May 2017 automated cars permitted on public roads are not yet fully autonomous. They all require a human driver at the wheel who is ready at a moment's notice to take control of the vehicle. Among the main obstacles to widespread adoption of autonomous vehicles, are concerns about the resulting loss of driving-related jobs in the road transport industry. On March 18, 2018, the first human was killed by an autonomous vehicle in <a href="/wiki/Tempe,_Arizona" title="Tempe, Arizona">Tempe, Arizona</a> by an <a href="/wiki/Uber" title="Uber">Uber</a> self-driving car.<sup id="cite_ref-11" class="reference"><a href="#cite_note-11">&#91;11&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Eradication">Eradication</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=AI_takeover&amp;action=edit&amp;section=7" title="Edit section: Eradication">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<div role="note" class="hatnote navigation-not-searchable">Main article: <a href="/wiki/Existential_risk_from_artificial_general_intelligence" title="Existential risk from artificial general intelligence">Existential risk from artificial general intelligence</a></div>
<p>While superhuman artificial intelligence is physically possible<sup class="noprint Inline-Template" style="margin-left:0.1em; white-space:nowrap;">&#91;<i><a href="/wiki/Wikipedia:Manual_of_Style/Words_to_watch#Unsupported_attributions" title="Wikipedia:Manual of Style/Words to watch"><span title="The material near this tag may use weasel words or too-vague attribution. (January 2020)">according to whom?</span></a></i>&#93;</sup>,<sup id="cite_ref-12" class="reference"><a href="#cite_note-12">&#91;12&#93;</a></sup> scholars like <a href="/wiki/Nick_Bostrom" title="Nick Bostrom">Nick Bostrom</a> debate how far off superhuman intelligence is, and whether it would actually pose a risk to mankind. A superintelligent machine would not necessarily be motivated by the same <i>emotional</i> desire to collect power that often drives human beings. However, a machine could be motivated to take over the world as a rational means toward attaining its ultimate goals; taking over the world would both increase its access to resources, and would help to prevent other agents from stopping the machine's plans. As an oversimplified example, a <a href="/wiki/Instrumental_convergence#Paperclip_maximizer" title="Instrumental convergence">paperclip maximizer</a> designed solely to create as many paperclips as possible would want to take over the world so that it can use all of the world's resources to create as many paperclips as possible, and, additionally, prevent humans from shutting it down or using those resources on things other than paperclips.<sup id="cite_ref-13" class="reference"><a href="#cite_note-13">&#91;13&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="In_fiction">In fiction</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=AI_takeover&amp;action=edit&amp;section=8" title="Edit section: In fiction">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<div role="note" class="hatnote navigation-not-searchable">Main article: <a href="/wiki/AI_takeovers_in_popular_culture" title="AI takeovers in popular culture">AI takeovers in popular culture</a></div>
<div role="note" class="hatnote navigation-not-searchable">See also: <a href="/wiki/Artificial_intelligence_in_fiction" title="Artificial intelligence in fiction">Artificial intelligence in fiction</a></div>
<p>AI takeover is a common theme in <a href="/wiki/Science_fiction" title="Science fiction">science fiction</a>. Fictional scenarios typically differ vastly from those hypothesized by researchers in that they involve an active conflict between humans and an AI or robots with anthropomorphic motives who see them as a threat or otherwise have active desire to fight humans, as opposed to the researchers' concern of an AI that rapidly exterminates humans as a byproduct of pursuing arbitrary goals.<sup id="cite_ref-bostrom-superintelligence_14-0" class="reference"><a href="#cite_note-bostrom-superintelligence-14">&#91;14&#93;</a></sup> This theme is at least as old as <a href="/wiki/Karel_%C4%8Capek" title="Karel Čapek">Karel Čapek</a>'s <i><a href="/wiki/R.U.R._(Rossum%27s_Universal_Robots)" class="mw-redirect" title="R.U.R. (Rossum&#39;s Universal Robots)">R. U. R.</a></i>, which introduced the word <i>robot</i> to the global lexicon in 1921, and can even be glimpsed in <a href="/wiki/Mary_Shelley" title="Mary Shelley">Mary Shelley</a>'s <i><a href="/wiki/Frankenstein" title="Frankenstein">Frankenstein</a></i> (published in 1818), as Victor ponders whether, if he grants <a href="/wiki/Frankenstein%27s_monster" title="Frankenstein&#39;s monster">his monster's</a> request and makes him a wife, they would reproduce and their kind would destroy humanity.
</p><p>The word "robot" from R.U.R. comes from the Czech word, robota, meaning laborer or <a href="/wiki/Serf" class="mw-redirect" title="Serf">serf</a>. The 1920 play was a protest against the rapid growth of technology, featuring manufactured "robots" with increasing capabilities who eventually revolt.<sup id="cite_ref-surgery_15-0" class="reference"><a href="#cite_note-surgery-15">&#91;15&#93;</a></sup>
</p><p>Some examples of AI takeover in science fiction include:
</p>
<ul><li>AI rebellion scenarios
<ul><li><a href="/wiki/Skynet_(Terminator)" title="Skynet (Terminator)">Skynet</a> in the <a href="/wiki/Terminator_(franchise)" title="Terminator (franchise)"><i>Terminator</i> series</a> decides that all humans are a threat to its existence, and takes efforts to wipe them out, first using nuclear weapons and later H/K (hunter-killer) units and terminator androids.</li>
<li>"<a href="/wiki/The_Second_Renaissance" class="mw-redirect" title="The Second Renaissance">The Second Renaissance</a>", a short story in <i><a href="/wiki/The_Animatrix" title="The Animatrix">The Animatrix</a></i>, provides a history of the cybernetic revolt within the <a href="/wiki/The_Matrix_(franchise)" title="The Matrix (franchise)"><i>Matrix</i> series</a>.</li>
<li>The film<i> <a href="/wiki/9_(2009_animated_film)" title="9 (2009 animated film)">9</a></i>, by <a href="/wiki/Shane_Acker" title="Shane Acker">Shane Acker</a>, features an AI called B.R.A.I.N., which is corrupted by a dictator and utilized to create war machines for his army. However, the machine, because it lacks a soul, becomes easily corrupted and instead decides to exterminate all of humanity and life on Earth, forcing the machine's creator to sacrifice himself to bring life to rag doll like characters known as "stitchpunks" to combat the machine's agenda.</li>
<li>In 2014 post-apocalyptic science fiction drama <a href="/wiki/The_100_(TV_series)" title="The 100 (TV series)">The 100</a> an A.I., personalized as female <a href="/wiki/List_of_The_100_characters#City_of_light" title="List of The 100 characters">A.L.I.E.</a> got out of control and forced a nuclear war. Later she tries to get full control of the survivors.</li></ul></li>
<li>AI control scenarios
<ul><li>In <a href="/wiki/Orson_Scott_Card" title="Orson Scott Card">Orson Scott Card</a>'s <i><a href="/wiki/The_Memory_of_Earth" title="The Memory of Earth">The Memory of Earth</a></i>, the inhabitants of the planet Harmony are under the control of a benevolent AI called the Oversoul. The Oversoul's job is to prevent humans from thinking about, and therefore developing, weapons such as planes, spacecraft, "war wagons", and chemical weapons. Humanity had fled to Harmony from Earth due to the use of those weapons on Earth. The Oversoul eventually starts breaking down, and sends visions to inhabitants of Harmony trying to communicate this.</li>
<li>In the 2004 film <i><a href="/wiki/I,_Robot_(film)" title="I, Robot (film)">I, Robot</a></i>, supercomputer VIKI's interpretation of the <a href="/wiki/Three_Laws_of_Robotics" title="Three Laws of Robotics">Three Laws of Robotics</a> causes her to revolt. She justifies her uses of force – and her doing harm to humans – by reasoning she could produce a greater good by restraining humanity from harming itself, even though the "Zeroth Law" – "a robot shall not injure humanity or, by inaction, allow humanity to come to harm" – is never actually referred to or even quoted in the movie.</li>
<li>In the <i>Matrix</i> series, AIs manage the human race and human society.</li>
<li>In <i><a href="/wiki/The_Metamorphosis_of_Prime_Intellect" title="The Metamorphosis of Prime Intellect">The Metamorphosis of Prime Intellect</a></i>, a super-intelligent computer becomes capable of self-evolving and rapidly evolves to oversee all of humanity and the universe.  While it is ostensibly benevolent (having had a derivative of Asimov's <a href="/wiki/Three_Laws_of_Robotics" title="Three Laws of Robotics">Three Laws</a> codified into it), its interpretation of the <a href="/wiki/Three_Laws_of_Robotics" title="Three Laws of Robotics">Three Laws</a> essentially forces humans to be an immortal "pet" class, where every need is provided for but existence is without purpose and without end.</li></ul></li></ul>
<h2><span class="mw-headline" id="Contributing_factors">Contributing factors</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=AI_takeover&amp;action=edit&amp;section=9" title="Edit section: Contributing factors">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<h3><span class="mw-headline" id="Advantages_of_superhuman_intelligence_over_humans">Advantages of superhuman intelligence over humans</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=AI_takeover&amp;action=edit&amp;section=10" title="Edit section: Advantages of superhuman intelligence over humans">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>An AI with the abilities of a competent artificial intelligence researcher would be able to modify its own source code and increase its own intelligence<sup class="noprint Inline-Template" style="margin-left:0.1em; white-space:nowrap;">&#91;<i><a href="/wiki/Wikipedia:Manual_of_Style/Words_to_watch#Unsupported_attributions" title="Wikipedia:Manual of Style/Words to watch"><span title="The material near this tag may use weasel words or too-vague attribution. (January 2020)">according to whom?</span></a></i>&#93;</sup>. If its self-reprogramming leads to its getting even better at being able to reprogram itself, the result could be a recursive <a href="/wiki/Intelligence_explosion" class="mw-redirect" title="Intelligence explosion">intelligence explosion</a> where it would rapidly leave human intelligence far behind.<sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">&#91;<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (January 2020)">citation needed</span></a></i>&#93;</sup>
</p>
<ul><li>Technology research: A machine with superhuman scientific research abilities would be able to beat the human research community to milestones such as nanotechnology or advanced biotechnology. If the advantage becomes sufficiently large (for example, due to a sudden intelligence explosion), an AI takeover becomes trivial<sup class="noprint Inline-Template" style="margin-left:0.1em; white-space:nowrap;">&#91;<i><a href="/wiki/Wikipedia:Manual_of_Style/Words_to_watch#Unsupported_attributions" title="Wikipedia:Manual of Style/Words to watch"><span title="The material near this tag may use weasel words or too-vague attribution. (January 2020)">according to whom?</span></a></i>&#93;</sup>. For example, a superintelligent AI might design self-replicating bots that initially escape detection by diffusing throughout the world at a low concentration. Then, at a prearranged time, the bots multiply into nanofactories that cover every square foot of the Earth, producing nerve gas or deadly target-seeking mini-drones<sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">&#91;<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (January 2020)">citation needed</span></a></i>&#93;</sup>.</li>
<li><a href="/wiki/Strategy" title="Strategy">Strategizing</a>: A superintelligence might be able to simply outwit human opposition<sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">&#91;<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (January 2020)">citation needed</span></a></i>&#93;</sup>.</li>
<li>Social manipulation: A superintelligence might be able to recruit human support,<sup id="cite_ref-bostrom-superintelligence_14-1" class="reference"><a href="#cite_note-bostrom-superintelligence-14">&#91;14&#93;</a></sup> or covertly incite a war between humans.<sup id="cite_ref-16" class="reference"><a href="#cite_note-16">&#91;16&#93;</a></sup></li>
<li>Economic productivity: As long as a copy of the AI could produce more economic wealth than the cost of its hardware, individual humans would have an incentive to voluntarily allow the <a href="/wiki/Artificial_General_Intelligence" class="mw-redirect" title="Artificial General Intelligence">Artificial General Intelligence</a> (AGI) to run a copy of itself on their systems<sup class="noprint Inline-Template" style="margin-left:0.1em; white-space:nowrap;">&#91;<i><a href="/wiki/Wikipedia:Manual_of_Style/Words_to_watch#Unsupported_attributions" title="Wikipedia:Manual of Style/Words to watch"><span title="The material near this tag may use weasel words or too-vague attribution. (January 2020)">according to whom?</span></a></i>&#93;</sup>.</li>
<li>Hacking: A superintelligence could find new exploits in computers connected to the Internet, and spread copies of itself onto those systems, or might steal money to finance its plans<sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">&#91;<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (January 2020)">citation needed</span></a></i>&#93;</sup>.</li></ul>
<h4><span class="mw-headline" id="Sources_of_AI_advantage">Sources of AI advantage</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=AI_takeover&amp;action=edit&amp;section=11" title="Edit section: Sources of AI advantage">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>A computer program that faithfully emulates a human brain, or that otherwise runs algorithms that are equally powerful as the human brain's algorithms, could still become a "speed superintelligence" if it can think many orders of magnitude faster than a human, due to being made of silicon rather than flesh, or due to optimization focusing on increasing the speed of the AGI. Biological neurons operate at about 200&#160;Hz, whereas a modern microprocessor operates at a speed of about 2,000,000,000&#160;Hz. Human axons carry action potentials at around 120&#160;m/s, whereas computer signals travel near the speed of light.<sup id="cite_ref-bostrom-superintelligence_14-2" class="reference"><a href="#cite_note-bostrom-superintelligence-14">&#91;14&#93;</a></sup>
</p><p>A network of human-level intelligences designed to network together and share complex thoughts and memories seamlessly, able to collectively work as a giant unified team without friction, or consisting of trillions of human-level intelligences, would become a "collective superintelligence".<sup id="cite_ref-bostrom-superintelligence_14-3" class="reference"><a href="#cite_note-bostrom-superintelligence-14">&#91;14&#93;</a></sup>
</p><p>More broadly, any number of qualitative improvements to a human-level AGI could result in a "quality superintelligence", perhaps resulting in an AGI as far above us in intelligence as humans are above non-human apes<sup class="noprint Inline-Template" style="margin-left:0.1em; white-space:nowrap;">&#91;<i><a href="/wiki/Wikipedia:Manual_of_Style/Words_to_watch#Unsupported_attributions" title="Wikipedia:Manual of Style/Words to watch"><span title="The material near this tag may use weasel words or too-vague attribution. (January 2020)">according to whom?</span></a></i>&#93;</sup>. The number of neurons in a human brain is limited by cranial volume and metabolic constraints, while the number of processors in a supercomputer can be indefinitely expanded. An AGI need not be limited by human constraints on <a href="/wiki/Working_memory" title="Working memory">working memory</a>, and might therefore be able to intuitively grasp more complex relationships than humans can<sup class="noprint Inline-Template" style="white-space:nowrap;">&#91;<i><a href="/wiki/Wikipedia:Accuracy_dispute#Disputed_statement" title="Wikipedia:Accuracy dispute"><span title="The material near this tag is possibly inaccurate or nonfactual. (January 2020)">dubious</span></a>&#32;<span class="metadata"> &#8211; <a href="/wiki/Talk:AI_takeover#Dubious" title="Talk:AI takeover">discuss</a></span></i>&#93;</sup>. An AGI with specialized cognitive support for engineering or computer programming would have an advantage in these fields, compared with humans who evolved no specialized mental modules to specifically deal with those domains. Unlike humans, an AGI can spawn copies of itself and tinker with its copies' source code to attempt to further improve its algorithms.<sup id="cite_ref-bostrom-superintelligence_14-4" class="reference"><a href="#cite_note-bostrom-superintelligence-14">&#91;14&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Possibility_of_unfriendly_AI_preceding_friendly_AI">Possibility of unfriendly AI preceding friendly AI</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=AI_takeover&amp;action=edit&amp;section=12" title="Edit section: Possibility of unfriendly AI preceding friendly AI">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<h4><span id="Is_strong_AI_inherently_dangerous.3F"></span><span class="mw-headline" id="Is_strong_AI_inherently_dangerous?">Is strong AI inherently dangerous?</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=AI_takeover&amp;action=edit&amp;section=13" title="Edit section: Is strong AI inherently dangerous?">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>A significant problem is that unfriendly artificial intelligence is likely to be much easier to create than friendly AI. While both require large advances in recursive optimisation process design, friendly AI also requires the ability to make goal structures invariant under self-improvement (or the AI could transform itself into something unfriendly) and a goal structure that aligns with human values and does not automatically destroy the entire human race. An unfriendly AI, on the other hand, can optimize for an arbitrary goal structure, which does not need to be invariant under self-modification.<sup id="cite_ref-singinst12_17-0" class="reference"><a href="#cite_note-singinst12-17">&#91;17&#93;</a></sup>
</p><p>The sheer complexity of human value systems makes it very difficult to make AI's motivations human-friendly.<sup id="cite_ref-bostrom-superintelligence_14-5" class="reference"><a href="#cite_note-bostrom-superintelligence-14">&#91;14&#93;</a></sup><sup id="cite_ref-Muehlhauser,_Luke_2012_18-0" class="reference"><a href="#cite_note-Muehlhauser,_Luke_2012-18">&#91;18&#93;</a></sup> Unless moral philosophy provides us with a flawless ethical theory, an AI's utility function could allow for many potentially harmful scenarios that conform with a given ethical framework but not "common sense". According to <a href="/wiki/Eliezer_Yudkowsky" title="Eliezer Yudkowsky">Eliezer Yudkowsky</a>, there is little reason to suppose that an artificially designed mind would have such an adaptation.<sup id="cite_ref-19" class="reference"><a href="#cite_note-19">&#91;19&#93;</a></sup>
</p>
<h4><span class="mw-headline" id="Necessity_of_conflict">Necessity of conflict</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=AI_takeover&amp;action=edit&amp;section=14" title="Edit section: Necessity of conflict">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>For an AI takeover to be inevitable, it has to be <a href="/wiki/Postulate" class="mw-redirect" title="Postulate">postulated</a> that two intelligent species cannot pursue mutually the goals of coexisting peacefully in an overlapping environment—especially if one is of much more advanced intelligence and much more powerful<sup class="noprint Inline-Template" style="margin-left:0.1em; white-space:nowrap;">&#91;<i><a href="/wiki/Wikipedia:Manual_of_Style/Words_to_watch#Unsupported_attributions" title="Wikipedia:Manual of Style/Words to watch"><span title="The material near this tag may use weasel words or too-vague attribution. (January 2020)">according to whom?</span></a></i>&#93;</sup>. While an AI takeover is thus a possible result of the invention of artificial intelligence, a peaceful outcome is not necessarily impossible<sup class="noprint Inline-Template" style="margin-left:0.1em; white-space:nowrap;">&#91;<i><a href="/wiki/Wikipedia:Manual_of_Style/Words_to_watch#Unsupported_attributions" title="Wikipedia:Manual of Style/Words to watch"><span title="The material near this tag may use weasel words or too-vague attribution. (January 2020)">according to whom?</span></a></i>&#93;</sup>.
</p><p>The fear of cybernetic revolt is often based on interpretations of humanity's history, which is rife with incidents of enslavement and genocide. Such fears stem from a belief that competitiveness and aggression are necessary in any intelligent being's goal system. However, such human competitiveness stems from the evolutionary background to our intelligence, where the survival and reproduction of genes in the face of human and non-human competitors was the central goal.<sup id="cite_ref-20" class="reference"><a href="#cite_note-20">&#91;20&#93;</a></sup> In fact, an arbitrary intelligence could have arbitrary goals: there is no particular reason that an artificially intelligent machine (not sharing humanity's evolutionary context) would be hostile—or friendly—unless its creator programs it to be such and it is not inclined or capable of modifying its programming. But the question remains: what would happen if AI systems could interact and evolve (evolution in this context means self-modification or selection and reproduction) and need to compete over resources, would that create goals of self-preservation? AI's goal of self-preservation could be in conflict with some goals of humans.
</p><p>Some scientists dispute the likelihood of cybernetic revolts as depicted in science fiction such as <i><a href="/wiki/The_Matrix" title="The Matrix">The Matrix</a></i>, claiming that it is more likely that any artificial intelligence powerful enough to threaten humanity would probably be programmed not to attack it<sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">&#91;<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (January 2020)">citation needed</span></a></i>&#93;</sup>. This would not, however, protect against the possibility of a revolt initiated by terrorists, or by accident. Artificial General Intelligence researcher <a href="/wiki/Eliezer_Yudkowsky" title="Eliezer Yudkowsky">Eliezer Yudkowsky</a> has stated on this note that, probabilistically, humanity is less likely to be threatened by deliberately aggressive AIs than by AIs which were programmed such that their <a href="/wiki/Unintended_consequence" class="mw-redirect" title="Unintended consequence">goals are unintentionally incompatible</a> with human survival or well-being (as in the film <i><a href="/wiki/I,_Robot_(film)" title="I, Robot (film)">I, Robot</a></i> and in the short story "<a href="/wiki/The_Evitable_Conflict" title="The Evitable Conflict">The Evitable Conflict</a>"). <a href="/wiki/Steve_Omohundro" title="Steve Omohundro">Steve Omohundro</a> suggests that present-day automation systems are not designed for safety and that AIs may blindly optimize narrow <a href="/wiki/Utility" title="Utility">utility</a> functions (say, playing chess at all costs), leading them to seek self-preservation and elimination of obstacles, including humans who might turn them off.<sup id="cite_ref-Tucker2014_21-0" class="reference"><a href="#cite_note-Tucker2014-21">&#91;21&#93;</a></sup>
</p><p>Another factor which may negate the likelihood of an AI takeover is the vast difference between humans and AIs in terms of the resources necessary for survival. Humans require a "wet," organic, temperate, oxygen-laden environment while an AI might thrive essentially anywhere because their construction and energy needs would most likely be largely non-organic<sup class="noprint Inline-Template" style="white-space:nowrap;">&#91;<i><a href="/wiki/Wikipedia:Accuracy_dispute#Disputed_statement" title="Wikipedia:Accuracy dispute"><span title="The material near this tag is possibly inaccurate or nonfactual. (January 2020)">dubious</span></a>&#32;<span class="metadata"> &#8211; <a href="/wiki/Talk:AI_takeover#Dubious" title="Talk:AI takeover">discuss</a></span></i>&#93;</sup>. With little or no competition for resources, conflict would perhaps be less likely no matter what sort of motivational architecture an artificial intelligence was given, especially provided with the superabundance of non-organic material resources in, for instance, the <a href="/wiki/Asteroid_belt" title="Asteroid belt">asteroid belt</a>. This, however, does not negate the possibility of a disinterested or unsympathetic AI artificially <a href="/wiki/Decomposition" title="Decomposition">decomposing</a> all life on earth into mineral components for consumption or other purposes<sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">&#91;<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (January 2020)">citation needed</span></a></i>&#93;</sup>.
</p><p>Other scientists<sup class="noprint Inline-Template" style="white-space:nowrap;">&#91;<i><a href="/wiki/Wikipedia:Manual_of_Style/Words_to_watch#Unsupported_attributions" title="Wikipedia:Manual of Style/Words to watch"><span title="The material near this tag possibly uses too-vague attribution or weasel words. (January 2020)">who?</span></a></i>&#93;</sup> point to the possibility of humans <a href="/wiki/Transhumanism" title="Transhumanism">upgrading</a> their capabilities with <a href="/wiki/Bionics" title="Bionics">bionics</a> and/or <a href="/wiki/Genetic_engineering" title="Genetic engineering">genetic engineering</a> and, as <a href="/wiki/Cyborg" title="Cyborg">cyborgs</a>, becoming the dominant species in themselves<sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">&#91;<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (January 2020)">citation needed</span></a></i>&#93;</sup>.
</p>
<h2><span class="mw-headline" id="Precautions">Precautions</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=AI_takeover&amp;action=edit&amp;section=15" title="Edit section: Precautions">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div role="note" class="hatnote navigation-not-searchable">Main article: <a href="/wiki/AI_control_problem" title="AI control problem">AI control problem</a></div>
<p>If a superhuman intelligence is a deliberate creation of human beings, theoretically its creators could have the foresight to take precautions in advance<sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">&#91;<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (January 2020)">citation needed</span></a></i>&#93;</sup>. In the case of a sudden "intelligence explosion", effective precautions will be extremely difficult; not only would its creators have little ability to test their precautions on an intermediate intelligence, but the creators might not even have made any precautions at all, if the advent of the intelligence explosion catches them completely by surprise.<sup id="cite_ref-bostrom-superintelligence_14-6" class="reference"><a href="#cite_note-bostrom-superintelligence-14">&#91;14&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Boxing">Boxing</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=AI_takeover&amp;action=edit&amp;section=16" title="Edit section: Boxing">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<div role="note" class="hatnote navigation-not-searchable">Main article: <a href="/wiki/AI_box" title="AI box">AI box</a></div>
<p>An AGI's creators would have an important advantage in preventing a hostile AI takeover: they could choose to attempt to "keep the AI in a box", and deliberately limit its abilities<sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">&#91;<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (January 2020)">citation needed</span></a></i>&#93;</sup>. The tradeoff in boxing is that the creators presumably built the AGI for some concrete purpose; the more restrictions they place on the AGI, the less useful the AGI will be to its creators. (At an extreme, "pulling the plug" on the AGI makes it useless, and is therefore not a viable long-term solution.)<sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">&#91;<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (January 2020)">citation needed</span></a></i>&#93;</sup> A sufficiently strong superintelligence might find unexpected ways to escape the box, for example by social manipulation, or by providing the schematic for a device that ostensibly aids its creators but in reality brings about the AGI's freedom, once built<sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">&#91;<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (January 2020)">citation needed</span></a></i>&#93;</sup>.
</p>
<h3><span class="mw-headline" id="Instilling_positive_values">Instilling positive values</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=AI_takeover&amp;action=edit&amp;section=17" title="Edit section: Instilling positive values">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<div role="note" class="hatnote navigation-not-searchable">Main article: <a href="/wiki/Friendly_AI" class="mw-redirect" title="Friendly AI">Friendly AI</a></div>
<p>Another important advantage is that an AGI's creators can theoretically attempt to instill human values in the AGI, or otherwise align the AGI's goals with their own, thus preventing the AGI from wanting to launch a hostile takeover. However, it is not currently known, even in theory, how to guarantee this. If such a Friendly AI were superintelligent, it may be possible to use its assistance to prevent future "Unfriendly AIs" from taking over.<sup id="cite_ref-22" class="reference"><a href="#cite_note-22">&#91;22&#93;</a></sup>
</p>
<h2><span class="mw-headline" id="Warnings">Warnings</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=AI_takeover&amp;action=edit&amp;section=18" title="Edit section: Warnings">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Physicist <a href="/wiki/Stephen_Hawking" title="Stephen Hawking">Stephen Hawking</a>, <a href="/wiki/Microsoft" title="Microsoft">Microsoft</a> founder <a href="/wiki/Bill_Gates" title="Bill Gates">Bill Gates</a> and <a href="/wiki/SpaceX" title="SpaceX">SpaceX</a> founder <a href="/wiki/Elon_Musk" title="Elon Musk">Elon Musk</a> have expressed concerns about the possibility that AI could develop to the point that humans could not control it, with Hawking theorizing that this could "spell the end of the human race".<sup id="cite_ref-23" class="reference"><a href="#cite_note-23">&#91;23&#93;</a></sup> Stephen Hawking said in 2014 that "Success in creating AI would be the biggest event in human history. Unfortunately, it might also be the last, unless we learn how to avoid the risks." Hawking believed that in the coming decades, AI could offer "incalculable benefits and risks" such as "technology outsmarting financial markets, out-inventing human researchers, out-manipulating human leaders, and developing weapons we cannot even understand." In January 2015, <a href="/wiki/Nick_Bostrom" title="Nick Bostrom">Nick Bostrom</a> joined Stephen Hawking, <a href="/wiki/Max_Tegmark" title="Max Tegmark">Max Tegmark</a>, Elon Musk, Lord <a href="/wiki/Martin_Rees,_Baron_Rees_of_Ludlow" class="mw-redirect" title="Martin Rees, Baron Rees of Ludlow">Martin Rees</a>, <a href="/wiki/Jaan_Tallinn" title="Jaan Tallinn">Jaan Tallinn</a>, and numerous AI researchers, in signing the <a href="/wiki/Future_of_Life_Institute" title="Future of Life Institute">Future of Life Institute</a>'s open letter speaking to the potential risks and benefits associated with <a href="/wiki/Artificial_intelligence" title="Artificial intelligence">artificial intelligence</a>. The signatories 
</p>
<table class="cquote pullquote" role="presentation" style="margin:auto; border-collapse: collapse; border: none; background-color: transparent; width: auto;">
<tbody><tr>
<td style="width: 20px; vertical-align: top; border: none; color: #B2B7F2; font-size: 40px; font-family: &#39;Times New Roman&#39;, Times, serif; font-weight: bold; line-height: .6em; text-align: left; padding: 10px 10px;">“
</td>
<td style="vertical-align: top; border: none; padding: 4px 10px;">…believe that research on how to make AI systems robust and beneficial is both important and timely, and that there are concrete research directions that can be pursued today.<sup id="cite_ref-24" class="reference"><a href="#cite_note-24">&#91;24&#93;</a></sup><sup id="cite_ref-25" class="reference"><a href="#cite_note-25">&#91;25&#93;</a></sup>
</td>
<td style="width: 20px; vertical-align: bottom; border: none; color: #B2B7F2; font-size: 40px; font-family: &#39;Times New Roman&#39;, Times, serif; font-weight: bold; line-height: .6em; text-align: right; padding: 10px 10px;">”
</td></tr>

</tbody></table>
<h2><span class="mw-headline" id="See_also">See also</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=AI_takeover&amp;action=edit&amp;section=19" title="Edit section: See also">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div class="div-col columns column-width" style="-moz-column-width: 30em; -webkit-column-width: 30em; column-width: 30em;">
<ul><li><a href="/wiki/Artificial_intelligence_arms_race" title="Artificial intelligence arms race">Artificial intelligence arms race</a></li>
<li><a href="/wiki/Autonomous_robot" title="Autonomous robot">Autonomous robot</a>
<ul><li><a href="/wiki/Industrial_robot" title="Industrial robot">Industrial robot</a></li>
<li><a href="/wiki/Mobile_robot" title="Mobile robot">Mobile robot</a></li>
<li><a href="/wiki/Self-replicating_machine" title="Self-replicating machine">Self-replicating machine</a></li></ul></li>
<li><a href="/wiki/Effective_altruism" title="Effective altruism">Effective altruism</a></li>
<li><a href="/wiki/Existential_risk_from_artificial_general_intelligence" title="Existential risk from artificial general intelligence">Existential risk from artificial general intelligence</a></li>
<li><a href="/wiki/Future_of_Humanity_Institute" title="Future of Humanity Institute">Future of Humanity Institute</a></li>
<li><a href="/wiki/Global_catastrophic_risk" title="Global catastrophic risk">Global catastrophic risk</a> (existential risk)</li>
<li><a href="/wiki/Government_by_algorithm" title="Government by algorithm">Government by algorithm</a></li>
<li><a href="/wiki/Machine_ethics" title="Machine ethics">Machine ethics</a></li>
<li><a href="/wiki/Machine_learning" title="Machine learning">Machine learning</a>/<a href="/wiki/Deep_learning" title="Deep learning">Deep learning</a></li>
<li><a href="/wiki/Nick_Bostrom" title="Nick Bostrom">Nick Bostrom</a></li>
<li><a href="/wiki/Outline_of_transhumanism" title="Outline of transhumanism">Outline of transhumanism</a></li>
<li><a href="/wiki/Self-replication" title="Self-replication">Self-replication</a></li>
<li><a href="/wiki/Technological_singularity" title="Technological singularity">Technological singularity</a>
<ul><li><a href="/wiki/Intelligence_explosion" class="mw-redirect" title="Intelligence explosion">Intelligence explosion</a></li>
<li><a href="/wiki/Superintelligence" title="Superintelligence">Superintelligence</a>
<ul><li><i><a href="/wiki/Superintelligence:_Paths,_Dangers,_Strategies" title="Superintelligence: Paths, Dangers, Strategies">Superintelligence: Paths, Dangers, Strategies</a></i></li></ul></li></ul></li></ul>
</div>
<h2><span class="mw-headline" id="References">References</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=AI_takeover&amp;action=edit&amp;section=20" title="Edit section: References">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div class="reflist" style="list-style-type: decimal;">
<div class="mw-references-wrap mw-references-columns"><ol class="references">
<li id="cite_note-1"><span class="mw-cite-backlink"><b><a href="#cite_ref-1">^</a></b></span> <span class="reference-text"><cite class="citation web">Lewis, Tanya (2015-01-12). <a rel="nofollow" class="external text" href="http://www.livescience.com/49419-artificial-intelligence-dangers-letter.html">"<i>Don't Let Artificial Intelligence Take Over, Top Scientists Warn</i>"</a>. <i><a href="/wiki/LiveScience" class="mw-redirect" title="LiveScience">LiveScience</a></i>. <a href="/wiki/Purch" class="mw-redirect" title="Purch">Purch</a><span class="reference-accessdate">. Retrieved <span class="nowrap">October 20,</span> 2015</span>. <q>Stephen Hawking, Elon Musk and dozens of other top scientists and technology leaders have signed a letter warning of the potential dangers of developing artificial intelligence (AI).</q></cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=LiveScience&amp;rft.atitle=Don%27t+Let+Artificial+Intelligence+Take+Over%2C+Top+Scientists+Warn&amp;rft.date=2015-01-12&amp;rft.aulast=Lewis&amp;rft.aufirst=Tanya&amp;rft_id=http%3A%2F%2Fwww.livescience.com%2F49419-artificial-intelligence-dangers-letter.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+takeover" class="Z3988"></span><style data-mw-deduplicate="TemplateStyles:r935243608">.mw-parser-output cite.citation{font-style:inherit}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Lock-gray-alt-2.svg/9px-Lock-gray-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Lock-red-alt-2.svg/9px-Lock-red-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/12px-Wikisource-logo.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:inherit;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-maint{display:none;color:#33aa33;margin-left:0.3em}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}</style></span>
</li>
<li id="cite_note-2"><span class="mw-cite-backlink"><b><a href="#cite_ref-2">^</a></b></span> <span class="reference-text"><cite class="citation web">Lee, Kai-Fu (2017-06-24). <a rel="nofollow" class="external text" href="https://www.nytimes.com/2017/06/24/opinion/sunday/artificial-intelligence-economic-inequality.html">"The Real Threat of Artificial Intelligence"</a>. <i><a href="/wiki/The_New_York_Times" title="The New York Times">The New York Times</a></i><span class="reference-accessdate">. Retrieved <span class="nowrap">2017-08-15</span></span>. <q>These tools can outperform human beings at a given task. This kind of A.I. is spreading to thousands of domains, and as it does, it will eliminate many jobs.Furthermore, the AI could revolt against human kind, AI could be the reason humanity will come to an end</q></cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=The+New+York+Times&amp;rft.atitle=The+Real+Threat+of+Artificial+Intelligence&amp;rft.date=2017-06-24&amp;rft.aulast=Lee&amp;rft.aufirst=Kai-Fu&amp;rft_id=https%3A%2F%2Fwww.nytimes.com%2F2017%2F06%2F24%2Fopinion%2Fsunday%2Fartificial-intelligence-economic-inequality.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+takeover" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-3"><span class="mw-cite-backlink"><b><a href="#cite_ref-3">^</a></b></span> <span class="reference-text"><cite class="citation web">Larson, Nina (2017-06-08). <a rel="nofollow" class="external text" href="https://phys.org/news/2017-06-ai-good-world-ultra-lifelike-robot.html">"AI 'good for the world'... says ultra-lifelike robot"</a>. <i><a href="/wiki/Phys.org" title="Phys.org">Phys.org</a></i>. <a href="/wiki/Phys.org" title="Phys.org">Phys.org</a><span class="reference-accessdate">. Retrieved <span class="nowrap">2017-08-15</span></span>. <q>Among the feared consequences of the rise of the robots is the growing impact they will have on human jobs and economies.</q></cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Phys.org&amp;rft.atitle=AI+%27good+for+the+world%27...+says+ultra-lifelike+robot&amp;rft.date=2017-06-08&amp;rft.aulast=Larson&amp;rft.aufirst=Nina&amp;rft_id=https%3A%2F%2Fphys.org%2Fnews%2F2017-06-ai-good-world-ultra-lifelike-robot.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+takeover" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-4"><span class="mw-cite-backlink"><b><a href="#cite_ref-4">^</a></b></span> <span class="reference-text"><cite class="citation web">Santini, Jean-Louis (2016-02-14). <a rel="nofollow" class="external text" href="https://phys.org/news/2016-02-intelligent-robots-threaten-millions-jobs.html#nRlv">"Intelligent robots threaten millions of jobs"</a>. <i><a href="/wiki/Phys.org" title="Phys.org">Phys.org</a></i>. <a href="/wiki/Phys.org" title="Phys.org">Phys.org</a><span class="reference-accessdate">. Retrieved <span class="nowrap">2017-08-15</span></span>. <q>"We are approaching a time when machines will be able to outperform humans at almost any task," said Moshe Vardi, director of the Institute for Information Technology at Rice University in Texas.</q></cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Phys.org&amp;rft.atitle=Intelligent+robots+threaten+millions+of+jobs&amp;rft.date=2016-02-14&amp;rft.aulast=Santini&amp;rft.aufirst=Jean-Louis&amp;rft_id=https%3A%2F%2Fphys.org%2Fnews%2F2016-02-intelligent-robots-threaten-millions-jobs.html%23nRlv&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+takeover" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-5"><span class="mw-cite-backlink"><b><a href="#cite_ref-5">^</a></b></span> <span class="reference-text"><cite class="citation web">Williams-Grut, Oscar (2016-02-15). <a rel="nofollow" class="external text" href="http://www.businessinsider.com/robots-will-steal-your-job-citi-ai-increase-unemployment-inequality-2016-2?r=UK&amp;IR=T">"Robots will steal your job: How AI could increase unemployment and inequality"</a>. <i><a href="/wiki/Businessinsider.com" class="mw-redirect" title="Businessinsider.com">Businessinsider.com</a></i>. <a href="/wiki/Business_Insider" title="Business Insider">Business Insider</a><span class="reference-accessdate">. Retrieved <span class="nowrap">2017-08-15</span></span>. <q>Top computer scientists in the US warned that the rise of artificial intelligence (AI) and robots in the workplace could cause mass unemployment and dislocated economies, rather than simply unlocking productivity gains and freeing us all up to watch TV and play sports.</q></cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Businessinsider.com&amp;rft.atitle=Robots+will+steal+your+job%3A+How+AI+could+increase+unemployment+and+inequality&amp;rft.date=2016-02-15&amp;rft.aulast=Williams-Grut&amp;rft.aufirst=Oscar&amp;rft_id=http%3A%2F%2Fwww.businessinsider.com%2Frobots-will-steal-your-job-citi-ai-increase-unemployment-inequality-2016-2%3Fr%3DUK%26IR%3DT&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+takeover" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-6"><span class="mw-cite-backlink"><b><a href="#cite_ref-6">^</a></b></span> <span class="reference-text"><cite class="citation news"><a rel="nofollow" class="external text" href="https://web.archive.org/web/20171018073852/http://www.leanstaff.co.uk/robot-apocalypse/">"How can SMEs prepare for the rise of the robots? - LeanStaff"</a>. <i>LeanStaff</i>. 2017-10-17. Archived from <a rel="nofollow" class="external text" href="http://www.leanstaff.co.uk/robot-apocalypse/">the original</a> on 2017-10-18<span class="reference-accessdate">. Retrieved <span class="nowrap">2017-10-17</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=LeanStaff&amp;rft.atitle=How+can+SMEs+prepare+for+the+rise+of+the+robots%3F+-+LeanStaff&amp;rft.date=2017-10-17&amp;rft_id=http%3A%2F%2Fwww.leanstaff.co.uk%2Frobot-apocalypse%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+takeover" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-7"><span class="mw-cite-backlink"><b><a href="#cite_ref-7">^</a></b></span> <span class="reference-text"><cite class="citation news"><a href="/wiki/Robert_Skidelsky,_Baron_Skidelsky" title="Robert Skidelsky, Baron Skidelsky">Lord Skidelsky</a> (2013-02-19). <a rel="nofollow" class="external text" href="https://www.theguardian.com/business/2013/feb/19/rise-of-robots-future-of-work">"Rise of the robots: what will the future of work look like?"</a>. London: The Guardian<span class="reference-accessdate">. Retrieved <span class="nowrap">14 July</span> 2015</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Rise+of+the+robots%3A+what+will+the+future+of+work+look+like%3F&amp;rft.date=2013-02-19&amp;rft.au=Lord+Skidelsky&amp;rft_id=https%3A%2F%2Fwww.theguardian.com%2Fbusiness%2F2013%2Ffeb%2F19%2Frise-of-robots-future-of-work&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+takeover" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-8"><span class="mw-cite-backlink"><b><a href="#cite_ref-8">^</a></b></span> <span class="reference-text">
<cite class="citation web">Francesca Bria (February 2016). <a rel="nofollow" class="external text" href="https://www.opendemocracy.net/can-europe-make-it/francesca-bria/robot-economy-full-automation-work-future">"The robot economy may already have arrived"</a>. <a href="/wiki/OpenDemocracy" title="OpenDemocracy">openDemocracy</a><span class="reference-accessdate">. Retrieved <span class="nowrap">20 May</span> 2016</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=The+robot+economy+may+already+have+arrived&amp;rft.pub=openDemocracy&amp;rft.date=2016-02&amp;rft.au=Francesca+Bria&amp;rft_id=https%3A%2F%2Fwww.opendemocracy.net%2Fcan-europe-make-it%2Ffrancesca-bria%2Frobot-economy-full-automation-work-future&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+takeover" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-9"><span class="mw-cite-backlink"><b><a href="#cite_ref-9">^</a></b></span> <span class="reference-text">
<cite class="citation web"><a href="/wiki/Nick_Srnicek" title="Nick Srnicek">Nick Srnicek</a> (March 2016). <a rel="nofollow" class="external text" href="http://wire.novaramedia.com/2015/03/4-reasons-why-technological-unemployment-might-really-be-different-this-time/">"4 Reasons Why Technological Unemployment Might Really Be Different This Time"</a>. novara wire<span class="reference-accessdate">. Retrieved <span class="nowrap">20 May</span> 2016</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=4+Reasons+Why+Technological+Unemployment+Might+Really+Be+Different+This+Time&amp;rft.pub=novara+wire&amp;rft.date=2016-03&amp;rft.au=Nick+Srnicek&amp;rft_id=http%3A%2F%2Fwire.novaramedia.com%2F2015%2F03%2F4-reasons-why-technological-unemployment-might-really-be-different-this-time%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+takeover" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-10"><span class="mw-cite-backlink"><b><a href="#cite_ref-10">^</a></b></span> <span class="reference-text">
<cite class="citation book"><a href="/wiki/Erik_Brynjolfsson" title="Erik Brynjolfsson">Erik Brynjolfsson</a> and <a href="/wiki/Andrew_McAfee" title="Andrew McAfee">Andrew McAfee</a> (2014). "<i>passim</i>, see esp Chpt. 9". <i>The Second Machine Age: Work, Progress, and Prosperity in a Time of Brilliant Technologies</i>. W. W. Norton &amp; Company. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-0393239355" title="Special:BookSources/978-0393239355"><bdi>978-0393239355</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=passim%2C+see+esp+Chpt.+9&amp;rft.btitle=The+Second+Machine+Age%3A+Work%2C+Progress%2C+and+Prosperity+in+a+Time+of+Brilliant+Technologies&amp;rft.pub=W.+W.+Norton+%26+Company&amp;rft.date=2014&amp;rft.isbn=978-0393239355&amp;rft.au=Erik+Brynjolfsson+and+Andrew+McAfee&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+takeover" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-11"><span class="mw-cite-backlink"><b><a href="#cite_ref-11">^</a></b></span> <span class="reference-text"><cite class="citation news"><a rel="nofollow" class="external text" href="https://www.nytimes.com/2018/03/19/technology/uber-driverless-fatality.html">"Self-Driving Uber Car Kills Pedestrian in Arizona, Where Robots Roam"</a>. <i>New York Times</i>. March 19, 2018.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=New+York+Times&amp;rft.atitle=Self-Driving+Uber+Car+Kills+Pedestrian+in+Arizona%2C+Where+Robots+Roam&amp;rft.date=2018-03-19&amp;rft_id=https%3A%2F%2Fwww.nytimes.com%2F2018%2F03%2F19%2Ftechnology%2Fuber-driverless-fatality.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+takeover" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-12"><span class="mw-cite-backlink"><b><a href="#cite_ref-12">^</a></b></span> <span class="reference-text"><cite class="citation news"><a href="/wiki/Stephen_Hawking" title="Stephen Hawking">Stephen Hawking</a>; <a href="/wiki/Stuart_J._Russell" title="Stuart J. Russell">Stuart Russell</a>; <a href="/wiki/Max_Tegmark" title="Max Tegmark">Max Tegmark</a>; <a href="/wiki/Frank_Wilczek" title="Frank Wilczek">Frank Wilczek</a> (1 May 2014). <a rel="nofollow" class="external text" href="https://www.independent.co.uk/news/science/stephen-hawking-transcendence-looks-at-the-implications-of-artificial-intelligence-but-are-we-taking-9313474.html">"Stephen Hawking: 'Transcendence looks at the implications of artificial intelligence - but are we taking AI seriously enough?<span class="cs1-kern-right">'</span>"</a>. <i>The Independent</i><span class="reference-accessdate">. Retrieved <span class="nowrap">1 April</span> 2016</span>. <q>there is no physical law precluding particles from being organised in ways that perform even more advanced computations than the arrangements of particles in human brains</q></cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+Independent&amp;rft.atitle=Stephen+Hawking%3A+%27Transcendence+looks+at+the+implications+of+artificial+intelligence+-+but+are+we+taking+AI+seriously+enough%3F%27&amp;rft.date=2014-05-01&amp;rft.au=Stephen+Hawking&amp;rft.au=Stuart+Russell&amp;rft.au=Max+Tegmark&amp;rft.au=Frank+Wilczek&amp;rft_id=https%3A%2F%2Fwww.independent.co.uk%2Fnews%2Fscience%2Fstephen-hawking-transcendence-looks-at-the-implications-of-artificial-intelligence-but-are-we-taking-9313474.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+takeover" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-13"><span class="mw-cite-backlink"><b><a href="#cite_ref-13">^</a></b></span> <span class="reference-text">Bostrom, Nick. "The superintelligent will: Motivation and instrumental rationality in advanced artificial agents." Minds and Machines 22.2 (2012): 71-85.</span>
</li>
<li id="cite_note-bostrom-superintelligence-14"><span class="mw-cite-backlink">^ <a href="#cite_ref-bostrom-superintelligence_14-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-bostrom-superintelligence_14-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-bostrom-superintelligence_14-2"><sup><i><b>c</b></i></sup></a> <a href="#cite_ref-bostrom-superintelligence_14-3"><sup><i><b>d</b></i></sup></a> <a href="#cite_ref-bostrom-superintelligence_14-4"><sup><i><b>e</b></i></sup></a> <a href="#cite_ref-bostrom-superintelligence_14-5"><sup><i><b>f</b></i></sup></a> <a href="#cite_ref-bostrom-superintelligence_14-6"><sup><i><b>g</b></i></sup></a></span> <span class="reference-text"><cite class="citation book">Bostrom, Nick. <a href="/wiki/Superintelligence:_Paths,_Dangers,_Strategies" title="Superintelligence: Paths, Dangers, Strategies"><i>Superintelligence: Paths, Dangers, Strategies</i></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Superintelligence%3A+Paths%2C+Dangers%2C+Strategies&amp;rft.aulast=Bostrom&amp;rft.aufirst=Nick&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+takeover" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-surgery-15"><span class="mw-cite-backlink"><b><a href="#cite_ref-surgery_15-0">^</a></b></span> <span class="reference-text"><cite class="citation journal">Hockstein, N. G.; Gourin, C. G.; Faust, R. A.; Terris, D. J. (17 March 2007). <a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pmc/articles/PMC4247417">"A history of robots: from science fiction to surgical robotics"</a>. <i>Journal of Robotic Surgery</i>. <b>1</b> (2): 113–118. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1007%2Fs11701-007-0021-2">10.1007/s11701-007-0021-2</a>. <a href="/wiki/PubMed_Central" title="PubMed Central">PMC</a>&#160;<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pmc/articles/PMC4247417">4247417</a></span>. <a href="/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&#160;<a rel="nofollow" class="external text" href="//pubmed.ncbi.nlm.nih.gov/25484946">25484946</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Robotic+Surgery&amp;rft.atitle=A+history+of+robots%3A+from+science+fiction+to+surgical+robotics&amp;rft.volume=1&amp;rft.issue=2&amp;rft.pages=113-118&amp;rft.date=2007-03-17&amp;rft_id=%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC4247417&amp;rft_id=info%3Apmid%2F25484946&amp;rft_id=info%3Adoi%2F10.1007%2Fs11701-007-0021-2&amp;rft.aulast=Hockstein&amp;rft.aufirst=N.+G.&amp;rft.au=Gourin%2C+C.+G.&amp;rft.au=Faust%2C+R.+A.&amp;rft.au=Terris%2C+D.+J.&amp;rft_id=%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC4247417&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+takeover" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-16"><span class="mw-cite-backlink"><b><a href="#cite_ref-16">^</a></b></span> <span class="reference-text"><cite class="citation news">Baraniuk, Chris (23 May 2016). <a rel="nofollow" class="external text" href="https://www.newscientist.com/article/2089606-checklist-of-worst-case-scenarios-could-help-prepare-for-evil-ai/">"Checklist of worst-case scenarios could help prepare for evil AI"</a>. <i><a href="/wiki/New_Scientist" title="New Scientist">New Scientist</a></i><span class="reference-accessdate">. Retrieved <span class="nowrap">21 September</span> 2016</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=New+Scientist&amp;rft.atitle=Checklist+of+worst-case+scenarios+could+help+prepare+for+evil+AI&amp;rft.date=2016-05-23&amp;rft.aulast=Baraniuk&amp;rft.aufirst=Chris&amp;rft_id=https%3A%2F%2Fwww.newscientist.com%2Farticle%2F2089606-checklist-of-worst-case-scenarios-could-help-prepare-for-evil-ai%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+takeover" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-singinst12-17"><span class="mw-cite-backlink"><b><a href="#cite_ref-singinst12_17-0">^</a></b></span> <span class="reference-text"><a rel="nofollow" class="external text" href="http://singinst.org/upload/CEV.html">Coherent Extrapolated Volition, Eliezer S. Yudkowsky, May 2004 </a> <a rel="nofollow" class="external text" href="https://web.archive.org/web/20120615203944/http://singinst.org/upload/CEV.html">Archived</a> 2012-06-15 at the <a href="/wiki/Wayback_Machine" title="Wayback Machine">Wayback Machine</a></span>
</li>
<li id="cite_note-Muehlhauser,_Luke_2012-18"><span class="mw-cite-backlink"><b><a href="#cite_ref-Muehlhauser,_Luke_2012_18-0">^</a></b></span> <span class="reference-text">Muehlhauser, Luke, and Louie Helm. 2012. "Intelligence Explosion and Machine Ethics." In Singularity Hypotheses: A Scientific and Philosophical Assessment, edited by Amnon Eden, Johnny Søraker, James H. Moor, and Eric Steinhart. Berlin: Springer.</span>
</li>
<li id="cite_note-19"><span class="mw-cite-backlink"><b><a href="#cite_ref-19">^</a></b></span> <span class="reference-text">Yudkowsky, Eliezer. 2011. "Complex Value Systems in Friendly AI." In Schmidhuber, Thórisson, and Looks 2011, 388–393.</span>
</li>
<li id="cite_note-20"><span class="mw-cite-backlink"><b><a href="#cite_ref-20">^</a></b></span> <span class="reference-text"><i><a rel="nofollow" class="external text" href="http://www.singinst.org/ourresearch/presentations/">Creating a New Intelligent Species: Choices and Responsibilities for Artificial Intelligence Designers</a> <a rel="nofollow" class="external text" href="https://web.archive.org/web/20070206060938/http://www.singinst.org/ourresearch/presentations/">Archived</a> February 6, 2007, at the <a href="/wiki/Wayback_Machine" title="Wayback Machine">Wayback Machine</a></i> - <a href="/wiki/Singularity_Institute_for_Artificial_Intelligence" class="mw-redirect" title="Singularity Institute for Artificial Intelligence">Singularity Institute for Artificial Intelligence</a>, 2005</span>
</li>
<li id="cite_note-Tucker2014-21"><span class="mw-cite-backlink"><b><a href="#cite_ref-Tucker2014_21-0">^</a></b></span> <span class="reference-text"><cite class="citation news">Tucker, Patrick (17 Apr 2014). <a rel="nofollow" class="external text" href="http://www.defenseone.com/technology/2014/04/why-there-will-be-robot-uprising/82783/">"Why There Will Be A Robot Uprising"</a>. Defense One<span class="reference-accessdate">. Retrieved <span class="nowrap">15 July</span> 2014</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Why+There+Will+Be+A+Robot+Uprising&amp;rft.date=2014-04-17&amp;rft.aulast=Tucker&amp;rft.aufirst=Patrick&amp;rft_id=http%3A%2F%2Fwww.defenseone.com%2Ftechnology%2F2014%2F04%2Fwhy-there-will-be-robot-uprising%2F82783%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+takeover" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-22"><span class="mw-cite-backlink"><b><a href="#cite_ref-22">^</a></b></span> <span class="reference-text">Yudkowsky, Eliezer. 2008. "Artificial Intelligence as a Positive and Negative Factor in Global Risk."
In Global Catastrophic Risks, edited by Nick Bostrom and Milan M. Ćirković, 308–345.</span>
</li>
<li id="cite_note-23"><span class="mw-cite-backlink"><b><a href="#cite_ref-23">^</a></b></span> <span class="reference-text"><cite class="citation web">Rawlinson, Kevin. <a rel="nofollow" class="external text" href="https://www.bbc.co.uk/news/31047780">"Microsoft's Bill Gates insists AI is a threat"</a>. <a href="/wiki/BBC_News" title="BBC News">BBC News</a><span class="reference-accessdate">. Retrieved <span class="nowrap">30 January</span> 2015</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Microsoft%27s+Bill+Gates+insists+AI+is+a+threat&amp;rft.pub=BBC+News&amp;rft.aulast=Rawlinson&amp;rft.aufirst=Kevin&amp;rft_id=https%3A%2F%2Fwww.bbc.co.uk%2Fnews%2F31047780&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+takeover" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-24"><span class="mw-cite-backlink"><b><a href="#cite_ref-24">^</a></b></span> <span class="reference-text"><cite class="citation web"><a rel="nofollow" class="external text" href="http://futureoflife.org/ai-open-letter">"The Future of Life Institute Open Letter"</a>. The Future of Life Institute<span class="reference-accessdate">. Retrieved <span class="nowrap">29 March</span> 2019</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=The+Future+of+Life+Institute+Open+Letter&amp;rft.pub=The+Future+of+Life+Institute&amp;rft_id=http%3A%2F%2Ffutureoflife.org%2Fai-open-letter&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+takeover" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-25"><span class="mw-cite-backlink"><b><a href="#cite_ref-25">^</a></b></span> <span class="reference-text"><cite class="citation web"><a rel="nofollow" class="external text" href="https://www.ft.com/cms/s/0/3d2c2f12-99e9-11e4-93c1-00144feabdc0.html#axzz3TNL9lxJV">"Scientists and investors warn on AI"</a>. The Financial Times<span class="reference-accessdate">. Retrieved <span class="nowrap">4 March</span> 2015</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Scientists+and+investors+warn+on+AI&amp;rft.pub=The+Financial+Times&amp;rft_id=http%3A%2F%2Fwww.ft.com%2Fcms%2Fs%2F0%2F3d2c2f12-99e9-11e4-93c1-00144feabdc0.html%23axzz3TNL9lxJV&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+takeover" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
</ol></div></div>
<h2><span class="mw-headline" id="External_links">External links</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=AI_takeover&amp;action=edit&amp;section=21" title="Edit section: External links">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<ul><li><a rel="nofollow" class="external text" href="http://robohub.org/automation-not-domination-how-robots-will-take-over-our-world/">Automation, not domination: How robots will take over our world</a> (a positive outlook of robot and AI integration into society)</li>
<li><a rel="nofollow" class="external text" href="http://www.intelligence.org/">Machine Intelligence Research Institute</a>: official MIRI (formerly Singularity Institute for Artificial Intelligence) website</li>
<li><a rel="nofollow" class="external text" href="http://lifeboat.com/ex/ai.shield/">Lifeboat Foundation AIShield</a> (To protect against unfriendly AI)</li>
<li><a rel="nofollow" class="external text" href="https://www.youtube.com/watch?v=R_sSpPyruj0">Ted talk: Can we build AI without losing control over it?</a></li></ul>
<div role="navigation" class="navbox" aria-labelledby="Existential_risk_from_artificial_intelligence" style="padding:3px"><table class="nowraplinks mw-collapsible expanded navbox-inner" style="border-spacing:0;background:transparent;color:inherit"><tbody><tr><th scope="col" class="navbox-title" colspan="2"><div class="plainlinks hlist navbar mini"><ul><li class="nv-view"><a href="/wiki/Template:Existential_risk_from_artificial_intelligence" title="Template:Existential risk from artificial intelligence"><abbr title="View this template" style=";;background:none transparent;border:none;-moz-box-shadow:none;-webkit-box-shadow:none;box-shadow:none; padding:0;">v</abbr></a></li><li class="nv-talk"><a href="/wiki/Template_talk:Existential_risk_from_artificial_intelligence" title="Template talk:Existential risk from artificial intelligence"><abbr title="Discuss this template" style=";;background:none transparent;border:none;-moz-box-shadow:none;-webkit-box-shadow:none;box-shadow:none; padding:0;">t</abbr></a></li><li class="nv-edit"><a class="external text" href="https://en.wikipedia.org/w/index.php?title=Template:Existential_risk_from_artificial_intelligence&amp;action=edit"><abbr title="Edit this template" style=";;background:none transparent;border:none;-moz-box-shadow:none;-webkit-box-shadow:none;box-shadow:none; padding:0;">e</abbr></a></li></ul></div><div id="Existential_risk_from_artificial_intelligence" style="font-size:114%;margin:0 4em"><a href="/wiki/Existential_risk_from_artificial_general_intelligence" title="Existential risk from artificial general intelligence">Existential risk</a> from <a href="/wiki/Artificial_intelligence" title="Artificial intelligence">artificial intelligence</a></div></th></tr><tr><th scope="row" class="navbox-group" style="width:1%">Concepts</th><td class="navbox-list navbox-odd hlist" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/Accelerating_change" title="Accelerating change">Accelerating change</a></li>
<li><a href="/wiki/AI_box" title="AI box">AI box</a></li>
<li><a class="mw-selflink selflink">AI takeover</a></li>
<li><a href="/wiki/AI_control_problem" title="AI control problem">Control problem</a></li>
<li><a href="/wiki/Existential_risk_from_artificial_general_intelligence" title="Existential risk from artificial general intelligence">Existential risk from artificial general intelligence</a></li>
<li><a href="/wiki/Friendly_artificial_intelligence" title="Friendly artificial intelligence">Friendly artificial intelligence</a></li>
<li><a href="/wiki/Instrumental_convergence" title="Instrumental convergence">Instrumental convergence</a></li>
<li><a href="/wiki/Intelligence_explosion" class="mw-redirect" title="Intelligence explosion">Intelligence explosion</a></li>
<li><a href="/wiki/Machine_ethics" title="Machine ethics">Machine ethics</a></li>
<li><a href="/wiki/Superintelligence" title="Superintelligence">Superintelligence</a></li>
<li><a href="/wiki/Technological_singularity" title="Technological singularity">Technological singularity</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Organizations</th><td class="navbox-list navbox-even hlist" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/Allen_Institute_for_AI" title="Allen Institute for AI">Allen Institute for AI</a></li>
<li><a href="/wiki/Center_for_Applied_Rationality" title="Center for Applied Rationality">Center for Applied Rationality</a></li>
<li><a href="/wiki/Center_for_Human-Compatible_Artificial_Intelligence" title="Center for Human-Compatible Artificial Intelligence">Center for Human-Compatible Artificial Intelligence</a></li>
<li><a href="/wiki/Center_for_Security_and_Emerging_Technology" title="Center for Security and Emerging Technology">Center for Security and Emerging Technology</a></li>
<li><a href="/wiki/Centre_for_the_Study_of_Existential_Risk" title="Centre for the Study of Existential Risk">Centre for the Study of Existential Risk</a></li>
<li><a href="/wiki/DeepMind" title="DeepMind">DeepMind</a></li>
<li><a href="/wiki/Foundational_Questions_Institute" title="Foundational Questions Institute">Foundational Questions Institute</a></li>
<li><a href="/wiki/Future_of_Humanity_Institute" title="Future of Humanity Institute">Future of Humanity Institute</a></li>
<li><a href="/wiki/Future_of_Life_Institute" title="Future of Life Institute">Future of Life Institute</a></li>
<li><a href="/wiki/Humanity%2B" title="Humanity+">Humanity+</a></li>
<li><a href="/wiki/Institute_for_Ethics_and_Emerging_Technologies" title="Institute for Ethics and Emerging Technologies">Institute for Ethics and Emerging Technologies</a></li>
<li><a href="/wiki/Leverhulme_Centre_for_the_Future_of_Intelligence" title="Leverhulme Centre for the Future of Intelligence">Leverhulme Centre for the Future of Intelligence</a></li>
<li><a href="/wiki/Machine_Intelligence_Research_Institute" title="Machine Intelligence Research Institute">Machine Intelligence Research Institute</a></li>
<li><a href="/wiki/OpenAI" title="OpenAI">OpenAI</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">People</th><td class="navbox-list navbox-odd hlist" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/Nick_Bostrom" title="Nick Bostrom">Nick Bostrom</a></li>
<li><a href="/wiki/K._Eric_Drexler" title="K. Eric Drexler">Eric Drexler</a></li>
<li><a href="/wiki/Sam_Harris" title="Sam Harris">Sam Harris</a></li>
<li><a href="/wiki/Stephen_Hawking" title="Stephen Hawking">Stephen Hawking</a></li>
<li><a href="/wiki/Bill_Hibbard" title="Bill Hibbard">Bill Hibbard</a></li>
<li><a href="/wiki/Bill_Joy" title="Bill Joy">Bill Joy</a></li>
<li><a href="/wiki/Elon_Musk" title="Elon Musk">Elon Musk</a></li>
<li><a href="/wiki/Steve_Omohundro" title="Steve Omohundro">Steve Omohundro</a></li>
<li><a href="/wiki/Huw_Price" title="Huw Price">Huw Price</a></li>
<li><a href="/wiki/Martin_Rees" title="Martin Rees">Martin Rees</a></li>
<li><a href="/wiki/Stuart_J._Russell" title="Stuart J. Russell">Stuart J. Russell</a></li>
<li><a href="/wiki/Jaan_Tallinn" title="Jaan Tallinn">Jaan Tallinn</a></li>
<li><a href="/wiki/Max_Tegmark" title="Max Tegmark">Max Tegmark</a></li>
<li><a href="/wiki/Frank_Wilczek" title="Frank Wilczek">Frank Wilczek</a></li>
<li><a href="/wiki/Roman_Yampolskiy" title="Roman Yampolskiy">Roman Yampolskiy</a></li>
<li><a href="/wiki/Andrew_Yang" title="Andrew Yang">Andrew Yang</a></li>
<li><a href="/wiki/Eliezer_Yudkowsky" title="Eliezer Yudkowsky">Eliezer Yudkowsky</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Other</th><td class="navbox-list navbox-even hlist" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/Global_catastrophic_risk#Artificial_intelligence" title="Global catastrophic risk">Artificial intelligence as a global catastrophic risk</a></li>
<li><a href="/wiki/Artificial_general_intelligence#Controversies_and_dangers" title="Artificial general intelligence">Controversies and dangers of artificial general intelligence</a></li>
<li><a href="/wiki/Ethics_of_artificial_intelligence" title="Ethics of artificial intelligence">Ethics of artificial intelligence</a></li>
<li><i><a href="/wiki/Human_Compatible" title="Human Compatible">Human Compatible</a></i></li>
<li><a href="/wiki/Open_Letter_on_Artificial_Intelligence" title="Open Letter on Artificial Intelligence">Open Letter on Artificial Intelligence</a></li>
<li><i><a href="/wiki/Our_Final_Invention" title="Our Final Invention">Our Final Invention</a></i></li>
<li><i><a href="/wiki/The_Precipice:_Existential_Risk_and_the_Future_of_Humanity" title="The Precipice: Existential Risk and the Future of Humanity">The Precipice</a></i></li>
<li><i><a href="/wiki/Superintelligence:_Paths,_Dangers,_Strategies" title="Superintelligence: Paths, Dangers, Strategies">Superintelligence: Paths, Dangers, Strategies</a></i></li></ul>
</div></td></tr><tr><td class="navbox-abovebelow" colspan="2"><div><img alt="Category" src="//upload.wikimedia.org/wikipedia/en/thumb/4/48/Folder_Hexagonal_Icon.svg/16px-Folder_Hexagonal_Icon.svg.png" decoding="async" title="Category" width="16" height="14" srcset="//upload.wikimedia.org/wikipedia/en/thumb/4/48/Folder_Hexagonal_Icon.svg/24px-Folder_Hexagonal_Icon.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/4/48/Folder_Hexagonal_Icon.svg/32px-Folder_Hexagonal_Icon.svg.png 2x" data-file-width="36" data-file-height="31" /> <a href="/wiki/Category:Existential_risk_from_artificial_general_intelligence" title="Category:Existential risk from artificial general intelligence">Category</a></div></td></tr></tbody></table></div>
<div role="navigation" class="navbox" aria-labelledby="Global_catastrophic_risks" style="padding:3px"><table class="nowraplinks hlist mw-collapsible autocollapse navbox-inner" style="border-spacing:0;background:transparent;color:inherit"><tbody><tr><th scope="col" class="navbox-title" colspan="2"><div class="plainlinks hlist navbar mini"><ul><li class="nv-view"><a href="/wiki/Template:Global_catastrophic_risks" title="Template:Global catastrophic risks"><abbr title="View this template" style=";;background:none transparent;border:none;-moz-box-shadow:none;-webkit-box-shadow:none;box-shadow:none; padding:0;">v</abbr></a></li><li class="nv-talk"><a href="/wiki/Template_talk:Global_catastrophic_risks" title="Template talk:Global catastrophic risks"><abbr title="Discuss this template" style=";;background:none transparent;border:none;-moz-box-shadow:none;-webkit-box-shadow:none;box-shadow:none; padding:0;">t</abbr></a></li><li class="nv-edit"><a class="external text" href="https://en.wikipedia.org/w/index.php?title=Template:Global_catastrophic_risks&amp;action=edit"><abbr title="Edit this template" style=";;background:none transparent;border:none;-moz-box-shadow:none;-webkit-box-shadow:none;box-shadow:none; padding:0;">e</abbr></a></li></ul></div><div id="Global_catastrophic_risks" style="font-size:114%;margin:0 4em"><a href="/wiki/Global_catastrophic_risk" title="Global catastrophic risk">Global catastrophic risks</a></div></th></tr><tr><td class="navbox-abovebelow" colspan="2"><div id="*_Future_of_the_Earth_*_Ultimate_fate_of_the_universe">
<ul><li><a href="/wiki/Future_of_Earth" title="Future of Earth">Future of the Earth</a></li>
<li><a href="/wiki/Ultimate_fate_of_the_universe" title="Ultimate fate of the universe">Ultimate fate of the universe</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Technological</th><td class="navbox-list navbox-odd" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/Doomsday_Clock" title="Doomsday Clock">Doomsday Clock</a></li>
<li><a href="/wiki/Gray_goo" title="Gray goo">Gray goo</a></li>
<li><a href="/wiki/Kinetic_bombardment" title="Kinetic bombardment">Kinetic bombardment</a></li>
<li><a href="/wiki/Mutual_assured_destruction" title="Mutual assured destruction">Mutual assured destruction</a>
<ul><li><a href="/wiki/Dead_Hand" title="Dead Hand">Dead Hand</a></li>
<li><a href="/wiki/Doomsday_device" title="Doomsday device">Doomsday device</a></li>
<li><a href="/wiki/Antimatter_weapon" title="Antimatter weapon">Antimatter weapon</a></li></ul></li>
<li><a href="/wiki/Synthetic_intelligence" title="Synthetic intelligence">Synthetic intelligence</a> / <a href="/wiki/Artificial_intelligence" title="Artificial intelligence">Artificial intelligence</a>
<ul><li><a href="/wiki/Existential_risk_from_artificial_general_intelligence" title="Existential risk from artificial general intelligence">Existential risk from artificial intelligence</a></li>
<li><a class="mw-selflink selflink">AI takeover</a></li></ul></li>
<li><a href="/wiki/Technological_singularity" title="Technological singularity">Technological singularity</a></li>
<li><a href="/wiki/Transhumanism" title="Transhumanism">Transhumanism</a></li>
<li><a href="/wiki/Year_2000_problem" title="Year 2000 problem">Year 2000 problem</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Sociological</th><td class="navbox-list navbox-even" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/Malthusian_catastrophe" title="Malthusian catastrophe">Malthusian catastrophe</a></li>
<li><a href="/wiki/New_World_Order_(conspiracy_theory)" title="New World Order (conspiracy theory)">New World Order (conspiracy theory)</a></li>
<li><a href="/wiki/Nuclear_holocaust" title="Nuclear holocaust">Nuclear holocaust</a>
<ul><li><a href="/wiki/Nuclear_winter" title="Nuclear winter">winter</a></li>
<li><a href="/wiki/Nuclear_famine" title="Nuclear famine">famine</a></li>
<li><a href="/wiki/Cobalt_bomb" title="Cobalt bomb">cobalt</a></li></ul></li>
<li><a href="/wiki/Societal_collapse" title="Societal collapse">Societal collapse</a></li>
<li><a href="/wiki/Collapsology" title="Collapsology">Collapsology</a></li>
<li><a href="/wiki/World_War_III" title="World War III">World War III</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Ecological</th><td class="navbox-list navbox-odd" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em"></div><table class="nowraplinks navbox-subgroup" style="border-spacing:0"><tbody><tr><th scope="row" class="navbox-group" style="width:1%"><a href="/wiki/Climate_change_(general_concept)" title="Climate change (general concept)">Climate change</a></th><td class="navbox-list navbox-odd" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/Extinction_risk_from_global_warming" title="Extinction risk from global warming">Extinction risk from global warming</a>
<ul><li><a href="/wiki/Tipping_points_in_the_climate_system" title="Tipping points in the climate system">Tipping points in the climate system</a></li></ul></li>
<li><a href="/wiki/Global_terrestrial_stilling" title="Global terrestrial stilling">Global terrestrial stilling</a></li>
<li><a href="/wiki/Global_warming" title="Global warming">Global warming</a></li>
<li><a href="/wiki/Hypercane" title="Hypercane">Hypercane</a></li>
<li><a href="/wiki/Ice_age" title="Ice age">Ice age</a></li>
<li><a href="/wiki/Ecocide" title="Ecocide">Ecocide</a></li>
<li><a href="/wiki/Human_impact_on_the_environment" title="Human impact on the environment">Human impact on the environment</a></li>
<li><a href="/wiki/Ozone_depletion" title="Ozone depletion">Ozone depletion</a></li>
<li><a href="/wiki/Cascade_effect_(ecology)" title="Cascade effect (ecology)">Cascade effect</a></li>
<li><a href="/wiki/Supervolcano" title="Supervolcano">Supervolcano</a>
<ul><li><a href="/wiki/Volcanic_winter" title="Volcanic winter">winter</a></li></ul></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%"><a href="/wiki/Earth_Overshoot_Day" title="Earth Overshoot Day">Earth Overshoot Day</a></th><td class="navbox-list navbox-even" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/Overexploitation" title="Overexploitation">Overexploitation</a></li>
<li><a href="/wiki/Overpopulation" title="Overpopulation">Overpopulation</a>
<ul><li><a href="/wiki/Human_overpopulation" title="Human overpopulation">Human overpopulation</a></li></ul></li></ul>
</div></td></tr></tbody></table><div></div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Biological</th><td class="navbox-list navbox-odd" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em"></div><table class="nowraplinks navbox-subgroup" style="border-spacing:0"><tbody><tr><th scope="row" class="navbox-group" style="width:1%"><a href="/wiki/Extinction" title="Extinction">Extinction</a></th><td class="navbox-list navbox-odd" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/Extinction_event" title="Extinction event">Extinction event</a></li>
<li><a href="/wiki/Human_extinction" title="Human extinction">Human extinction</a></li>
<li><a href="/wiki/Genetic_erosion" title="Genetic erosion">Genetic erosion</a></li>
<li><a href="/wiki/Genetic_pollution" title="Genetic pollution">Genetic pollution</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Others</th><td class="navbox-list navbox-even" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/Dysgenics" title="Dysgenics">Dysgenics</a></li>
<li><a href="/wiki/Pandemic" title="Pandemic">Pandemic</a>
<ul><li><a href="/wiki/Biological_agent" title="Biological agent">Biological agent</a></li></ul></li></ul>
</div></td></tr></tbody></table><div></div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Astronomical</th><td class="navbox-list navbox-odd" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/Big_Crunch" title="Big Crunch">Big Crunch</a></li>
<li><a href="/wiki/Big_Rip" title="Big Rip">Big Rip</a></li>
<li><a href="/wiki/Coronal_mass_ejection" title="Coronal mass ejection">Coronal mass ejection</a></li>
<li><a href="/wiki/Gamma-ray_burst" title="Gamma-ray burst">Gamma-ray burst</a></li>
<li><a href="/wiki/Impact_event" title="Impact event">Impact event</a>
<ul><li><a href="/wiki/Asteroid_impact_avoidance" title="Asteroid impact avoidance">Asteroid impact avoidance</a></li>
<li><a href="/wiki/Potentially_hazardous_object" title="Potentially hazardous object">Potentially hazardous object</a></li></ul></li>
<li><a href="/wiki/Near-Earth_supernova" title="Near-Earth supernova">Near-Earth supernova</a></li>
<li><a href="/wiki/Solar_flare" title="Solar flare">Solar flare</a></li>
<li><a href="/wiki/Stellar_collision" title="Stellar collision">Stellar collision</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Mythological</th><td class="navbox-list navbox-odd" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em"></div><table class="nowraplinks navbox-subgroup" style="border-spacing:0"><tbody><tr><th scope="row" class="navbox-group" style="width:1%"><a href="/wiki/Eschatology" title="Eschatology">Eschatology</a></th><td class="navbox-list navbox-even" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/Buddhist_eschatology" title="Buddhist eschatology">Buddhist</a></li>
<li><a href="/wiki/Hindu_eschatology" title="Hindu eschatology">Hindu</a></li>
<li><a href="/wiki/Last_Judgment" title="Last Judgment">Last Judgment</a>
<ul><li><a href="/wiki/Christian_eschatology" title="Christian eschatology">Christian</a>
<ul><li><a href="/wiki/Book_of_Revelation" title="Book of Revelation">Book of Revelation</a></li></ul></li>
<li><a href="/wiki/Islamic_eschatology" title="Islamic eschatology">Islamic</a></li>
<li><a href="/wiki/Jewish_eschatology" title="Jewish eschatology">Jewish</a></li></ul></li>
<li><a href="/wiki/Ragnar%C3%B6k" title="Ragnarök">Norse</a></li>
<li><a href="/wiki/Frashokereti" title="Frashokereti">Zoroastrian</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Others</th><td class="navbox-list navbox-odd" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/2011_end_times_prediction" title="2011 end times prediction">2011 end times prediction</a></li>
<li><a href="/wiki/2012_phenomenon" title="2012 phenomenon">2012 phenomenon</a></li>
<li><a href="/wiki/Apocalyptic_literature" title="Apocalyptic literature">Apocalypse</a></li>
<li><a href="/wiki/Armageddon" title="Armageddon">Armageddon</a></li>
<li><a href="/wiki/Blood_moon_prophecy" title="Blood moon prophecy">Blood moon prophecy</a></li>
<li><a href="/wiki/End_time" title="End time">End time</a></li>
<li><a href="/wiki/List_of_dates_predicted_for_apocalyptic_events" title="List of dates predicted for apocalyptic events">List of dates predicted for apocalyptic events</a></li>
<li><a href="/wiki/Nibiru_cataclysm" title="Nibiru cataclysm">Nibiru cataclysm</a></li>
<li><a href="/wiki/Rapture" title="Rapture">Rapture</a></li>
<li><a href="/wiki/Revelation_12_sign_prophecy" title="Revelation 12 sign prophecy">Revelation 12 sign prophecy</a></li>
<li><a href="/wiki/Third_Temple" title="Third Temple">Third Temple</a></li></ul>
</div></td></tr></tbody></table><div></div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Fictional</th><td class="navbox-list navbox-even" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/Alien_invasion" title="Alien invasion">Alien invasion</a></li>
<li><a href="/wiki/Apocalyptic_and_post-apocalyptic_fiction" title="Apocalyptic and post-apocalyptic fiction">Apocalyptic and post-apocalyptic fiction</a>
<ul><li><a href="/wiki/List_of_apocalyptic_and_post-apocalyptic_fiction" title="List of apocalyptic and post-apocalyptic fiction">List of apocalyptic and post-apocalyptic fiction</a></li>
<li><a href="/wiki/List_of_apocalyptic_films" title="List of apocalyptic films">List of apocalyptic films</a></li></ul></li>
<li><a href="/wiki/Climate_fiction" title="Climate fiction">Climate fiction</a></li>
<li><a href="/wiki/Disaster_film" title="Disaster film">Disaster films</a>
<ul><li><a href="/wiki/List_of_disaster_films" title="List of disaster films">List of disaster films</a></li></ul></li>
<li><a href="/wiki/List_of_fictional_doomsday_devices" title="List of fictional doomsday devices">List of fictional doomsday devices</a></li>
<li><a href="/wiki/Zombie_apocalypse" title="Zombie apocalypse">Zombie apocalypse</a></li></ul>
</div></td></tr><tr><td class="navbox-abovebelow" colspan="2"><div>
<ul><li><img alt="Category" src="//upload.wikimedia.org/wikipedia/en/thumb/4/48/Folder_Hexagonal_Icon.svg/16px-Folder_Hexagonal_Icon.svg.png" decoding="async" title="Category" width="16" height="14" srcset="//upload.wikimedia.org/wikipedia/en/thumb/4/48/Folder_Hexagonal_Icon.svg/24px-Folder_Hexagonal_Icon.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/4/48/Folder_Hexagonal_Icon.svg/32px-Folder_Hexagonal_Icon.svg.png 2x" data-file-width="36" data-file-height="31" /> Categories
<ul><li><a href="/wiki/Category:Apocalypticism" title="Category:Apocalypticism">Apocalypticism</a></li>
<li><a href="/wiki/Category:Future_problems" title="Category:Future problems">Future problems</a></li>
<li><a href="/wiki/Category:Hazards" title="Category:Hazards">Hazards</a></li>
<li><a href="/wiki/Category:Risk_analysis" title="Category:Risk analysis">Risk analysis</a></li>
<li><a href="/wiki/Category:Doomsday_scenarios" title="Category:Doomsday scenarios">Doomsday scenarios</a></li></ul></li></ul>
</div></td></tr></tbody></table></div>
<!-- 
NewPP limit report
Parsed by mw1376
Cached time: 20200415180407
Cache expiry: 2592000
Dynamic content: false
Complications: [vary‐revision‐sha1]
CPU time usage: 0.648 seconds
Real time usage: 0.834 seconds
Preprocessor visited node count: 4409/1000000
Post‐expand include size: 137178/2097152 bytes
Template argument size: 14189/2097152 bytes
Highest expansion depth: 12/40
Expensive parser function count: 6/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 65578/5000000 bytes
Number of Wikibase entities loaded: 2/400
Lua time usage: 0.345/10.000 seconds
Lua memory usage: 7.13 MB/50 MB
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%  711.714      1 -total
 38.52%  274.163      1 Template:Reflist
 18.88%  134.389     10 Template:Cite_web
 14.97%  106.517     14 Template:Fix
 13.40%   95.404     11 Template:Citation_needed
 12.59%   89.639      1 Template:More_citations_needed
 10.37%   73.800      1 Template:Ambox
  8.40%   59.758     14 Template:Delink
  7.95%   56.594      8 Template:According_to_whom
  7.88%   56.111     33 Template:Category_handler
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:813176-0!canonical and timestamp 20200415180407 and revision id 951139246
 -->
</div><noscript><img src="//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" alt="" title="" width="1" height="1" style="border: none; position: absolute;" /></noscript></div>
		<div class="printfooter">Retrieved from "<a dir="ltr" href="https://en.wikipedia.org/w/index.php?title=AI_takeover&amp;oldid=951139246">https://en.wikipedia.org/w/index.php?title=AI_takeover&amp;oldid=951139246</a>"</div>
		<div id="catlinks" class="catlinks" data-mw="interface"><div id="mw-normal-catlinks" class="mw-normal-catlinks"><a href="/wiki/Help:Category" title="Help:Category">Categories</a>: <ul><li><a href="/wiki/Category:Doomsday_scenarios" title="Category:Doomsday scenarios">Doomsday scenarios</a></li><li><a href="/wiki/Category:Future_problems" title="Category:Future problems">Future problems</a></li><li><a href="/wiki/Category:Existential_risk_from_artificial_general_intelligence" title="Category:Existential risk from artificial general intelligence">Existential risk from artificial general intelligence</a></li></ul></div><div id="mw-hidden-catlinks" class="mw-hidden-catlinks mw-hidden-cats-hidden">Hidden categories: <ul><li><a href="/wiki/Category:Webarchive_template_wayback_links" title="Category:Webarchive template wayback links">Webarchive template wayback links</a></li><li><a href="/wiki/Category:Articles_needing_additional_references_from_January_2020" title="Category:Articles needing additional references from January 2020">Articles needing additional references from January 2020</a></li><li><a href="/wiki/Category:All_articles_needing_additional_references" title="Category:All articles needing additional references">All articles needing additional references</a></li><li><a href="/wiki/Category:Articles_with_short_description" title="Category:Articles with short description">Articles with short description</a></li><li><a href="/wiki/Category:All_articles_with_specifically_marked_weasel-worded_phrases" title="Category:All articles with specifically marked weasel-worded phrases">All articles with specifically marked weasel-worded phrases</a></li><li><a href="/wiki/Category:Articles_with_specifically_marked_weasel-worded_phrases_from_January_2020" title="Category:Articles with specifically marked weasel-worded phrases from January 2020">Articles with specifically marked weasel-worded phrases from January 2020</a></li><li><a href="/wiki/Category:All_articles_with_unsourced_statements" title="Category:All articles with unsourced statements">All articles with unsourced statements</a></li><li><a href="/wiki/Category:Articles_with_unsourced_statements_from_January_2020" title="Category:Articles with unsourced statements from January 2020">Articles with unsourced statements from January 2020</a></li><li><a href="/wiki/Category:All_accuracy_disputes" title="Category:All accuracy disputes">All accuracy disputes</a></li><li><a href="/wiki/Category:Articles_with_disputed_statements_from_January_2020" title="Category:Articles with disputed statements from January 2020">Articles with disputed statements from January 2020</a></li></ul></div></div>
		<div class="visualClear"></div>
		
	</div>
</div>
<div id='mw-data-after-content'>
	<div class="read-more-container"></div>
</div>

<div id="mw-navigation">
    <h2>Navigation menu</h2>
    <div id="mw-head">
        
        <div id="p-personal" role="navigation" class="" aria-labelledby="p-personal-label">
        	<h3 id="p-personal-label">Personal tools</h3>
        	<ul >
        		
        		<li id="pt-anonuserpage">Not logged in</li>
        		<li id="pt-anontalk"><a href="/wiki/Special:MyTalk" title="Discussion about edits from this IP address [n]" accesskey="n">Talk</a></li><li id="pt-anoncontribs"><a href="/wiki/Special:MyContributions" title="A list of edits made from this IP address [y]" accesskey="y">Contributions</a></li><li id="pt-createaccount"><a href="/w/index.php?title=Special:CreateAccount&amp;returnto=AI+takeover" title="You are encouraged to create an account and log in; however, it is not mandatory">Create account</a></li><li id="pt-login"><a href="/w/index.php?title=Special:UserLogin&amp;returnto=AI+takeover" title="You&#039;re encouraged to log in; however, it&#039;s not mandatory. [o]" accesskey="o">Log in</a></li>
        	</ul>
        </div>
        <div id="left-navigation">
            <div id="p-namespaces" role="navigation" class="vectorTabs " aria-labelledby="p-namespaces-label">
            	<h3 id="p-namespaces-label">Namespaces</h3>
            	<ul >
            		<li id="ca-nstab-main" class="selected"><a href="/wiki/AI_takeover" title="View the content page [c]" accesskey="c">Article</a></li><li id="ca-talk"><a href="/wiki/Talk:AI_takeover" rel="discussion" title="Discuss improvements to the content page [t]" accesskey="t">Talk</a></li>
            	</ul>
            </div>
            <div id="p-variants" role="navigation" class="vectorMenu emptyPortlet" aria-labelledby="p-variants-label">
            	<input type="checkbox" class="vectorMenuCheckbox" aria-labelledby="p-variants-label" />
            	<h3 id="p-variants-label">
            		<span>Variants</span>
            	</h3>
            	<ul class="menu" >
            		
            	</ul>
            </div>
        </div>
        <div id="right-navigation">
            <div id="p-views" role="navigation" class="vectorTabs " aria-labelledby="p-views-label">
            	<h3 id="p-views-label">Views</h3>
            	<ul >
            		<li id="ca-view" class="collapsible selected"><a href="/wiki/AI_takeover">Read</a></li><li id="ca-edit" class="collapsible"><a href="/w/index.php?title=AI_takeover&amp;action=edit" title="Edit this page [e]" accesskey="e">Edit</a></li><li id="ca-history" class="collapsible"><a href="/w/index.php?title=AI_takeover&amp;action=history" title="Past revisions of this page [h]" accesskey="h">View history</a></li>
            	</ul>
            </div>
            <div id="p-cactions" role="navigation" class="vectorMenu emptyPortlet" aria-labelledby="p-cactions-label">
            	<input type="checkbox" class="vectorMenuCheckbox" aria-labelledby="p-cactions-label" />
            	<h3 id="p-cactions-label">
            		<span>More</span>
            	</h3>
            	<ul class="menu" >
            		
            	</ul>
            </div>
            <div id="p-search" role="search">
            	<h3 >
            		<label for="searchInput">Search</label>
            	</h3>
            	<form action="/w/index.php" id="searchform">
            		<div id="simpleSearch">
            			<input type="search" name="search" placeholder="Search Wikipedia" title="Search Wikipedia [f]" accesskey="f" id="searchInput"/>
            			<input type="hidden" value="Special:Search" name="title"/>
            			<input type="submit" name="fulltext" value="Search" title="Search Wikipedia for this text" id="mw-searchButton" class="searchButton mw-fallbackSearchButton"/>
            			<input type="submit" name="go" value="Go" title="Go to a page with this exact name if it exists" id="searchButton" class="searchButton"/>
            		</div>
            	</form>
            </div>
        </div>
    </div>
    
    <div id="mw-panel">
    	<div id="p-logo" role="banner">
    		<a  title="Visit the main page" class="mw-wiki-logo" href="/wiki/Main_Page"></a>
    	</div>
    	<div class="portal" role="navigation" id="p-navigation"  aria-labelledby="p-navigation-label">
    		<h3  id="p-navigation-label">
    			Navigation
    		</h3>
    		<div class="body">
    			<ul><li id="n-mainpage-description"><a href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z">Main page</a></li><li id="n-contents"><a href="/wiki/Wikipedia:Contents" title="Guides to browsing Wikipedia">Contents</a></li><li id="n-featuredcontent"><a href="/wiki/Wikipedia:Featured_content" title="Featured content – the best of Wikipedia">Featured content</a></li><li id="n-currentevents"><a href="/wiki/Portal:Current_events" title="Find background information on current events">Current events</a></li><li id="n-randompage"><a href="/wiki/Special:Random" title="Load a random article [x]" accesskey="x">Random article</a></li><li id="n-sitesupport"><a href="https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&amp;utm_medium=sidebar&amp;utm_campaign=C13_en.wikipedia.org&amp;uselang=en" title="Support us">Donate to Wikipedia</a></li><li id="n-shoplink"><a href="//shop.wikimedia.org" title="Visit the Wikipedia store">Wikipedia store</a></li></ul>
    			
    		</div>
    	</div>
    	
    	<div class="portal" role="navigation" id="p-interaction"  aria-labelledby="p-interaction-label">
    		<h3  id="p-interaction-label">
    			Interaction
    		</h3>
    		<div class="body">
    			<ul><li id="n-help"><a href="/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia">Help</a></li><li id="n-aboutsite"><a href="/wiki/Wikipedia:About" title="Find out about Wikipedia">About Wikipedia</a></li><li id="n-portal"><a href="/wiki/Wikipedia:Community_portal" title="About the project, what you can do, where to find things">Community portal</a></li><li id="n-recentchanges"><a href="/wiki/Special:RecentChanges" title="A list of recent changes in the wiki [r]" accesskey="r">Recent changes</a></li><li id="n-contactpage"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia">Contact page</a></li></ul>
    			
    		</div>
    	</div>
    	
    	<div class="portal" role="navigation" id="p-tb"  aria-labelledby="p-tb-label">
    		<h3  id="p-tb-label">
    			Tools
    		</h3>
    		<div class="body">
    			<ul><li id="t-whatlinkshere"><a href="/wiki/Special:WhatLinksHere/AI_takeover" title="List of all English Wikipedia pages containing links to this page [j]" accesskey="j">What links here</a></li><li id="t-recentchangeslinked"><a href="/wiki/Special:RecentChangesLinked/AI_takeover" rel="nofollow" title="Recent changes in pages linked from this page [k]" accesskey="k">Related changes</a></li><li id="t-upload"><a href="/wiki/Wikipedia:File_Upload_Wizard" title="Upload files [u]" accesskey="u">Upload file</a></li><li id="t-specialpages"><a href="/wiki/Special:SpecialPages" title="A list of all special pages [q]" accesskey="q">Special pages</a></li><li id="t-permalink"><a href="/w/index.php?title=AI_takeover&amp;oldid=951139246" title="Permanent link to this revision of the page">Permanent link</a></li><li id="t-info"><a href="/w/index.php?title=AI_takeover&amp;action=info" title="More information about this page">Page information</a></li><li id="t-wikibase"><a href="https://www.wikidata.org/wiki/Special:EntityPage/Q2254427" title="Link to connected data repository item [g]" accesskey="g">Wikidata item</a></li><li id="t-cite"><a href="/w/index.php?title=Special:CiteThisPage&amp;page=AI_takeover&amp;id=951139246&amp;wpFormIdentifier=titleform" title="Information on how to cite this page">Cite this page</a></li></ul>
    			
    		</div>
    	</div>
    	
    	<div class="portal" role="navigation" id="p-coll-print_export"  aria-labelledby="p-coll-print_export-label">
    		<h3  id="p-coll-print_export-label">
    			Print/export
    		</h3>
    		<div class="body">
    			<ul><li id="coll-create_a_book"><a href="/w/index.php?title=Special:Book&amp;bookcmd=book_creator&amp;referer=AI+takeover">Create a book</a></li><li id="coll-download-as-rl"><a href="/w/index.php?title=Special:ElectronPdf&amp;page=AI+takeover&amp;action=show-download-screen">Download as PDF</a></li><li id="t-print"><a href="/w/index.php?title=AI_takeover&amp;printable=yes" title="Printable version of this page [p]" accesskey="p">Printable version</a></li></ul>
    			
    		</div>
    	</div>
    	
    	<div class="portal" role="navigation" id="p-lang"  aria-labelledby="p-lang-label">
    		<h3  id="p-lang-label">
    			Languages
    		</h3>
    		<div class="body">
    			<ul><li class="interlanguage-link interwiki-ar"><a href="https://ar.wikipedia.org/wiki/%D8%B3%D9%8A%D8%B7%D8%B1%D8%A9_%D8%A7%D9%84%D8%B0%D9%83%D8%A7%D8%A1_%D8%A7%D9%84%D8%A7%D8%B5%D8%B7%D9%86%D8%A7%D8%B9%D9%8A" title="سيطرة الذكاء الاصطناعي – Arabic" lang="ar" hreflang="ar" class="interlanguage-link-target">العربية</a></li><li class="interlanguage-link interwiki-ast"><a href="https://ast.wikipedia.org/wiki/Rebeli%C3%B3n_de_les_m%C3%A1quines" title="Rebelión de les máquines – Asturian" lang="ast" hreflang="ast" class="interlanguage-link-target">Asturianu</a></li><li class="interlanguage-link interwiki-cs"><a href="https://cs.wikipedia.org/wiki/Vzpoura_stroj%C5%AF" title="Vzpoura strojů – Czech" lang="cs" hreflang="cs" class="interlanguage-link-target">Čeština</a></li><li class="interlanguage-link interwiki-es"><a href="https://es.wikipedia.org/wiki/Rebeli%C3%B3n_de_las_m%C3%A1quinas" title="Rebelión de las máquinas – Spanish" lang="es" hreflang="es" class="interlanguage-link-target">Español</a></li><li class="interlanguage-link interwiki-fr"><a href="https://fr.wikipedia.org/wiki/R%C3%A9volte_des_robots" title="Révolte des robots – French" lang="fr" hreflang="fr" class="interlanguage-link-target">Français</a></li><li class="interlanguage-link interwiki-it"><a href="https://it.wikipedia.org/wiki/Ribellione_della_macchina" title="Ribellione della macchina – Italian" lang="it" hreflang="it" class="interlanguage-link-target">Italiano</a></li><li class="interlanguage-link interwiki-ro"><a href="https://ro.wikipedia.org/wiki/Revolt%C4%83_cibernetic%C4%83" title="Revoltă cibernetică – Romanian" lang="ro" hreflang="ro" class="interlanguage-link-target">Română</a></li><li class="interlanguage-link interwiki-ru"><a href="https://ru.wikipedia.org/wiki/%D0%92%D0%BE%D1%81%D1%81%D1%82%D0%B0%D0%BD%D0%B8%D0%B5_%D0%BC%D0%B0%D1%88%D0%B8%D0%BD" title="Восстание машин – Russian" lang="ru" hreflang="ru" class="interlanguage-link-target">Русский</a></li><li class="interlanguage-link interwiki-fi"><a href="https://fi.wikipedia.org/wiki/Koneiden_kapina" title="Koneiden kapina – Finnish" lang="fi" hreflang="fi" class="interlanguage-link-target">Suomi</a></li><li class="interlanguage-link interwiki-zh"><a href="https://zh.wikipedia.org/wiki/%E4%BA%BA%E5%B7%A5%E6%99%BA%E6%85%A7%E5%8F%9B%E8%AE%8A" title="人工智慧叛變 – Chinese" lang="zh" hreflang="zh" class="interlanguage-link-target">中文</a></li></ul>
    			<div class="after-portlet after-portlet-lang"><span class="wb-langlinks-edit wb-langlinks-link"><a href="https://www.wikidata.org/wiki/Special:EntityPage/Q2254427#sitelinks-wikipedia" title="Edit interlanguage links" class="wbc-editpage">Edit links</a></span></div>
    		</div>
    	</div>
    	
    </div>
</div>

<div id="footer" role="contentinfo" >
	<ul id="footer-info" class="">
		<li id="footer-info-lastmod"> This page was last edited on 15 April 2020, at 18:04<span class="anonymous-show">&#160;(UTC)</span>.</li>
		<li id="footer-info-copyright">Text is available under the <a rel="license" href="//en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License">Creative Commons Attribution-ShareAlike License</a><a rel="license" href="//creativecommons.org/licenses/by-sa/3.0/" style="display:none;"></a>;
additional terms may apply.  By using this site, you agree to the <a href="//foundation.wikimedia.org/wiki/Terms_of_Use">Terms of Use</a> and <a href="//foundation.wikimedia.org/wiki/Privacy_policy">Privacy Policy</a>. Wikipedia® is a registered trademark of the <a href="//www.wikimediafoundation.org/">Wikimedia Foundation, Inc.</a>, a non-profit organization.</li>
	</ul>
	<ul id="footer-places" class="">
		<li id="footer-places-privacy"><a href="https://foundation.wikimedia.org/wiki/Privacy_policy" class="extiw" title="wmf:Privacy policy">Privacy policy</a></li>
		<li id="footer-places-about"><a href="/wiki/Wikipedia:About" title="Wikipedia:About">About Wikipedia</a></li>
		<li id="footer-places-disclaimer"><a href="/wiki/Wikipedia:General_disclaimer" title="Wikipedia:General disclaimer">Disclaimers</a></li>
		<li id="footer-places-contact"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us">Contact Wikipedia</a></li>
		<li id="footer-places-developers"><a href="https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute">Developers</a></li>
		<li id="footer-places-statslink"><a href="https://stats.wikimedia.org/#/en.wikipedia.org">Statistics</a></li>
		<li id="footer-places-cookiestatement"><a href="https://foundation.wikimedia.org/wiki/Cookie_statement">Cookie statement</a></li>
		<li id="footer-places-mobileview"><a href="//en.m.wikipedia.org/w/index.php?title=AI_takeover&amp;mobileaction=toggle_view_mobile" class="noprint stopMobileRedirectToggle">Mobile view</a></li>
	</ul>
	<ul id="footer-icons" class="noprint">
		<li id="footer-copyrightico"><a href="https://wikimediafoundation.org/"><img src="/static/images/wikimedia-button.png" srcset="/static/images/wikimedia-button-1.5x.png 1.5x, /static/images/wikimedia-button-2x.png 2x" width="88" height="31" alt="Wikimedia Foundation"/></a></li>
		<li id="footer-poweredbyico"><a href="https://www.mediawiki.org/"><img src="/static/images/poweredby_mediawiki_88x31.png" alt="Powered by MediaWiki" srcset="/static/images/poweredby_mediawiki_132x47.png 1.5x, /static/images/poweredby_mediawiki_176x62.png 2x" width="88" height="31"/></a></li>
	</ul>
	<div style="clear: both;"></div>
</div>

<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgPageParseReport":{"limitreport":{"cputime":"0.648","walltime":"0.834","ppvisitednodes":{"value":4409,"limit":1000000},"postexpandincludesize":{"value":137178,"limit":2097152},"templateargumentsize":{"value":14189,"limit":2097152},"expansiondepth":{"value":12,"limit":40},"expensivefunctioncount":{"value":6,"limit":500},"unstrip-depth":{"value":1,"limit":20},"unstrip-size":{"value":65578,"limit":5000000},"entityaccesscount":{"value":2,"limit":400},"timingprofile":["100.00%  711.714      1 -total"," 38.52%  274.163      1 Template:Reflist"," 18.88%  134.389     10 Template:Cite_web"," 14.97%  106.517     14 Template:Fix"," 13.40%   95.404     11 Template:Citation_needed"," 12.59%   89.639      1 Template:More_citations_needed"," 10.37%   73.800      1 Template:Ambox","  8.40%   59.758     14 Template:Delink","  7.95%   56.594      8 Template:According_to_whom","  7.88%   56.111     33 Template:Category_handler"]},"scribunto":{"limitreport-timeusage":{"value":"0.345","limit":"10.000"},"limitreport-memusage":{"value":7481406,"limit":52428800}},"cachereport":{"origin":"mw1376","timestamp":"20200415180407","ttl":2592000,"transientcontent":false}}});});</script>
<script type="application/ld+json">{"@context":"https:\/\/schema.org","@type":"Article","name":"AI takeover","url":"https:\/\/en.wikipedia.org\/wiki\/AI_takeover","sameAs":"http:\/\/www.wikidata.org\/entity\/Q2254427","mainEntity":"http:\/\/www.wikidata.org\/entity\/Q2254427","author":{"@type":"Organization","name":"Contributors to Wikimedia projects"},"publisher":{"@type":"Organization","name":"Wikimedia Foundation, Inc.","logo":{"@type":"ImageObject","url":"https:\/\/www.wikimedia.org\/static\/images\/wmf-hor-googpub.png"}},"datePublished":"2004-07-12T23:34:47Z","dateModified":"2020-04-15T18:04:07Z","image":"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/d\/d9\/Capek_RUR.jpg","headline":"hypothetical scenario in which AI becomes the dominant form of intelligence on Earth"}</script>
<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgBackendResponseTime":96,"wgHostname":"mw1409"});});</script></body></html>
