
<!DOCTYPE html>
<html class="client-nojs" lang="en" dir="ltr">
<head>
<meta charset="UTF-8"/>
<title>Multimodal sentiment analysis - Wikipedia</title>
<script>document.documentElement.className="client-js";RLCONF={"wgBreakFrames":!1,"wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgRequestId":"XpdtGApAMM4AAUNWo78AAAAS","wgCSPNonce":!1,"wgCanonicalNamespace":"","wgCanonicalSpecialPageName":!1,"wgNamespaceNumber":0,"wgPageName":"Multimodal_sentiment_analysis","wgTitle":"Multimodal sentiment analysis","wgCurRevisionId":943074429,"wgRevisionId":943074429,"wgArticleId":57687371,"wgIsArticle":!0,"wgIsRedirect":!1,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":["CS1 errors: missing periodical","All articles with peacock terms","Articles with peacock terms from June 2018","Natural language processing","Affective computing","Social media","Machine learning"],"wgPageContentLanguage":"en","wgPageContentModel":"wikitext",
"wgRelevantPageName":"Multimodal_sentiment_analysis","wgRelevantArticleId":57687371,"wgIsProbablyEditable":!0,"wgRelevantPageIsProbablyEditable":!0,"wgRestrictionEdit":[],"wgRestrictionMove":[],"wgMediaViewerOnClick":!0,"wgMediaViewerEnabledByDefault":!0,"wgPopupsReferencePreviews":!1,"wgPopupsConflictsWithNavPopupGadget":!1,"wgVisualEditor":{"pageLanguageCode":"en","pageLanguageDir":"ltr","pageVariantFallbacks":"en"},"wgMFDisplayWikibaseDescriptions":{"search":!0,"nearby":!0,"watchlist":!0,"tagline":!1},"wgWMESchemaEditAttemptStepOversample":!1,"wgULSCurrentAutonym":"English","wgNoticeProject":"wikipedia","wgWikibaseItemId":"Q55008106","wgCentralAuthMobileDomain":!1,"wgEditSubmitButtonLabelPublish":!0};RLSTATE={"ext.globalCssJs.user.styles":"ready","site.styles":"ready","noscript":"ready","user.styles":"ready","ext.globalCssJs.user":"ready","user":"ready","user.options":"loading","ext.cite.styles":"ready","mediawiki.toc.styles":"ready",
"skins.vector.styles.legacy":"ready","wikibase.client.init":"ready","ext.visualEditor.desktopArticleTarget.noscript":"ready","ext.uls.interlanguage":"ready","ext.wikimediaBadges":"ready"};RLPAGEMODULES=["ext.cite.ux-enhancements","site","mediawiki.page.startup","skins.vector.js","mediawiki.page.ready","mediawiki.toc","ext.gadget.ReferenceTooltips","ext.gadget.charinsert","ext.gadget.refToolbar","ext.gadget.extra-toolbar-buttons","ext.gadget.switcher","ext.centralauth.centralautologin","ext.popups","ext.visualEditor.desktopArticleTarget.init","ext.visualEditor.targetLoader","ext.eventLogging","ext.wikimediaEvents","ext.navigationTiming","ext.uls.compactlinks","ext.uls.interface","ext.cx.eventlogging.campaigns","ext.quicksurveys.init","ext.centralNotice.geoIP","ext.centralNotice.startUp"];</script>
<script>(RLQ=window.RLQ||[]).push(function(){mw.loader.implement("user.options@1hzgi",function($,jQuery,require,module){/*@nomin*/mw.user.tokens.set({"patrolToken":"+\\","watchToken":"+\\","csrfToken":"+\\"});
});});</script>
<link rel="stylesheet" href="/w/load.php?lang=en&amp;modules=ext.cite.styles%7Cext.uls.interlanguage%7Cext.visualEditor.desktopArticleTarget.noscript%7Cext.wikimediaBadges%7Cmediawiki.toc.styles%7Cskins.vector.styles.legacy%7Cwikibase.client.init&amp;only=styles&amp;skin=vector"/>
<script async="" src="/w/load.php?lang=en&amp;modules=startup&amp;only=scripts&amp;raw=1&amp;skin=vector"></script>
<meta name="ResourceLoaderDynamicStyles" content=""/>
<link rel="stylesheet" href="/w/load.php?lang=en&amp;modules=site.styles&amp;only=styles&amp;skin=vector"/>
<meta name="generator" content="MediaWiki 1.35.0-wmf.27"/>
<meta name="referrer" content="origin"/>
<meta name="referrer" content="origin-when-crossorigin"/>
<meta name="referrer" content="origin-when-cross-origin"/>
<link rel="alternate" type="application/x-wiki" title="Edit this page" href="/w/index.php?title=Multimodal_sentiment_analysis&amp;action=edit"/>
<link rel="edit" title="Edit this page" href="/w/index.php?title=Multimodal_sentiment_analysis&amp;action=edit"/>
<link rel="apple-touch-icon" href="/static/apple-touch/wikipedia.png"/>
<link rel="shortcut icon" href="/static/favicon/wikipedia.ico"/>
<link rel="search" type="application/opensearchdescription+xml" href="/w/opensearch_desc.php" title="Wikipedia (en)"/>
<link rel="EditURI" type="application/rsd+xml" href="//en.wikipedia.org/w/api.php?action=rsd"/>
<link rel="license" href="//creativecommons.org/licenses/by-sa/3.0/"/>
<link rel="alternate" type="application/atom+xml" title="Wikipedia Atom feed" href="/w/index.php?title=Special:RecentChanges&amp;feed=atom"/>
<link rel="canonical" href="https://en.wikipedia.org/wiki/Multimodal_sentiment_analysis"/>
<link rel="dns-prefetch" href="//login.wikimedia.org"/>
<link rel="dns-prefetch" href="//meta.wikimedia.org" />
<!--[if lt IE 9]><script src="/w/resources/lib/html5shiv/html5shiv.js"></script><![endif]-->
</head>
<body class="mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject mw-editable page-Multimodal_sentiment_analysis rootpage-Multimodal_sentiment_analysis skin-vector action-view">
<div id="mw-page-base" class="noprint"></div>
<div id="mw-head-base" class="noprint"></div>
<div id="content" class="mw-body" role="main">
	<a id="top"></a>
	<div id="siteNotice" class="mw-body-content"><!-- CentralNotice --></div>
	<div class="mw-indicators mw-body-content">
</div>

	<h1 id="firstHeading" class="firstHeading" lang="en">Multimodal sentiment analysis</h1>
	
	<div id="bodyContent" class="mw-body-content">
		<div id="siteSub" class="noprint">From Wikipedia, the free encyclopedia</div>
		<div id="contentSub"></div>
		
		
		<div id="jump-to-nav"></div>
		<a class="mw-jump-link" href="#mw-head">Jump to navigation</a>
		<a class="mw-jump-link" href="#p-search">Jump to search</a>
		<div id="mw-content-text" lang="en" dir="ltr" class="mw-content-ltr"><div class="mw-parser-output"><p><b>Multimodal sentiment analysis</b> is a new dimension<sup class="noprint Inline-Template" style="white-space:nowrap;">&#91;<i><a href="/wiki/Wikipedia:Manual_of_Style/Words_to_watch#Puffery" title="Wikipedia:Manual of Style/Words to watch"><span title="Unverified descriptions (June 2018)">peacock&#160;term</span></a></i>&#93;</sup> of the traditional text-based <a href="/wiki/Sentiment_analysis" title="Sentiment analysis">sentiment analysis</a>, which goes beyond the analysis of texts, and includes other <a href="/wiki/Modality_(human%E2%80%93computer_interaction)" title="Modality (human–computer interaction)">modalities</a> such as audio and visual data.<sup id="cite_ref-1" class="reference"><a href="#cite_note-1">&#91;1&#93;</a></sup> It can be bimodal, which includes different combinations of two modalities, or trimodal, which incorporates three modalities.<sup id="cite_ref-2" class="reference"><a href="#cite_note-2">&#91;2&#93;</a></sup> With the extensive amount of <a href="/wiki/Social_media" title="Social media">social media</a> data available online in different forms such as videos and images, the conventional text-based <a href="/wiki/Sentiment_analysis" title="Sentiment analysis">sentiment analysis</a> has evolved into more complex models of multimodal sentiment analysis,<sup id="cite_ref-s1_3-0" class="reference"><a href="#cite_note-s1-3">&#91;3&#93;</a></sup> which can be applied in the development of  <a href="/wiki/Virtual_assistant" title="Virtual assistant">virtual assistants</a>,<sup id="cite_ref-s5_4-0" class="reference"><a href="#cite_note-s5-4">&#91;4&#93;</a></sup> <a href="/wiki/Social_media_analytics" title="Social media analytics">analysis</a> of YouTube movie reviews,<sup id="cite_ref-s4_5-0" class="reference"><a href="#cite_note-s4-5">&#91;5&#93;</a></sup> <a href="/wiki/Social_media_analytics" title="Social media analytics">analysis</a> of news videos,<sup id="cite_ref-6" class="reference"><a href="#cite_note-6">&#91;6&#93;</a></sup> and <a href="/wiki/Emotion_recognition" title="Emotion recognition">emotion recognition</a> (sometimes known as <a href="/wiki/Emotion" title="Emotion">emotion</a> detection) such as <a href="/wiki/Depression_(mood)" title="Depression (mood)">depression</a> monitoring,<sup id="cite_ref-s6_7-0" class="reference"><a href="#cite_note-s6-7">&#91;7&#93;</a></sup> among others.
</p><p>Similar to the traditional <a href="/wiki/Sentiment_analysis" title="Sentiment analysis">sentiment analysis</a>, one of the most basic task in multimodal sentiment analysis is <a href="/wiki/Feeling" title="Feeling">sentiment</a> classification, which classifies different sentiments into categories such as positive, negative, or neutral.<sup id="cite_ref-8" class="reference"><a href="#cite_note-8">&#91;8&#93;</a></sup> The complexity of <a href="/wiki/Social_media_analytics" title="Social media analytics">analyzing</a> text, audio, and visual features to perform such a task requires the application of different fusion techniques, such as feature-level, decision-level, and hybrid fusion.<sup id="cite_ref-s1_3-1" class="reference"><a href="#cite_note-s1-3">&#91;3&#93;</a></sup> The performance of these fusion techniques and the <a href="/wiki/Classification" title="Classification">classification</a> <a href="/wiki/Algorithm" title="Algorithm">algorithms</a> applied, are influenced by the type of textual, audio, and visual features employed in the analysis.<sup id="cite_ref-s7_9-0" class="reference"><a href="#cite_note-s7-9">&#91;9&#93;</a></sup>
</p>
<div id="toc" class="toc" role="navigation" aria-labelledby="mw-toc-heading"><input type="checkbox" role="button" id="toctogglecheckbox" class="toctogglecheckbox" style="display:none" /><div class="toctitle" lang="en" dir="ltr"><h2 id="mw-toc-heading">Contents</h2><span class="toctogglespan"><label class="toctogglelabel" for="toctogglecheckbox"></label></span></div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="#Features"><span class="tocnumber">1</span> <span class="toctext">Features</span></a>
<ul>
<li class="toclevel-2 tocsection-2"><a href="#Textual_features"><span class="tocnumber">1.1</span> <span class="toctext">Textual features</span></a></li>
<li class="toclevel-2 tocsection-3"><a href="#Audio_features"><span class="tocnumber">1.2</span> <span class="toctext">Audio features</span></a></li>
<li class="toclevel-2 tocsection-4"><a href="#Visual_features"><span class="tocnumber">1.3</span> <span class="toctext">Visual features</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-5"><a href="#Fusion_techniques"><span class="tocnumber">2</span> <span class="toctext">Fusion techniques</span></a>
<ul>
<li class="toclevel-2 tocsection-6"><a href="#Feature-level_fusion"><span class="tocnumber">2.1</span> <span class="toctext">Feature-level fusion</span></a></li>
<li class="toclevel-2 tocsection-7"><a href="#Decision-level_fusion"><span class="tocnumber">2.2</span> <span class="toctext">Decision-level fusion</span></a></li>
<li class="toclevel-2 tocsection-8"><a href="#Hybrid_fusion"><span class="tocnumber">2.3</span> <span class="toctext">Hybrid fusion</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-9"><a href="#Applications"><span class="tocnumber">3</span> <span class="toctext">Applications</span></a></li>
<li class="toclevel-1 tocsection-10"><a href="#References"><span class="tocnumber">4</span> <span class="toctext">References</span></a></li>
</ul>
</div>

<h2><span class="mw-headline" id="Features">Features</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Multimodal_sentiment_analysis&amp;action=edit&amp;section=1" title="Edit section: Features">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p><a href="/wiki/Feature_engineering" title="Feature engineering">Feature engineering</a>, which involves the selection of features that are fed into <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a> algorithms, plays a key role in the sentiment classification performance.<sup id="cite_ref-s7_9-1" class="reference"><a href="#cite_note-s7-9">&#91;9&#93;</a></sup> In multimodal sentiment analysis, a combination of different textual, audio, and visual features are employed.<sup id="cite_ref-s1_3-2" class="reference"><a href="#cite_note-s1-3">&#91;3&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Textual_features">Textual features</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Multimodal_sentiment_analysis&amp;action=edit&amp;section=2" title="Edit section: Textual features">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Similar to the conventional text-based <a href="/wiki/Sentiment_analysis" title="Sentiment analysis">sentiment analysis</a>, some of the most commonly used textual features in multimodal sentiment analysis are <a href="/wiki/N-grams" class="mw-redirect" title="N-grams">unigrams</a> and <a href="/wiki/N-gram" title="N-gram">n-grams</a>, which are basically a sequence of words in a given textual document.<sup id="cite_ref-10" class="reference"><a href="#cite_note-10">&#91;10&#93;</a></sup> These features are applied using <a href="/wiki/Bag-of-words" class="mw-redirect" title="Bag-of-words">bag-of-words</a> or bag-of-concepts feature representations, in which words or concepts are represented as vectors in a suitable space.<sup id="cite_ref-s2_11-0" class="reference"><a href="#cite_note-s2-11">&#91;11&#93;</a></sup><sup id="cite_ref-12" class="reference"><a href="#cite_note-12">&#91;12&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Audio_features">Audio features</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Multimodal_sentiment_analysis&amp;action=edit&amp;section=3" title="Edit section: Audio features">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p><a href="/wiki/Feeling" title="Feeling">Sentiment</a> and <a href="/wiki/Emotion" title="Emotion">emotion</a> characteristics are prominent in different <a href="/wiki/Phonetic" class="mw-redirect" title="Phonetic">phonetic</a> and <a href="/wiki/Prosodic" class="mw-redirect" title="Prosodic">prosodic</a> properties contained in audio features.<sup id="cite_ref-13" class="reference"><a href="#cite_note-13">&#91;13&#93;</a></sup> Some of the most important audio features employed in multimodal sentiment analysis are <a href="/wiki/Mel-frequency_cepstrum" title="Mel-frequency cepstrum"> mel-frequency cepstrum (MFCC)</a>, <a href="/wiki/Spectral_centroid" title="Spectral centroid">spectral centroid</a>, <a href="/wiki/Spectral_flux" title="Spectral flux">spectral flux</a>, beat histogram, beat sum, strongest beat, pause duration, and <a href="/wiki/Pitch_accent" class="mw-redirect" title="Pitch accent">pitch</a>.<sup id="cite_ref-s1_3-3" class="reference"><a href="#cite_note-s1-3">&#91;3&#93;</a></sup> <a href="/wiki/OpenSMILE" title="OpenSMILE">OpenSMILE</a><sup id="cite_ref-14" class="reference"><a href="#cite_note-14">&#91;14&#93;</a></sup> and <a href="/wiki/Praat" title="Praat">Praat</a> are popular open-source toolkits for extracting such audio features.<sup id="cite_ref-15" class="reference"><a href="#cite_note-15">&#91;15&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Visual_features">Visual features</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Multimodal_sentiment_analysis&amp;action=edit&amp;section=4" title="Edit section: Visual features">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>One of the main advantages of analyzing videos with respect to texts alone, is the presence of rich sentiment cues in visual data.<sup id="cite_ref-16" class="reference"><a href="#cite_note-16">&#91;16&#93;</a></sup> Visual features include <a href="/wiki/Facial_expression" title="Facial expression">facial expressions</a>, which are of paramount importance in capturing sentiments and <a href="/wiki/Emotion" title="Emotion">emotions</a>, as they are a main channel of forming a person's present state of mind.<sup id="cite_ref-s1_3-4" class="reference"><a href="#cite_note-s1-3">&#91;3&#93;</a></sup> Specifically, <a href="/wiki/Smile" title="Smile">smile</a>, is considered to be one of the most predictive visual cues in multimodal sentiment analysis.<sup id="cite_ref-s2_11-1" class="reference"><a href="#cite_note-s2-11">&#91;11&#93;</a></sup> OpenFace is an open-source facial analysis toolkit available for extracting and understanding such visual features.<sup id="cite_ref-17" class="reference"><a href="#cite_note-17">&#91;17&#93;</a></sup>
</p>
<h2><span class="mw-headline" id="Fusion_techniques">Fusion techniques</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Multimodal_sentiment_analysis&amp;action=edit&amp;section=5" title="Edit section: Fusion techniques">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Unlike the traditional text-based <a href="/wiki/Sentiment_analysis" title="Sentiment analysis">sentiment analysis</a>, multimodal sentiment analysis undergo a fusion process in which data from different modalities (text, audio, or visual) are fused and analyzed together.<sup id="cite_ref-s1_3-5" class="reference"><a href="#cite_note-s1-3">&#91;3&#93;</a></sup> The existing approaches in multimodal sentiment analysis <a href="/wiki/Data_fusion" title="Data fusion">data fusion</a> can be grouped into three main categories: feature-level, decision-level, and hybrid fusion, and the performance of the sentiment classification depends on which type of fusion technique is employed.<sup id="cite_ref-s1_3-6" class="reference"><a href="#cite_note-s1-3">&#91;3&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Feature-level_fusion">Feature-level fusion</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Multimodal_sentiment_analysis&amp;action=edit&amp;section=6" title="Edit section: Feature-level fusion">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Feature-level fusion (sometimes known as early fusion) gathers all the features from each <a href="/wiki/Modality_(human%E2%80%93computer_interaction)" title="Modality (human–computer interaction)">modality</a> (text, audio, or visual) and joins them together into a single feature vector, which is eventually fed into a classification algorithm.<sup id="cite_ref-s3_18-0" class="reference"><a href="#cite_note-s3-18">&#91;18&#93;</a></sup> One of the difficulties in implementing this technique is the integration of the heterogeneous features.<sup id="cite_ref-s1_3-7" class="reference"><a href="#cite_note-s1-3">&#91;3&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Decision-level_fusion">Decision-level fusion</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Multimodal_sentiment_analysis&amp;action=edit&amp;section=7" title="Edit section: Decision-level fusion">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Decision-level fusion (sometimes known as late fusion), feeds data from each modality (text, audio, or visual) independently into its own classification algorithm, and obtains the final sentiment classification results by fusing each result into a single decision vector.<sup id="cite_ref-s3_18-1" class="reference"><a href="#cite_note-s3-18">&#91;18&#93;</a></sup> One of the advantages of this fusion technique is that it eliminates the need to fuse heterogeneous data, and each <a href="/wiki/Modality_(human%E2%80%93computer_interaction)" title="Modality (human–computer interaction)">modality</a> can utilize its most appropriate <a href="/wiki/Classification" title="Classification">classification</a> <a href="/wiki/Algorithm" title="Algorithm">algorithm</a>.<sup id="cite_ref-s1_3-8" class="reference"><a href="#cite_note-s1-3">&#91;3&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Hybrid_fusion">Hybrid fusion</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Multimodal_sentiment_analysis&amp;action=edit&amp;section=8" title="Edit section: Hybrid fusion">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Hybrid fusion is a combination of feature-level and decision-level fusion techniques, which exploits complementary information from both methods during the classification process.<sup id="cite_ref-s4_5-1" class="reference"><a href="#cite_note-s4-5">&#91;5&#93;</a></sup> It usually involves a two-step procedure wherein feature-level fusion is initially performed between two modalities, and decision-level fusion is then applied as a second step, to fuse the initial results from the feature-level fusion, with the remaining <a href="/wiki/Modality_(human%E2%80%93computer_interaction)" title="Modality (human–computer interaction)">modality</a>.<sup id="cite_ref-19" class="reference"><a href="#cite_note-19">&#91;19&#93;</a></sup><sup id="cite_ref-20" class="reference"><a href="#cite_note-20">&#91;20&#93;</a></sup>
</p>
<h2><span class="mw-headline" id="Applications">Applications</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Multimodal_sentiment_analysis&amp;action=edit&amp;section=9" title="Edit section: Applications">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Similar to text-based sentiment analysis, multimodal sentiment analysis can be applied in the development of different forms of <a href="/wiki/Recommender_system" title="Recommender system">recommender systems</a> such as in the analysis of user-generated videos of movie reviews<sup id="cite_ref-s4_5-2" class="reference"><a href="#cite_note-s4-5">&#91;5&#93;</a></sup> and general product reviews,<sup id="cite_ref-21" class="reference"><a href="#cite_note-21">&#91;21&#93;</a></sup> to predict the sentiments of customers, and subsequently create product or service recommendations.<sup id="cite_ref-22" class="reference"><a href="#cite_note-22">&#91;22&#93;</a></sup> Multimodal sentiment analysis also plays an important role in the advancement of <a href="/wiki/Virtual_assistant" title="Virtual assistant">virtual assistants</a> through the application of <a href="/wiki/Natural_language_processing" title="Natural language processing">natural language processing</a> (NLP) and <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a> techniques.<sup id="cite_ref-s5_4-1" class="reference"><a href="#cite_note-s5-4">&#91;4&#93;</a></sup> In the healthcare domain, multimodal sentiment analysis can be utilized to detect certain medical conditions such as <a href="/wiki/Psychological_stress" title="Psychological stress">stress</a>, <a href="/wiki/Anxiety" title="Anxiety">anxiety</a>, or <a href="/wiki/Depression_(mood)" title="Depression (mood)">depression</a>.<sup id="cite_ref-s6_7-1" class="reference"><a href="#cite_note-s6-7">&#91;7&#93;</a></sup> Multimodal sentiment analysis can also be applied in understanding the sentiments contained in video news programs, which is considered as a complicated and challenging domain, as sentiments expressed by reporters tend to be less obvious or neutral.<sup id="cite_ref-23" class="reference"><a href="#cite_note-23">&#91;23&#93;</a></sup>
</p>
<h2><span class="mw-headline" id="References">References</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Multimodal_sentiment_analysis&amp;action=edit&amp;section=10" title="Edit section: References">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div class="reflist" style="list-style-type: decimal;">
<div class="mw-references-wrap mw-references-columns"><ol class="references">
<li id="cite_note-1"><span class="mw-cite-backlink"><b><a href="#cite_ref-1">^</a></b></span> <span class="reference-text"><cite class="citation journal">Soleymani, Mohammad; Garcia, David; Jou, Brendan; Schuller, Björn; Chang, Shih-Fu; Pantic, Maja (September 2017). <a rel="nofollow" class="external text" href="https://zenodo.org/record/3449163">"A survey of multimodal sentiment analysis"</a>. <i>Image and Vision Computing</i>. <b>65</b>: 3–14. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1016%2Fj.imavis.2017.08.003">10.1016/j.imavis.2017.08.003</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Image+and+Vision+Computing&amp;rft.atitle=A+survey+of+multimodal+sentiment+analysis&amp;rft.volume=65&amp;rft.pages=3-14&amp;rft.date=2017-09&amp;rft_id=info%3Adoi%2F10.1016%2Fj.imavis.2017.08.003&amp;rft.aulast=Soleymani&amp;rft.aufirst=Mohammad&amp;rft.au=Garcia%2C+David&amp;rft.au=Jou%2C+Brendan&amp;rft.au=Schuller%2C+Bj%C3%B6rn&amp;rft.au=Chang%2C+Shih-Fu&amp;rft.au=Pantic%2C+Maja&amp;rft_id=https%3A%2F%2Fzenodo.org%2Frecord%2F3449163&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMultimodal+sentiment+analysis" class="Z3988"></span><style data-mw-deduplicate="TemplateStyles:r935243608">.mw-parser-output cite.citation{font-style:inherit}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Lock-gray-alt-2.svg/9px-Lock-gray-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Lock-red-alt-2.svg/9px-Lock-red-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/12px-Wikisource-logo.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:inherit;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-maint{display:none;color:#33aa33;margin-left:0.3em}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}</style></span>
</li>
<li id="cite_note-2"><span class="mw-cite-backlink"><b><a href="#cite_ref-2">^</a></b></span> <span class="reference-text"><cite class="citation journal">Karray, Fakhreddine; Milad, Alemzadeh; Saleh, Jamil Abou; Mo Nours, Arab (2008). <a rel="nofollow" class="external text" href="http://s2is.org/Issues/v1/n1/papers/paper9.pdf">"Human-Computer Interaction: Overview on State of the Art"</a> <span class="cs1-format">(PDF)</span>. <i>International Journal on Smart Sensing and Intelligent Systems</i>. <b>1</b>: 137–159. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.21307%2Fijssis-2017-283">10.21307/ijssis-2017-283</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=International+Journal+on+Smart+Sensing+and+Intelligent+Systems&amp;rft.atitle=Human-Computer+Interaction%3A+Overview+on+State+of+the+Art&amp;rft.volume=1&amp;rft.pages=137-159&amp;rft.date=2008&amp;rft_id=info%3Adoi%2F10.21307%2Fijssis-2017-283&amp;rft.aulast=Karray&amp;rft.aufirst=Fakhreddine&amp;rft.au=Milad%2C+Alemzadeh&amp;rft.au=Saleh%2C+Jamil+Abou&amp;rft.au=Mo+Nours%2C+Arab&amp;rft_id=http%3A%2F%2Fs2is.org%2FIssues%2Fv1%2Fn1%2Fpapers%2Fpaper9.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMultimodal+sentiment+analysis" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-s1-3"><span class="mw-cite-backlink">^ <a href="#cite_ref-s1_3-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-s1_3-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-s1_3-2"><sup><i><b>c</b></i></sup></a> <a href="#cite_ref-s1_3-3"><sup><i><b>d</b></i></sup></a> <a href="#cite_ref-s1_3-4"><sup><i><b>e</b></i></sup></a> <a href="#cite_ref-s1_3-5"><sup><i><b>f</b></i></sup></a> <a href="#cite_ref-s1_3-6"><sup><i><b>g</b></i></sup></a> <a href="#cite_ref-s1_3-7"><sup><i><b>h</b></i></sup></a> <a href="#cite_ref-s1_3-8"><sup><i><b>i</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal">Poria, Soujanya; Cambria, Erik; Bajpai, Rajiv; Hussain, Amir (September 2017). "A review of affective computing: From unimodal analysis to multimodal fusion". <i>Information Fusion</i>. <b>37</b>: 98–125. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1016%2Fj.inffus.2017.02.003">10.1016/j.inffus.2017.02.003</a>. <a href="/wiki/Handle_System" title="Handle System">hdl</a>:<a rel="nofollow" class="external text" href="//hdl.handle.net/1893%2F25490">1893/25490</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Information+Fusion&amp;rft.atitle=A+review+of+affective+computing%3A+From+unimodal+analysis+to+multimodal+fusion&amp;rft.volume=37&amp;rft.pages=98-125&amp;rft.date=2017-09&amp;rft_id=info%3Ahdl%2F1893%2F25490&amp;rft_id=info%3Adoi%2F10.1016%2Fj.inffus.2017.02.003&amp;rft.aulast=Poria&amp;rft.aufirst=Soujanya&amp;rft.au=Cambria%2C+Erik&amp;rft.au=Bajpai%2C+Rajiv&amp;rft.au=Hussain%2C+Amir&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMultimodal+sentiment+analysis" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-s5-4"><span class="mw-cite-backlink">^ <a href="#cite_ref-s5_4-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-s5_4-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation web"><a rel="nofollow" class="external text" href="https://www.bbc.com/news/technology-44045424">"Google AI to make phone calls for you"</a>. <i>BBC News</i>. 8 May 2018<span class="reference-accessdate">. Retrieved <span class="nowrap">12 June</span> 2018</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=BBC+News&amp;rft.atitle=Google+AI+to+make+phone+calls+for+you&amp;rft.date=2018-05-08&amp;rft_id=https%3A%2F%2Fwww.bbc.com%2Fnews%2Ftechnology-44045424&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMultimodal+sentiment+analysis" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-s4-5"><span class="mw-cite-backlink">^ <a href="#cite_ref-s4_5-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-s4_5-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-s4_5-2"><sup><i><b>c</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal">Wollmer, Martin; Weninger, Felix; Knaup, Tobias; Schuller, Bjorn; Sun, Congkai; Sagae, Kenji; Morency, Louis-Philippe (May 2013). "YouTube Movie Reviews: Sentiment Analysis in an Audio-Visual Context". <i>IEEE Intelligent Systems</i>. <b>28</b> (3): 46–53. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1109%2FMIS.2013.34">10.1109/MIS.2013.34</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=IEEE+Intelligent+Systems&amp;rft.atitle=YouTube+Movie+Reviews%3A+Sentiment+Analysis+in+an+Audio-Visual+Context&amp;rft.volume=28&amp;rft.issue=3&amp;rft.pages=46-53&amp;rft.date=2013-05&amp;rft_id=info%3Adoi%2F10.1109%2FMIS.2013.34&amp;rft.aulast=Wollmer&amp;rft.aufirst=Martin&amp;rft.au=Weninger%2C+Felix&amp;rft.au=Knaup%2C+Tobias&amp;rft.au=Schuller%2C+Bjorn&amp;rft.au=Sun%2C+Congkai&amp;rft.au=Sagae%2C+Kenji&amp;rft.au=Morency%2C+Louis-Philippe&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMultimodal+sentiment+analysis" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-6"><span class="mw-cite-backlink"><b><a href="#cite_ref-6">^</a></b></span> <span class="reference-text"><cite class="citation arxiv">Pereira, Moisés H. R.; Pádua, Flávio L. C.; Pereira, Adriano C. M.; Benevenuto, Fabrício; Dalip, Daniel H. (9 April 2016). "Fusing Audio, Textual and Visual Features for Sentiment Analysis of News Videos". <a href="/wiki/ArXiv" title="ArXiv">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//arxiv.org/abs/1604.02612">1604.02612</a></span> [<a rel="nofollow" class="external text" href="//arxiv.org/archive/cs.CL">cs.CL</a>].</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Fusing+Audio%2C+Textual+and+Visual+Features+for+Sentiment+Analysis+of+News+Videos&amp;rft.date=2016-04-09&amp;rft_id=info%3Aarxiv%2F1604.02612&amp;rft.aulast=Pereira&amp;rft.aufirst=Mois%C3%A9s+H.+R.&amp;rft.au=P%C3%A1dua%2C+Fl%C3%A1vio+L.+C.&amp;rft.au=Pereira%2C+Adriano+C.+M.&amp;rft.au=Benevenuto%2C+Fabr%C3%ADcio&amp;rft.au=Dalip%2C+Daniel+H.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMultimodal+sentiment+analysis" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-s6-7"><span class="mw-cite-backlink">^ <a href="#cite_ref-s6_7-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-s6_7-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation book">Zucco, Chiara; Calabrese, Barbara; Cannataro, Mario (November 2017). <i>Sentiment analysis and affective computing for depression monitoring</i>. <i>2017 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)</i>. IEEE. pp.&#160;1988–1995. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1109%2Fbibm.2017.8217966">10.1109/bibm.2017.8217966</a>. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-1-5090-3050-7" title="Special:BookSources/978-1-5090-3050-7"><bdi>978-1-5090-3050-7</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Sentiment+analysis+and+affective+computing+for+depression+monitoring&amp;rft.pages=1988-1995&amp;rft.pub=IEEE&amp;rft.date=2017-11&amp;rft_id=info%3Adoi%2F10.1109%2Fbibm.2017.8217966&amp;rft.isbn=978-1-5090-3050-7&amp;rft.aulast=Zucco&amp;rft.aufirst=Chiara&amp;rft.au=Calabrese%2C+Barbara&amp;rft.au=Cannataro%2C+Mario&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMultimodal+sentiment+analysis" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-8"><span class="mw-cite-backlink"><b><a href="#cite_ref-8">^</a></b></span> <span class="reference-text"><cite class="citation book">Pang, Bo; Lee, Lillian (2008). <i>Opinion mining and sentiment analysis</i>. Hanover, MA: Now Publishers. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-1601981509" title="Special:BookSources/978-1601981509"><bdi>978-1601981509</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Opinion+mining+and+sentiment+analysis&amp;rft.place=Hanover%2C+MA&amp;rft.pub=Now+Publishers&amp;rft.date=2008&amp;rft.isbn=978-1601981509&amp;rft.aulast=Pang&amp;rft.aufirst=Bo&amp;rft.au=Lee%2C+Lillian&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMultimodal+sentiment+analysis" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-s7-9"><span class="mw-cite-backlink">^ <a href="#cite_ref-s7_9-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-s7_9-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal">Sun, Shiliang; Luo, Chen; Chen, Junyu (July 2017). "A review of natural language processing techniques for opinion mining systems". <i>Information Fusion</i>. <b>36</b>: 10–25. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1016%2Fj.inffus.2016.10.004">10.1016/j.inffus.2016.10.004</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Information+Fusion&amp;rft.atitle=A+review+of+natural+language+processing+techniques+for+opinion+mining+systems&amp;rft.volume=36&amp;rft.pages=10-25&amp;rft.date=2017-07&amp;rft_id=info%3Adoi%2F10.1016%2Fj.inffus.2016.10.004&amp;rft.aulast=Sun&amp;rft.aufirst=Shiliang&amp;rft.au=Luo%2C+Chen&amp;rft.au=Chen%2C+Junyu&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMultimodal+sentiment+analysis" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-10"><span class="mw-cite-backlink"><b><a href="#cite_ref-10">^</a></b></span> <span class="reference-text"><cite class="citation journal">Yadollahi, Ali; Shahraki, Ameneh Gholipour; Zaiane, Osmar R. (25 May 2017). "Current State of Text Sentiment Analysis from Opinion to Emotion Mining". <i>ACM Computing Surveys</i>. <b>50</b> (2): 1–33. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1145%2F3057270">10.1145/3057270</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=ACM+Computing+Surveys&amp;rft.atitle=Current+State+of+Text+Sentiment+Analysis+from+Opinion+to+Emotion+Mining&amp;rft.volume=50&amp;rft.issue=2&amp;rft.pages=1-33&amp;rft.date=2017-05-25&amp;rft_id=info%3Adoi%2F10.1145%2F3057270&amp;rft.aulast=Yadollahi&amp;rft.aufirst=Ali&amp;rft.au=Shahraki%2C+Ameneh+Gholipour&amp;rft.au=Zaiane%2C+Osmar+R.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMultimodal+sentiment+analysis" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-s2-11"><span class="mw-cite-backlink">^ <a href="#cite_ref-s2_11-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-s2_11-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal">Perez Rosas, Veronica; Mihalcea, Rada; Morency, Louis-Philippe (May 2013). "Multimodal Sentiment Analysis of Spanish Online Videos". <i>IEEE Intelligent Systems</i>. <b>28</b> (3): 38–45. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1109%2FMIS.2013.9">10.1109/MIS.2013.9</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=IEEE+Intelligent+Systems&amp;rft.atitle=Multimodal+Sentiment+Analysis+of+Spanish+Online+Videos&amp;rft.volume=28&amp;rft.issue=3&amp;rft.pages=38-45&amp;rft.date=2013-05&amp;rft_id=info%3Adoi%2F10.1109%2FMIS.2013.9&amp;rft.aulast=Perez+Rosas&amp;rft.aufirst=Veronica&amp;rft.au=Mihalcea%2C+Rada&amp;rft.au=Morency%2C+Louis-Philippe&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMultimodal+sentiment+analysis" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-12"><span class="mw-cite-backlink"><b><a href="#cite_ref-12">^</a></b></span> <span class="reference-text"><cite class="citation journal">Poria, Soujanya; Cambria, Erik; Hussain, Amir; Huang, Guang-Bin (March 2015). "Towards an intelligent framework for multimodal affective data analysis". <i>Neural Networks</i>. <b>63</b>: 104–116. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1016%2Fj.neunet.2014.10.005">10.1016/j.neunet.2014.10.005</a>. <a href="/wiki/Handle_System" title="Handle System">hdl</a>:<a rel="nofollow" class="external text" href="//hdl.handle.net/1893%2F21310">1893/21310</a>. <a href="/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&#160;<a rel="nofollow" class="external text" href="//pubmed.ncbi.nlm.nih.gov/25523041">25523041</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Neural+Networks&amp;rft.atitle=Towards+an+intelligent+framework+for+multimodal+affective+data+analysis&amp;rft.volume=63&amp;rft.pages=104-116&amp;rft.date=2015-03&amp;rft_id=info%3Ahdl%2F1893%2F21310&amp;rft_id=info%3Apmid%2F25523041&amp;rft_id=info%3Adoi%2F10.1016%2Fj.neunet.2014.10.005&amp;rft.aulast=Poria&amp;rft.aufirst=Soujanya&amp;rft.au=Cambria%2C+Erik&amp;rft.au=Hussain%2C+Amir&amp;rft.au=Huang%2C+Guang-Bin&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMultimodal+sentiment+analysis" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-13"><span class="mw-cite-backlink"><b><a href="#cite_ref-13">^</a></b></span> <span class="reference-text"><cite class="citation journal">Chung-Hsien Wu; Wei-Bin Liang (January 2011). "Emotion Recognition of Affective Speech Based on Multiple Classifiers Using Acoustic-Prosodic Information and Semantic Labels". <i>IEEE Transactions on Affective Computing</i>. <b>2</b> (1): 10–21. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1109%2FT-AFFC.2010.16">10.1109/T-AFFC.2010.16</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=IEEE+Transactions+on+Affective+Computing&amp;rft.atitle=Emotion+Recognition+of+Affective+Speech+Based+on+Multiple+Classifiers+Using+Acoustic-Prosodic+Information+and+Semantic+Labels&amp;rft.volume=2&amp;rft.issue=1&amp;rft.pages=10-21&amp;rft.date=2011-01&amp;rft_id=info%3Adoi%2F10.1109%2FT-AFFC.2010.16&amp;rft.au=Chung-Hsien+Wu&amp;rft.au=Wei-Bin+Liang&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMultimodal+sentiment+analysis" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-14"><span class="mw-cite-backlink"><b><a href="#cite_ref-14">^</a></b></span> <span class="reference-text"><cite class="citation book">Eyben, Florian; Wöllmer, Martin; Schuller, Björn (2009). "OpenEAR — Introducing the munich open-source emotion and affect recognition toolkit". <i>OpenEAR — Introducing the munich open-source emotion and affect recognition toolkit - IEEE Conference Publication</i>. p.&#160;1. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1109%2FACII.2009.5349350">10.1109/ACII.2009.5349350</a>. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-1-4244-4800-5" title="Special:BookSources/978-1-4244-4800-5"><bdi>978-1-4244-4800-5</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=OpenEAR+%E2%80%94+Introducing+the+munich+open-source+emotion+and+affect+recognition+toolkit&amp;rft.btitle=OpenEAR+%E2%80%94+Introducing+the+munich+open-source+emotion+and+affect+recognition+toolkit+-+IEEE+Conference+Publication&amp;rft.pages=1&amp;rft.date=2009&amp;rft_id=info%3Adoi%2F10.1109%2FACII.2009.5349350&amp;rft.isbn=978-1-4244-4800-5&amp;rft.aulast=Eyben&amp;rft.aufirst=Florian&amp;rft.au=W%C3%B6llmer%2C+Martin&amp;rft.au=Schuller%2C+Bj%C3%B6rn&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMultimodal+sentiment+analysis" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-15"><span class="mw-cite-backlink"><b><a href="#cite_ref-15">^</a></b></span> <span class="reference-text"><cite class="citation book">Morency, Louis-Philippe; Mihalcea, Rada; Doshi, Payal (14 November 2011). "Towards multimodal sentiment analysis". <i>Towards multimodal sentiment analysis: harvesting opinions from the web</i>. ACM. pp.&#160;169–176. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1145%2F2070481.2070509">10.1145/2070481.2070509</a>. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/9781450306416" title="Special:BookSources/9781450306416"><bdi>9781450306416</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Towards+multimodal+sentiment+analysis&amp;rft.btitle=Towards+multimodal+sentiment+analysis%3A+harvesting+opinions+from+the+web&amp;rft.pages=169-176&amp;rft.pub=ACM&amp;rft.date=2011-11-14&amp;rft_id=info%3Adoi%2F10.1145%2F2070481.2070509&amp;rft.isbn=9781450306416&amp;rft.aulast=Morency&amp;rft.aufirst=Louis-Philippe&amp;rft.au=Mihalcea%2C+Rada&amp;rft.au=Doshi%2C+Payal&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMultimodal+sentiment+analysis" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-16"><span class="mw-cite-backlink"><b><a href="#cite_ref-16">^</a></b></span> <span class="reference-text"><cite class="citation journal">Poria, Soujanya; Cambria, Erik; Hazarika, Devamanyu; Majumder, Navonil; Zadeh, Amir; Morency, Louis-Philippe (2017). "Context-Dependent Sentiment Analysis in User-Generated Videos". <i>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</i>: 873–883. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.18653%2Fv1%2Fp17-1081">10.18653/v1/p17-1081</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Proceedings+of+the+55th+Annual+Meeting+of+the+Association+for+Computational+Linguistics+%28Volume+1%3A+Long+Papers%29&amp;rft.atitle=Context-Dependent+Sentiment+Analysis+in+User-Generated+Videos&amp;rft.pages=873-883&amp;rft.date=2017&amp;rft_id=info%3Adoi%2F10.18653%2Fv1%2Fp17-1081&amp;rft.aulast=Poria&amp;rft.aufirst=Soujanya&amp;rft.au=Cambria%2C+Erik&amp;rft.au=Hazarika%2C+Devamanyu&amp;rft.au=Majumder%2C+Navonil&amp;rft.au=Zadeh%2C+Amir&amp;rft.au=Morency%2C+Louis-Philippe&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMultimodal+sentiment+analysis" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-17"><span class="mw-cite-backlink"><b><a href="#cite_ref-17">^</a></b></span> <span class="reference-text"><cite class="citation journal">"OpenFace: An open source facial behavior analysis toolkit - IEEE Conference Publication". <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1109%2FWACV.2016.7477553">10.1109/WACV.2016.7477553</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=OpenFace%3A+An+open+source+facial+behavior+analysis+toolkit+-+IEEE+Conference+Publication&amp;rft_id=info%3Adoi%2F10.1109%2FWACV.2016.7477553&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMultimodal+sentiment+analysis" class="Z3988"></span> <span class="cs1-hidden-error error citation-comment">Cite journal requires <code class="cs1-code">&#124;journal=</code> (<a href="/wiki/Help:CS1_errors#missing_periodical" title="Help:CS1 errors">help</a>)</span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-s3-18"><span class="mw-cite-backlink">^ <a href="#cite_ref-s3_18-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-s3_18-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal">Poria, Soujanya; Cambria, Erik; Howard, Newton; Huang, Guang-Bin; Hussain, Amir (January 2016). "Fusing audio, visual and textual clues for sentiment analysis from multimodal content". <i>Neurocomputing</i>. <b>174</b>: 50–59. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1016%2Fj.neucom.2015.01.095">10.1016/j.neucom.2015.01.095</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Neurocomputing&amp;rft.atitle=Fusing+audio%2C+visual+and+textual+clues+for+sentiment+analysis+from+multimodal+content&amp;rft.volume=174&amp;rft.pages=50-59&amp;rft.date=2016-01&amp;rft_id=info%3Adoi%2F10.1016%2Fj.neucom.2015.01.095&amp;rft.aulast=Poria&amp;rft.aufirst=Soujanya&amp;rft.au=Cambria%2C+Erik&amp;rft.au=Howard%2C+Newton&amp;rft.au=Huang%2C+Guang-Bin&amp;rft.au=Hussain%2C+Amir&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMultimodal+sentiment+analysis" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-19"><span class="mw-cite-backlink"><b><a href="#cite_ref-19">^</a></b></span> <span class="reference-text"><cite class="citation journal">Shahla, Shahla; Naghsh-Nilchi, Ahmad Reza (2017). "Exploiting evidential theory in the fusion of textual, audio, and visual modalities for affective music video retrieval - IEEE Conference Publication". <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1109%2FPRIA.2017.7983051">10.1109/PRIA.2017.7983051</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Exploiting+evidential+theory+in+the+fusion+of+textual%2C+audio%2C+and+visual+modalities+for+affective+music+video+retrieval+-+IEEE+Conference+Publication&amp;rft.date=2017&amp;rft_id=info%3Adoi%2F10.1109%2FPRIA.2017.7983051&amp;rft.aulast=Shahla&amp;rft.aufirst=Shahla&amp;rft.au=Naghsh-Nilchi%2C+Ahmad+Reza&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMultimodal+sentiment+analysis" class="Z3988"></span> <span class="cs1-hidden-error error citation-comment">Cite journal requires <code class="cs1-code">&#124;journal=</code> (<a href="/wiki/Help:CS1_errors#missing_periodical" title="Help:CS1 errors">help</a>)</span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-20"><span class="mw-cite-backlink"><b><a href="#cite_ref-20">^</a></b></span> <span class="reference-text"><cite class="citation journal">Poria, Soujanya; Peng, Haiyun; Hussain, Amir; Howard, Newton; Cambria, Erik (October 2017). "Ensemble application of convolutional neural networks and multiple kernel learning for multimodal sentiment analysis". <i>Neurocomputing</i>. <b>261</b>: 217–230. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1016%2Fj.neucom.2016.09.117">10.1016/j.neucom.2016.09.117</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Neurocomputing&amp;rft.atitle=Ensemble+application+of+convolutional+neural+networks+and+multiple+kernel+learning+for+multimodal+sentiment+analysis&amp;rft.volume=261&amp;rft.pages=217-230&amp;rft.date=2017-10&amp;rft_id=info%3Adoi%2F10.1016%2Fj.neucom.2016.09.117&amp;rft.aulast=Poria&amp;rft.aufirst=Soujanya&amp;rft.au=Peng%2C+Haiyun&amp;rft.au=Hussain%2C+Amir&amp;rft.au=Howard%2C+Newton&amp;rft.au=Cambria%2C+Erik&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMultimodal+sentiment+analysis" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-21"><span class="mw-cite-backlink"><b><a href="#cite_ref-21">^</a></b></span> <span class="reference-text"><cite class="citation journal">Pérez-Rosas, Verónica; Mihalcea, Rada; Morency, Louis Philippe (1 January 2013). <a rel="nofollow" class="external text" href="https://experts.umich.edu/en/publications/utterance-level-multimodal-sentiment-analysis">"Utterance-level multimodal sentiment analysis"</a>. <i>Long Papers</i>. Association for Computational Linguistics (ACL).</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Long+Papers&amp;rft.atitle=Utterance-level+multimodal+sentiment+analysis&amp;rft.date=2013-01-01&amp;rft.aulast=P%C3%A9rez-Rosas&amp;rft.aufirst=Ver%C3%B3nica&amp;rft.au=Mihalcea%2C+Rada&amp;rft.au=Morency%2C+Louis+Philippe&amp;rft_id=https%3A%2F%2Fexperts.umich.edu%2Fen%2Fpublications%2Futterance-level-multimodal-sentiment-analysis&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMultimodal+sentiment+analysis" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-22"><span class="mw-cite-backlink"><b><a href="#cite_ref-22">^</a></b></span> <span class="reference-text"><cite class="citation web">Chui, Michael; Manyika, James; Miremadi, Mehdi; Henke, Nicolaus; Chung, Rita; Nel, Pieter; Malhotra, Sankalp. <a rel="nofollow" class="external text" href="https://www.mckinsey.com/mgi/">"Notes from the AI frontier. Insights from hundreds of use cases"</a>. <i>McKinsey &amp; Company</i>. McKinsey &amp; Company<span class="reference-accessdate">. Retrieved <span class="nowrap">13 June</span> 2018</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=McKinsey+%26+Company&amp;rft.atitle=Notes+from+the+AI+frontier.+Insights+from+hundreds+of+use+cases&amp;rft.aulast=Chui&amp;rft.aufirst=Michael&amp;rft.au=Manyika%2C+James&amp;rft.au=Miremadi%2C+Mehdi&amp;rft.au=Henke%2C+Nicolaus&amp;rft.au=Chung%2C+Rita&amp;rft.au=Nel%2C+Pieter&amp;rft.au=Malhotra%2C+Sankalp&amp;rft_id=https%3A%2F%2Fwww.mckinsey.com%2Fmgi%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMultimodal+sentiment+analysis" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-23"><span class="mw-cite-backlink"><b><a href="#cite_ref-23">^</a></b></span> <span class="reference-text"><cite class="citation book">Ellis, Joseph G.; Jou, Brendan; Chang, Shih-Fu (12 November 2014). "Why We Watch the News". <i>Why We Watch the News: A Dataset for Exploring Sentiment in Broadcast Video News</i>. ACM. pp.&#160;104–111. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1145%2F2663204.2663237">10.1145/2663204.2663237</a>. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/9781450328852" title="Special:BookSources/9781450328852"><bdi>9781450328852</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Why+We+Watch+the+News&amp;rft.btitle=Why+We+Watch+the+News%3A+A+Dataset+for+Exploring+Sentiment+in+Broadcast+Video+News&amp;rft.pages=104-111&amp;rft.pub=ACM&amp;rft.date=2014-11-12&amp;rft_id=info%3Adoi%2F10.1145%2F2663204.2663237&amp;rft.isbn=9781450328852&amp;rft.aulast=Ellis&amp;rft.aufirst=Joseph+G.&amp;rft.au=Jou%2C+Brendan&amp;rft.au=Chang%2C+Shih-Fu&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMultimodal+sentiment+analysis" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
</ol></div></div>
<!-- 
NewPP limit report
Parsed by mw1268
Cached time: 20200406011811
Cache expiry: 2592000
Dynamic content: false
Complications: [vary‐revision‐sha1]
CPU time usage: 0.408 seconds
Real time usage: 0.693 seconds
Preprocessor visited node count: 1458/1000000
Post‐expand include size: 47563/2097152 bytes
Template argument size: 608/2097152 bytes
Highest expansion depth: 12/40
Expensive parser function count: 4/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 79040/5000000 bytes
Number of Wikibase entities loaded: 3/400
Lua time usage: 0.268/10.000 seconds
Lua memory usage: 4.34 MB/50 MB
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%  661.789      1 -total
 85.72%  567.318      1 Template:Reflist
 56.98%  377.093     15 Template:Cite_journal
 12.36%   81.819      1 Template:Cite_arxiv
 10.38%   68.705      1 Template:Peacock_term
  7.47%   49.456      1 Template:Fix
  6.52%   43.126      5 Template:Cite_book
  5.00%   33.088      1 Template:Category_handler
  2.66%   17.615      2 Template:Cite_web
  1.70%   11.243      1 Template:Delink
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:57687371-0!canonical and timestamp 20200406011810 and revision id 943074429
 -->
</div><noscript><img src="//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" alt="" title="" width="1" height="1" style="border: none; position: absolute;" /></noscript></div>
		<div class="printfooter">Retrieved from "<a dir="ltr" href="https://en.wikipedia.org/w/index.php?title=Multimodal_sentiment_analysis&amp;oldid=943074429">https://en.wikipedia.org/w/index.php?title=Multimodal_sentiment_analysis&amp;oldid=943074429</a>"</div>
		<div id="catlinks" class="catlinks" data-mw="interface"><div id="mw-normal-catlinks" class="mw-normal-catlinks"><a href="/wiki/Help:Category" title="Help:Category">Categories</a>: <ul><li><a href="/wiki/Category:Natural_language_processing" title="Category:Natural language processing">Natural language processing</a></li><li><a href="/wiki/Category:Affective_computing" title="Category:Affective computing">Affective computing</a></li><li><a href="/wiki/Category:Social_media" title="Category:Social media">Social media</a></li><li><a href="/wiki/Category:Machine_learning" title="Category:Machine learning">Machine learning</a></li></ul></div><div id="mw-hidden-catlinks" class="mw-hidden-catlinks mw-hidden-cats-hidden">Hidden categories: <ul><li><a href="/wiki/Category:CS1_errors:_missing_periodical" title="Category:CS1 errors: missing periodical">CS1 errors: missing periodical</a></li><li><a href="/wiki/Category:All_articles_with_peacock_terms" title="Category:All articles with peacock terms">All articles with peacock terms</a></li><li><a href="/wiki/Category:Articles_with_peacock_terms_from_June_2018" title="Category:Articles with peacock terms from June 2018">Articles with peacock terms from June 2018</a></li></ul></div></div>
		<div class="visualClear"></div>
		
	</div>
</div>
<div id='mw-data-after-content'>
	<div class="read-more-container"></div>
</div>

<div id="mw-navigation">
    <h2>Navigation menu</h2>
    <div id="mw-head">
        
        <div id="p-personal" role="navigation" class="" aria-labelledby="p-personal-label">
        	<h3 id="p-personal-label">Personal tools</h3>
        	<ul >
        		
        		<li id="pt-anonuserpage">Not logged in</li>
        		<li id="pt-anontalk"><a href="/wiki/Special:MyTalk" title="Discussion about edits from this IP address [n]" accesskey="n">Talk</a></li><li id="pt-anoncontribs"><a href="/wiki/Special:MyContributions" title="A list of edits made from this IP address [y]" accesskey="y">Contributions</a></li><li id="pt-createaccount"><a href="/w/index.php?title=Special:CreateAccount&amp;returnto=Multimodal+sentiment+analysis" title="You are encouraged to create an account and log in; however, it is not mandatory">Create account</a></li><li id="pt-login"><a href="/w/index.php?title=Special:UserLogin&amp;returnto=Multimodal+sentiment+analysis" title="You&#039;re encouraged to log in; however, it&#039;s not mandatory. [o]" accesskey="o">Log in</a></li>
        	</ul>
        </div>
        <div id="left-navigation">
            <div id="p-namespaces" role="navigation" class="vectorTabs " aria-labelledby="p-namespaces-label">
            	<h3 id="p-namespaces-label">Namespaces</h3>
            	<ul >
            		<li id="ca-nstab-main" class="selected"><a href="/wiki/Multimodal_sentiment_analysis" title="View the content page [c]" accesskey="c">Article</a></li><li id="ca-talk"><a href="/wiki/Talk:Multimodal_sentiment_analysis" rel="discussion" title="Discuss improvements to the content page [t]" accesskey="t">Talk</a></li>
            	</ul>
            </div>
            <div id="p-variants" role="navigation" class="vectorMenu emptyPortlet" aria-labelledby="p-variants-label">
            	<input type="checkbox" class="vectorMenuCheckbox" aria-labelledby="p-variants-label" />
            	<h3 id="p-variants-label">
            		<span>Variants</span>
            	</h3>
            	<ul class="menu" >
            		
            	</ul>
            </div>
        </div>
        <div id="right-navigation">
            <div id="p-views" role="navigation" class="vectorTabs " aria-labelledby="p-views-label">
            	<h3 id="p-views-label">Views</h3>
            	<ul >
            		<li id="ca-view" class="collapsible selected"><a href="/wiki/Multimodal_sentiment_analysis">Read</a></li><li id="ca-edit" class="collapsible"><a href="/w/index.php?title=Multimodal_sentiment_analysis&amp;action=edit" title="Edit this page [e]" accesskey="e">Edit</a></li><li id="ca-history" class="collapsible"><a href="/w/index.php?title=Multimodal_sentiment_analysis&amp;action=history" title="Past revisions of this page [h]" accesskey="h">View history</a></li>
            	</ul>
            </div>
            <div id="p-cactions" role="navigation" class="vectorMenu emptyPortlet" aria-labelledby="p-cactions-label">
            	<input type="checkbox" class="vectorMenuCheckbox" aria-labelledby="p-cactions-label" />
            	<h3 id="p-cactions-label">
            		<span>More</span>
            	</h3>
            	<ul class="menu" >
            		
            	</ul>
            </div>
            <div id="p-search" role="search">
            	<h3 >
            		<label for="searchInput">Search</label>
            	</h3>
            	<form action="/w/index.php" id="searchform">
            		<div id="simpleSearch">
            			<input type="search" name="search" placeholder="Search Wikipedia" title="Search Wikipedia [f]" accesskey="f" id="searchInput"/>
            			<input type="hidden" value="Special:Search" name="title"/>
            			<input type="submit" name="fulltext" value="Search" title="Search Wikipedia for this text" id="mw-searchButton" class="searchButton mw-fallbackSearchButton"/>
            			<input type="submit" name="go" value="Go" title="Go to a page with this exact name if it exists" id="searchButton" class="searchButton"/>
            		</div>
            	</form>
            </div>
        </div>
    </div>
    
    <div id="mw-panel">
    	<div id="p-logo" role="banner">
    		<a  title="Visit the main page" class="mw-wiki-logo" href="/wiki/Main_Page"></a>
    	</div>
    	<div class="portal" role="navigation" id="p-navigation"  aria-labelledby="p-navigation-label">
    		<h3  id="p-navigation-label">
    			Navigation
    		</h3>
    		<div class="body">
    			<ul><li id="n-mainpage-description"><a href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z">Main page</a></li><li id="n-contents"><a href="/wiki/Wikipedia:Contents" title="Guides to browsing Wikipedia">Contents</a></li><li id="n-featuredcontent"><a href="/wiki/Wikipedia:Featured_content" title="Featured content – the best of Wikipedia">Featured content</a></li><li id="n-currentevents"><a href="/wiki/Portal:Current_events" title="Find background information on current events">Current events</a></li><li id="n-randompage"><a href="/wiki/Special:Random" title="Load a random article [x]" accesskey="x">Random article</a></li><li id="n-sitesupport"><a href="https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&amp;utm_medium=sidebar&amp;utm_campaign=C13_en.wikipedia.org&amp;uselang=en" title="Support us">Donate to Wikipedia</a></li><li id="n-shoplink"><a href="//shop.wikimedia.org" title="Visit the Wikipedia store">Wikipedia store</a></li></ul>
    			
    		</div>
    	</div>
    	
    	<div class="portal" role="navigation" id="p-interaction"  aria-labelledby="p-interaction-label">
    		<h3  id="p-interaction-label">
    			Interaction
    		</h3>
    		<div class="body">
    			<ul><li id="n-help"><a href="/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia">Help</a></li><li id="n-aboutsite"><a href="/wiki/Wikipedia:About" title="Find out about Wikipedia">About Wikipedia</a></li><li id="n-portal"><a href="/wiki/Wikipedia:Community_portal" title="About the project, what you can do, where to find things">Community portal</a></li><li id="n-recentchanges"><a href="/wiki/Special:RecentChanges" title="A list of recent changes in the wiki [r]" accesskey="r">Recent changes</a></li><li id="n-contactpage"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia">Contact page</a></li></ul>
    			
    		</div>
    	</div>
    	
    	<div class="portal" role="navigation" id="p-tb"  aria-labelledby="p-tb-label">
    		<h3  id="p-tb-label">
    			Tools
    		</h3>
    		<div class="body">
    			<ul><li id="t-whatlinkshere"><a href="/wiki/Special:WhatLinksHere/Multimodal_sentiment_analysis" title="List of all English Wikipedia pages containing links to this page [j]" accesskey="j">What links here</a></li><li id="t-recentchangeslinked"><a href="/wiki/Special:RecentChangesLinked/Multimodal_sentiment_analysis" rel="nofollow" title="Recent changes in pages linked from this page [k]" accesskey="k">Related changes</a></li><li id="t-upload"><a href="/wiki/Wikipedia:File_Upload_Wizard" title="Upload files [u]" accesskey="u">Upload file</a></li><li id="t-specialpages"><a href="/wiki/Special:SpecialPages" title="A list of all special pages [q]" accesskey="q">Special pages</a></li><li id="t-permalink"><a href="/w/index.php?title=Multimodal_sentiment_analysis&amp;oldid=943074429" title="Permanent link to this revision of the page">Permanent link</a></li><li id="t-info"><a href="/w/index.php?title=Multimodal_sentiment_analysis&amp;action=info" title="More information about this page">Page information</a></li><li id="t-wikibase"><a href="https://www.wikidata.org/wiki/Special:EntityPage/Q55008106" title="Link to connected data repository item [g]" accesskey="g">Wikidata item</a></li><li id="t-cite"><a href="/w/index.php?title=Special:CiteThisPage&amp;page=Multimodal_sentiment_analysis&amp;id=943074429&amp;wpFormIdentifier=titleform" title="Information on how to cite this page">Cite this page</a></li></ul>
    			
    		</div>
    	</div>
    	
    	<div class="portal" role="navigation" id="p-coll-print_export"  aria-labelledby="p-coll-print_export-label">
    		<h3  id="p-coll-print_export-label">
    			Print/export
    		</h3>
    		<div class="body">
    			<ul><li id="coll-create_a_book"><a href="/w/index.php?title=Special:Book&amp;bookcmd=book_creator&amp;referer=Multimodal+sentiment+analysis">Create a book</a></li><li id="coll-download-as-rl"><a href="/w/index.php?title=Special:ElectronPdf&amp;page=Multimodal+sentiment+analysis&amp;action=show-download-screen">Download as PDF</a></li><li id="t-print"><a href="/w/index.php?title=Multimodal_sentiment_analysis&amp;printable=yes" title="Printable version of this page [p]" accesskey="p">Printable version</a></li></ul>
    			
    		</div>
    	</div>
    	
    	<div class="portal" role="navigation" id="p-lang"  aria-labelledby="p-lang-label">
    		<h3  id="p-lang-label">
    			Languages
    		</h3>
    		<div class="body">
    			<ul><li class="interlanguage-link interwiki-tl"><a href="https://tl.wikipedia.org/wiki/Maramihang_Modalidad_na_Pagsusuri_ng_Damdamin" title="Maramihang Modalidad na Pagsusuri ng Damdamin – Tagalog" lang="tl" hreflang="tl" class="interlanguage-link-target">Tagalog</a></li></ul>
    			<div class="after-portlet after-portlet-lang"><span class="wb-langlinks-edit wb-langlinks-link"><a href="https://www.wikidata.org/wiki/Special:EntityPage/Q55008106#sitelinks-wikipedia" title="Edit interlanguage links" class="wbc-editpage">Edit links</a></span></div>
    		</div>
    	</div>
    	
    </div>
</div>

<div id="footer" role="contentinfo" >
	<ul id="footer-info" class="">
		<li id="footer-info-lastmod"> This page was last edited on 28 February 2020, at 17:41<span class="anonymous-show">&#160;(UTC)</span>.</li>
		<li id="footer-info-copyright">Text is available under the <a rel="license" href="//en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License">Creative Commons Attribution-ShareAlike License</a><a rel="license" href="//creativecommons.org/licenses/by-sa/3.0/" style="display:none;"></a>;
additional terms may apply.  By using this site, you agree to the <a href="//foundation.wikimedia.org/wiki/Terms_of_Use">Terms of Use</a> and <a href="//foundation.wikimedia.org/wiki/Privacy_policy">Privacy Policy</a>. Wikipedia® is a registered trademark of the <a href="//www.wikimediafoundation.org/">Wikimedia Foundation, Inc.</a>, a non-profit organization.</li>
	</ul>
	<ul id="footer-places" class="">
		<li id="footer-places-privacy"><a href="https://foundation.wikimedia.org/wiki/Privacy_policy" class="extiw" title="wmf:Privacy policy">Privacy policy</a></li>
		<li id="footer-places-about"><a href="/wiki/Wikipedia:About" title="Wikipedia:About">About Wikipedia</a></li>
		<li id="footer-places-disclaimer"><a href="/wiki/Wikipedia:General_disclaimer" title="Wikipedia:General disclaimer">Disclaimers</a></li>
		<li id="footer-places-contact"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us">Contact Wikipedia</a></li>
		<li id="footer-places-developers"><a href="https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute">Developers</a></li>
		<li id="footer-places-statslink"><a href="https://stats.wikimedia.org/#/en.wikipedia.org">Statistics</a></li>
		<li id="footer-places-cookiestatement"><a href="https://foundation.wikimedia.org/wiki/Cookie_statement">Cookie statement</a></li>
		<li id="footer-places-mobileview"><a href="//en.m.wikipedia.org/w/index.php?title=Multimodal_sentiment_analysis&amp;mobileaction=toggle_view_mobile" class="noprint stopMobileRedirectToggle">Mobile view</a></li>
	</ul>
	<ul id="footer-icons" class="noprint">
		<li id="footer-copyrightico"><a href="https://wikimediafoundation.org/"><img src="/static/images/wikimedia-button.png" srcset="/static/images/wikimedia-button-1.5x.png 1.5x, /static/images/wikimedia-button-2x.png 2x" width="88" height="31" alt="Wikimedia Foundation"/></a></li>
		<li id="footer-poweredbyico"><a href="https://www.mediawiki.org/"><img src="/static/images/poweredby_mediawiki_88x31.png" alt="Powered by MediaWiki" srcset="/static/images/poweredby_mediawiki_132x47.png 1.5x, /static/images/poweredby_mediawiki_176x62.png 2x" width="88" height="31"/></a></li>
	</ul>
	<div style="clear: both;"></div>
</div>

<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgPageParseReport":{"limitreport":{"cputime":"0.408","walltime":"0.693","ppvisitednodes":{"value":1458,"limit":1000000},"postexpandincludesize":{"value":47563,"limit":2097152},"templateargumentsize":{"value":608,"limit":2097152},"expansiondepth":{"value":12,"limit":40},"expensivefunctioncount":{"value":4,"limit":500},"unstrip-depth":{"value":1,"limit":20},"unstrip-size":{"value":79040,"limit":5000000},"entityaccesscount":{"value":3,"limit":400},"timingprofile":["100.00%  661.789      1 -total"," 85.72%  567.318      1 Template:Reflist"," 56.98%  377.093     15 Template:Cite_journal"," 12.36%   81.819      1 Template:Cite_arxiv"," 10.38%   68.705      1 Template:Peacock_term","  7.47%   49.456      1 Template:Fix","  6.52%   43.126      5 Template:Cite_book","  5.00%   33.088      1 Template:Category_handler","  2.66%   17.615      2 Template:Cite_web","  1.70%   11.243      1 Template:Delink"]},"scribunto":{"limitreport-timeusage":{"value":"0.268","limit":"10.000"},"limitreport-memusage":{"value":4554100,"limit":52428800}},"cachereport":{"origin":"mw1268","timestamp":"20200406011811","ttl":2592000,"transientcontent":false}}});});</script>
<script type="application/ld+json">{"@context":"https:\/\/schema.org","@type":"Article","name":"Multimodal sentiment analysis","url":"https:\/\/en.wikipedia.org\/wiki\/Multimodal_sentiment_analysis","sameAs":"http:\/\/www.wikidata.org\/entity\/Q55008106","mainEntity":"http:\/\/www.wikidata.org\/entity\/Q55008106","author":{"@type":"Organization","name":"Contributors to Wikimedia projects"},"publisher":{"@type":"Organization","name":"Wikimedia Foundation, Inc.","logo":{"@type":"ImageObject","url":"https:\/\/www.wikimedia.org\/static\/images\/wmf-hor-googpub.png"}},"datePublished":"2018-06-15T09:12:31Z","dateModified":"2020-02-28T17:41:41Z"}</script>
<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgBackendResponseTime":97,"wgHostname":"mw1364"});});</script></body></html>
