
<!DOCTYPE html>
<html class="client-nojs" lang="en" dir="ltr">
<head>
<meta charset="UTF-8"/>
<title>AI box - Wikipedia</title>
<script>document.documentElement.className="client-js";RLCONF={"wgBreakFrames":!1,"wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgRequestId":"XpaA8gpAMM8AAQDf2icAAACK","wgCSPNonce":!1,"wgCanonicalNamespace":"","wgCanonicalSpecialPageName":!1,"wgNamespaceNumber":0,"wgPageName":"AI_box","wgTitle":"AI box","wgCurRevisionId":949464398,"wgRevisionId":949464398,"wgArticleId":31641770,"wgIsArticle":!0,"wgIsRedirect":!1,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":["Articles with short description","Singularitarianism","Philosophy of artificial intelligence","Existential risk from artificial general intelligence"],"wgPageContentLanguage":"en","wgPageContentModel":"wikitext","wgRelevantPageName":"AI_box","wgRelevantArticleId":31641770,"wgIsProbablyEditable":!0,
"wgRelevantPageIsProbablyEditable":!0,"wgRestrictionEdit":[],"wgRestrictionMove":[],"wgMediaViewerOnClick":!0,"wgMediaViewerEnabledByDefault":!0,"wgPopupsReferencePreviews":!1,"wgPopupsConflictsWithNavPopupGadget":!1,"wgVisualEditor":{"pageLanguageCode":"en","pageLanguageDir":"ltr","pageVariantFallbacks":"en"},"wgMFDisplayWikibaseDescriptions":{"search":!0,"nearby":!0,"watchlist":!0,"tagline":!1},"wgWMESchemaEditAttemptStepOversample":!1,"wgULSCurrentAutonym":"English","wgNoticeProject":"wikipedia","wgWikibaseItemId":"Q4652026","wgCentralAuthMobileDomain":!1,"wgEditSubmitButtonLabelPublish":!0};RLSTATE={"ext.globalCssJs.user.styles":"ready","site.styles":"ready","noscript":"ready","user.styles":"ready","ext.globalCssJs.user":"ready","user":"ready","user.options":"loading","ext.cite.styles":"ready","jquery.makeCollapsible.styles":"ready","mediawiki.toc.styles":"ready","skins.vector.styles.legacy":"ready","wikibase.client.init":"ready",
"ext.visualEditor.desktopArticleTarget.noscript":"ready","ext.uls.interlanguage":"ready","ext.wikimediaBadges":"ready"};RLPAGEMODULES=["ext.cite.ux-enhancements","site","mediawiki.page.startup","skins.vector.js","mediawiki.page.ready","jquery.makeCollapsible","mediawiki.toc","ext.gadget.ReferenceTooltips","ext.gadget.charinsert","ext.gadget.refToolbar","ext.gadget.extra-toolbar-buttons","ext.gadget.switcher","ext.centralauth.centralautologin","mmv.head","mmv.bootstrap.autostart","ext.popups","ext.visualEditor.desktopArticleTarget.init","ext.visualEditor.targetLoader","ext.eventLogging","ext.wikimediaEvents","ext.navigationTiming","ext.uls.compactlinks","ext.uls.interface","ext.cx.eventlogging.campaigns","ext.quicksurveys.init","ext.centralNotice.geoIP","ext.centralNotice.startUp"];</script>
<script>(RLQ=window.RLQ||[]).push(function(){mw.loader.implement("user.options@1hzgi",function($,jQuery,require,module){/*@nomin*/mw.user.tokens.set({"patrolToken":"+\\","watchToken":"+\\","csrfToken":"+\\"});
});});</script>
<link rel="stylesheet" href="/w/load.php?lang=en&amp;modules=ext.cite.styles%7Cext.uls.interlanguage%7Cext.visualEditor.desktopArticleTarget.noscript%7Cext.wikimediaBadges%7Cjquery.makeCollapsible.styles%7Cmediawiki.toc.styles%7Cskins.vector.styles.legacy%7Cwikibase.client.init&amp;only=styles&amp;skin=vector"/>
<script async="" src="/w/load.php?lang=en&amp;modules=startup&amp;only=scripts&amp;raw=1&amp;skin=vector"></script>
<meta name="ResourceLoaderDynamicStyles" content=""/>
<link rel="stylesheet" href="/w/load.php?lang=en&amp;modules=site.styles&amp;only=styles&amp;skin=vector"/>
<meta name="generator" content="MediaWiki 1.35.0-wmf.27"/>
<meta name="referrer" content="origin"/>
<meta name="referrer" content="origin-when-crossorigin"/>
<meta name="referrer" content="origin-when-cross-origin"/>
<link rel="alternate" type="application/x-wiki" title="Edit this page" href="/w/index.php?title=AI_box&amp;action=edit"/>
<link rel="edit" title="Edit this page" href="/w/index.php?title=AI_box&amp;action=edit"/>
<link rel="apple-touch-icon" href="/static/apple-touch/wikipedia.png"/>
<link rel="shortcut icon" href="/static/favicon/wikipedia.ico"/>
<link rel="search" type="application/opensearchdescription+xml" href="/w/opensearch_desc.php" title="Wikipedia (en)"/>
<link rel="EditURI" type="application/rsd+xml" href="//en.wikipedia.org/w/api.php?action=rsd"/>
<link rel="license" href="//creativecommons.org/licenses/by-sa/3.0/"/>
<link rel="alternate" type="application/atom+xml" title="Wikipedia Atom feed" href="/w/index.php?title=Special:RecentChanges&amp;feed=atom"/>
<link rel="canonical" href="https://en.wikipedia.org/wiki/AI_box"/>
<link rel="dns-prefetch" href="//login.wikimedia.org"/>
<link rel="dns-prefetch" href="//meta.wikimedia.org" />
<!--[if lt IE 9]><script src="/w/resources/lib/html5shiv/html5shiv.js"></script><![endif]-->
</head>
<body class="mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject mw-editable page-AI_box rootpage-AI_box skin-vector action-view">
<div id="mw-page-base" class="noprint"></div>
<div id="mw-head-base" class="noprint"></div>
<div id="content" class="mw-body" role="main">
	<a id="top"></a>
	<div id="siteNotice" class="mw-body-content"><!-- CentralNotice --></div>
	<div class="mw-indicators mw-body-content">
</div>

	<h1 id="firstHeading" class="firstHeading" lang="en">AI box</h1>
	
	<div id="bodyContent" class="mw-body-content">
		<div id="siteSub" class="noprint">From Wikipedia, the free encyclopedia</div>
		<div id="contentSub"></div>
		
		
		<div id="jump-to-nav"></div>
		<a class="mw-jump-link" href="#mw-head">Jump to navigation</a>
		<a class="mw-jump-link" href="#p-search">Jump to search</a>
		<div id="mw-content-text" lang="en" dir="ltr" class="mw-content-ltr"><div class="mw-parser-output"><div class="shortdescription nomobile noexcerpt noprint searchaux" style="display:none">Hypothetical isolated computer system</div>
<p>An <b>AI box</b> is a hypothetical isolated computer hardware system where a possibly dangerous <a href="/wiki/Artificial_intelligence" title="Artificial intelligence">artificial intelligence</a>, or AI, is kept constrained in a "virtual prison" and not allowed to directly manipulate events in the external world. Such a box would be restricted to minimalist communication channels. Unfortunately, even if the box is well-designed, a sufficiently intelligent AI may nevertheless be able to persuade or trick its human keepers into releasing it, or otherwise be able to "hack" its way out of the box.<sup id="cite_ref-chalmers_1-0" class="reference"><a href="#cite_note-chalmers-1">&#91;1&#93;</a></sup>
</p>
<div id="toc" class="toc" role="navigation" aria-labelledby="mw-toc-heading"><input type="checkbox" role="button" id="toctogglecheckbox" class="toctogglecheckbox" style="display:none" /><div class="toctitle" lang="en" dir="ltr"><h2 id="mw-toc-heading">Contents</h2><span class="toctogglespan"><label class="toctogglelabel" for="toctogglecheckbox"></label></span></div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="#Motivation"><span class="tocnumber">1</span> <span class="toctext">Motivation</span></a></li>
<li class="toclevel-1 tocsection-2"><a href="#Avenues_of_escape"><span class="tocnumber">2</span> <span class="toctext">Avenues of escape</span></a>
<ul>
<li class="toclevel-2 tocsection-3"><a href="#Physical"><span class="tocnumber">2.1</span> <span class="toctext">Physical</span></a></li>
<li class="toclevel-2 tocsection-4"><a href="#Social_engineering"><span class="tocnumber">2.2</span> <span class="toctext">Social engineering</span></a>
<ul>
<li class="toclevel-3 tocsection-5"><a href="#AI-box_experiment"><span class="tocnumber">2.2.1</span> <span class="toctext">AI-box experiment</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toclevel-1 tocsection-6"><a href="#Overall_limitations"><span class="tocnumber">3</span> <span class="toctext">Overall limitations</span></a></li>
<li class="toclevel-1 tocsection-7"><a href="#In_fiction"><span class="tocnumber">4</span> <span class="toctext">In fiction</span></a></li>
<li class="toclevel-1 tocsection-8"><a href="#References"><span class="tocnumber">5</span> <span class="toctext">References</span></a></li>
<li class="toclevel-1 tocsection-9"><a href="#External_links"><span class="tocnumber">6</span> <span class="toctext">External links</span></a></li>
</ul>
</div>

<h2><span class="mw-headline" id="Motivation">Motivation</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=AI_box&amp;action=edit&amp;section=1" title="Edit section: Motivation">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div role="note" class="hatnote navigation-not-searchable">Main article: <a href="/wiki/Existential_risk_from_artificial_general_intelligence" title="Existential risk from artificial general intelligence">Existential risk from artificial general intelligence</a></div>
<p>Some hypothetical intelligence technologies, like "seed AI", are postulated such as to have the potential to make themselves faster and more intelligent, by modifying their source code. These improvements would make further improvements possible, which would in turn make further improvements possible, and so on, leading to a sudden <a href="/wiki/Intelligence_explosion" class="mw-redirect" title="Intelligence explosion">intelligence explosion</a>.<sup id="cite_ref-2" class="reference"><a href="#cite_note-2">&#91;2&#93;</a></sup> Following such an intelligence explosion, an unrestricted <a href="/wiki/Superintelligent" class="mw-redirect" title="Superintelligent">superintelligent</a> AI could, if its goals differed from humanity's, take actions resulting in <a href="/wiki/Human_extinction" title="Human extinction">human extinction</a>.<sup id="cite_ref-3" class="reference"><a href="#cite_note-3">&#91;3&#93;</a></sup> For example, imagining an extremely advanced computer of this sort, given the sole purpose of solving the <a href="/wiki/Riemann_hypothesis" title="Riemann hypothesis">Riemann hypothesis</a>, an innocuous mathematical conjecture, could decide to try to convert the planet into a giant supercomputer whose sole purpose is to make additional mathematical calculations (see also <a href="/wiki/Instrumental_convergence#Paperclip_maximizer" title="Instrumental convergence">paperclip maximizer</a>).<sup id="cite_ref-4" class="reference"><a href="#cite_note-4">&#91;4&#93;</a></sup> The purpose of an AI box would be to reduce the risk of the AI taking control of the environment away from its operators, while still allowing the AI to calculate and give its operators solutions to narrow technical problems.<sup id="cite_ref-5" class="reference"><a href="#cite_note-5">&#91;5&#93;</a></sup>
</p>
<h2><span class="mw-headline" id="Avenues_of_escape">Avenues of escape</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=AI_box&amp;action=edit&amp;section=2" title="Edit section: Avenues of escape">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<h3><span class="mw-headline" id="Physical">Physical</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=AI_box&amp;action=edit&amp;section=3" title="Edit section: Physical">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Such a superintelligent AI with access to the Internet could hack into other computer systems and copy itself like a computer virus. Less obviously, even if the AI only had access to its own computer operating system, it could attempt to send hidden Morse code messages to a human sympathizer by manipulating its cooling fans. Professor <a href="/wiki/Roman_Yampolskiy" title="Roman Yampolskiy">Roman Yampolskiy</a> takes inspiration from the field of computer security and proposes that a boxed AI could, like a potential virus, be run inside a "virtual machine" that limits access to its own networking and operating system hardware.<sup id="cite_ref-nbc_6-0" class="reference"><a href="#cite_note-nbc-6">&#91;6&#93;</a></sup> An additional safeguard, completely unnecessary for potential viruses but possibly useful for a superintelligent AI, would be to place the computer in a <a href="/wiki/Faraday_cage" title="Faraday cage">Faraday cage</a>; otherwise it might be able to transmit radio signals to local radio receivers by shuffling the electrons in its internal circuits in appropriate patterns. The main disadvantage of implementing physical containment is that it reduces the functionality of the AI.<sup id="cite_ref-superintelligence_7-0" class="reference"><a href="#cite_note-superintelligence-7">&#91;7&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Social_engineering">Social engineering</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=AI_box&amp;action=edit&amp;section=4" title="Edit section: Social engineering">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Even casual conversation with the computer's operators, or with a human guard, could allow such a superintelligent AI to deploy psychological tricks, ranging from befriending to blackmail, to convince a human gatekeeper, truthfully or deceitfully, that it is in the gatekeeper's interest to agree to allow the AI greater access to the outside world. The AI might offer a gatekeeper a recipe for perfect health, immortality, or whatever the gatekeeper is believed to most desire; on the other side of the coin, the AI could threaten that it will do horrific things to the gatekeeper and his family once it inevitably escapes. One strategy to attempt to box the AI would be to allow the AI to respond to narrow multiple-choice questions whose answers would benefit human science or medicine, but otherwise bar all other communication with or observation of the AI.<sup id="cite_ref-nbc_6-1" class="reference"><a href="#cite_note-nbc-6">&#91;6&#93;</a></sup> A more lenient "informational containment" strategy would restrict the AI to a low-bandwidth text-only interface, which would at least prevent emotive imagery or some kind of hypothetical "hypnotic pattern". Note that on a technical level, no system can be completely isolated and still remain useful: even if the operators refrain from allowing the AI to communicate and instead merely run the AI for the purpose of observing its inner dynamics, the AI could strategically alter its dynamics to influence the observers. For example, the AI could choose to creatively malfunction in a way that increases the probability that its operators will become lulled into a false sense of security and choose to reboot and then de-isolate the system.<sup id="cite_ref-superintelligence_7-1" class="reference"><a href="#cite_note-superintelligence-7">&#91;7&#93;</a></sup>
</p>
<h4><span class="mw-headline" id="AI-box_experiment">AI-box experiment</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=AI_box&amp;action=edit&amp;section=5" title="Edit section: AI-box experiment">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>The AI-box experiment is an informal experiment devised by <a href="/wiki/Eliezer_Yudkowsky" title="Eliezer Yudkowsky">Eliezer Yudkowsky</a> to attempt to demonstrate that a suitably advanced artificial intelligence can either convince, or perhaps even trick or coerce, a human being into voluntarily "releasing" it, using only text-based communication.  This is one of the points in Yudkowsky's work aimed at creating a friendly artificial intelligence that when "released" would not destroy the human race intentionally or unintentionally.
</p><p>The AI box experiment involves simulating a communication between an AI and a human being to see if the AI can be "released". As an actual super-intelligent AI has not yet been developed, it is substituted by a human. The other person in the experiment plays the "Gatekeeper", the person with the ability to "release" the AI. They communicate through a text interface/<a href="/wiki/Computer_terminal" title="Computer terminal">computer terminal</a> only, and the experiment ends when either the Gatekeeper releases the AI, or the allotted time of two hours ends.<sup id="cite_ref-:0_8-0" class="reference"><a href="#cite_note-:0-8">&#91;8&#93;</a></sup>
</p><p>Yudkowsky says that, despite being of human rather than superhuman intelligence, he was on two occasions able to convince the Gatekeeper, purely through argumentation, to let him out of the box.<sup id="cite_ref-9" class="reference"><a href="#cite_note-9">&#91;9&#93;</a></sup> Due to the rules of the experiment,<sup id="cite_ref-:0_8-1" class="reference"><a href="#cite_note-:0-8">&#91;8&#93;</a></sup> he did not reveal the transcript or his successful AI coercion tactics. Yudkowsky later said that he had tried it against three others and lost twice.<sup id="cite_ref-10" class="reference"><a href="#cite_note-10">&#91;10&#93;</a></sup>
</p>
<h2><span class="mw-headline" id="Overall_limitations">Overall limitations</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=AI_box&amp;action=edit&amp;section=6" title="Edit section: Overall limitations">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Boxing such a hypothetical AI could be supplemented with other methods of shaping the AI's capabilities, such as providing incentives to the AI, stunting the AI's growth, or implementing "tripwires" that automatically shut the AI off if a transgression attempt is somehow detected. However, the more intelligent a system grows, the more likely the system would be able to escape even the best-designed capability control methods.<sup id="cite_ref-11" class="reference"><a href="#cite_note-11">&#91;11&#93;</a></sup><sup id="cite_ref-12" class="reference"><a href="#cite_note-12">&#91;12&#93;</a></sup> In order to solve the overall "control problem" for a superintelligent AI and avoid existential risk, boxing would at best be an adjunct to "motivation selection" methods that seek to ensure the superintelligent AI's goals are compatible with human survival.<sup id="cite_ref-superintelligence_7-2" class="reference"><a href="#cite_note-superintelligence-7">&#91;7&#93;</a></sup><sup id="cite_ref-chalmers_1-1" class="reference"><a href="#cite_note-chalmers-1">&#91;1&#93;</a></sup>
</p><p>All physical boxing proposals are naturally dependent on our understanding of the laws of physics; if a superintelligence could infer and somehow exploit additional physical laws that we are currently unaware of, there is no way to conceive of a foolproof plan to contain it. More broadly, unlike with conventional computer security, attempting to box a superintelligent AI would be intrinsically risky as there could be no sure knowledge that the boxing plan will work. Scientific progress on boxing would be fundamentally difficult because there would be no way to test boxing hypotheses against a dangerous superintelligence until such an entity exists, by which point the consequences of a test failure would be catastrophic.<sup id="cite_ref-nbc_6-2" class="reference"><a href="#cite_note-nbc-6">&#91;6&#93;</a></sup>
</p>
<h2><span class="mw-headline" id="In_fiction">In fiction</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=AI_box&amp;action=edit&amp;section=7" title="Edit section: In fiction">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>The 2015 movie <i><a href="/wiki/Ex_Machina_(film)" title="Ex Machina (film)">Ex Machina</a></i> features an AI with a female humanoid body engaged in a social experiment with a male human in a confined building acting as a physical "AI box". Despite being watched by the experiment's organizer, the AI manages to escape by manipulating its human partner to help it, leaving him stranded inside.<sup id="cite_ref-13" class="reference"><a href="#cite_note-13">&#91;13&#93;</a></sup><sup id="cite_ref-14" class="reference"><a href="#cite_note-14">&#91;14&#93;</a></sup>
</p>
<h2><span class="mw-headline" id="References">References</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=AI_box&amp;action=edit&amp;section=8" title="Edit section: References">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div class="reflist" style="list-style-type: decimal;">
<div class="mw-references-wrap mw-references-columns"><ol class="references">
<li id="cite_note-chalmers-1"><span class="mw-cite-backlink">^ <a href="#cite_ref-chalmers_1-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-chalmers_1-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text">Chalmers, David. "The singularity: A philosophical analysis." <a href="/wiki/Journal_of_Consciousness_Studies" title="Journal of Consciousness Studies">Journal of Consciousness Studies</a> 17.9-10 (2010): 7-65.</span>
</li>
<li id="cite_note-2"><span class="mw-cite-backlink"><b><a href="#cite_ref-2">^</a></b></span> <span class="reference-text">I.J. Good, "Speculations Concerning the First Ultraintelligent Machine"], <i>Advances in Computers</i>, vol. 6, 1965.</span>
</li>
<li id="cite_note-3"><span class="mw-cite-backlink"><b><a href="#cite_ref-3">^</a></b></span> <span class="reference-text"><a href="/wiki/Vincent_C._M%C3%BCller" title="Vincent C. Müller">Vincent C. Müller</a> and <a href="/wiki/Nick_Bostrom" title="Nick Bostrom">Nick Bostrom</a>. "Future progress in artificial intelligence: A survey of expert opinion" in Fundamental Issues of Artificial Intelligence. Springer 553-571 (2016).</span>
</li>
<li id="cite_note-4"><span class="mw-cite-backlink"><b><a href="#cite_ref-4">^</a></b></span> <span class="reference-text"><cite class="citation book">Russell, Stuart J.; Norvig, Peter (2003). "Section 26.3: The Ethics and Risks of Developing Artificial Intelligence". <a href="/wiki/Artificial_Intelligence:_A_Modern_Approach" title="Artificial Intelligence: A Modern Approach"><i>Artificial Intelligence: A Modern Approach</i></a>. Upper Saddle River, N.J.: Prentice Hall. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-0137903955" title="Special:BookSources/978-0137903955"><bdi>978-0137903955</bdi></a>. <q>Similarly, Marvin Minsky once suggested that an AI program designed to solve the Riemann Hypothesis might end up taking over all the resources of Earth to build more powerful supercomputers to help achieve its goal.</q></cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Section+26.3%3A+The+Ethics+and+Risks+of+Developing+Artificial+Intelligence&amp;rft.btitle=Artificial+Intelligence%3A+A+Modern+Approach&amp;rft.place=Upper+Saddle+River%2C+N.J.&amp;rft.pub=Prentice+Hall&amp;rft.date=2003&amp;rft.isbn=978-0137903955&amp;rft.aulast=Russell&amp;rft.aufirst=Stuart+J.&amp;rft.au=Norvig%2C+Peter&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+box" class="Z3988"></span><style data-mw-deduplicate="TemplateStyles:r935243608">.mw-parser-output cite.citation{font-style:inherit}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Lock-gray-alt-2.svg/9px-Lock-gray-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Lock-red-alt-2.svg/9px-Lock-red-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/12px-Wikisource-logo.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:inherit;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-maint{display:none;color:#33aa33;margin-left:0.3em}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}</style></span>
</li>
<li id="cite_note-5"><span class="mw-cite-backlink"><b><a href="#cite_ref-5">^</a></b></span> <span class="reference-text">Yampolskiy, Roman V. "What to Do with the Singularity Paradox?" Philosophy and Theory of Artificial Intelligence 5 (2012): 397.</span>
</li>
<li id="cite_note-nbc-6"><span class="mw-cite-backlink">^ <a href="#cite_ref-nbc_6-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-nbc_6-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-nbc_6-2"><sup><i><b>c</b></i></sup></a></span> <span class="reference-text"><cite class="citation news">Hsu, Jeremy (1 March 2012). <a rel="nofollow" class="external text" href="http://www.nbcnews.com/id/46590591/ns/technology_and_science-innovation">"Control dangerous AI before it controls us, one expert says"</a>. <i><a href="/wiki/NBC_News" title="NBC News">NBC News</a></i><span class="reference-accessdate">. Retrieved <span class="nowrap">29 January</span> 2016</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=NBC+News&amp;rft.atitle=Control+dangerous+AI+before+it+controls+us%2C+one+expert+says&amp;rft.date=2012-03-01&amp;rft.aulast=Hsu&amp;rft.aufirst=Jeremy&amp;rft_id=http%3A%2F%2Fwww.nbcnews.com%2Fid%2F46590591%2Fns%2Ftechnology_and_science-innovation&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+box" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-superintelligence-7"><span class="mw-cite-backlink">^ <a href="#cite_ref-superintelligence_7-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-superintelligence_7-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-superintelligence_7-2"><sup><i><b>c</b></i></sup></a></span> <span class="reference-text"><cite class="citation book">Bostrom, Nick (2013). "Chapter 9: The Control Problem: boxing methods". <i>Superintelligence: the coming machine intelligence revolution</i>. Oxford: Oxford University Press. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/9780199678112" title="Special:BookSources/9780199678112"><bdi>9780199678112</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Chapter+9%3A+The+Control+Problem%3A+boxing+methods&amp;rft.btitle=Superintelligence%3A+the+coming+machine+intelligence+revolution&amp;rft.place=Oxford&amp;rft.pub=Oxford+University+Press&amp;rft.date=2013&amp;rft.isbn=9780199678112&amp;rft.aulast=Bostrom&amp;rft.aufirst=Nick&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+box" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-:0-8"><span class="mw-cite-backlink">^ <a href="#cite_ref-:0_8-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:0_8-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><a rel="nofollow" class="external text" href="http://yudkowsky.net/singularity/aibox">The AI-Box Experiment</a> by <a href="/wiki/Eliezer_Yudkowsky" title="Eliezer Yudkowsky">Eliezer Yudkowsky</a></span>
</li>
<li id="cite_note-9"><span class="mw-cite-backlink"><b><a href="#cite_ref-9">^</a></b></span> <span class="reference-text"><cite class="citation journal">Armstrong, Stuart; Sandberg, Anders; Bostrom, Nick (6 June 2012). "Thinking Inside the Box: Controlling and Using an Oracle AI". <i>Minds and Machines</i>. <b>22</b> (4): 299–324. <a href="/wiki/CiteSeerX" title="CiteSeerX">CiteSeerX</a>&#160;<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.396.799">10.1.1.396.799</a></span>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1007%2Fs11023-012-9282-2">10.1007/s11023-012-9282-2</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Minds+and+Machines&amp;rft.atitle=Thinking+Inside+the+Box%3A+Controlling+and+Using+an+Oracle+AI&amp;rft.volume=22&amp;rft.issue=4&amp;rft.pages=299-324&amp;rft.date=2012-06-06&amp;rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.396.799&amp;rft_id=info%3Adoi%2F10.1007%2Fs11023-012-9282-2&amp;rft.aulast=Armstrong&amp;rft.aufirst=Stuart&amp;rft.au=Sandberg%2C+Anders&amp;rft.au=Bostrom%2C+Nick&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+box" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-10"><span class="mw-cite-backlink"><b><a href="#cite_ref-10">^</a></b></span> <span class="reference-text"><cite class="citation web">Yudkowsky, Eliezer (8 October 2008). <a rel="nofollow" class="external text" href="http://lesswrong.com/lw/up/shut_up_and_do_the_impossible/">"Shut up and do the impossible!"</a><span class="reference-accessdate">. Retrieved <span class="nowrap">11 August</span> 2015</span>. <q>There were three more AI-Box experiments besides the ones described on the linked page, which I never got around to adding in. ... So, after investigating to make sure they could afford to lose it, I played another three AI-Box experiments. I won the first, and then lost the next two. And then I called a halt to it.</q></cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Shut+up+and+do+the+impossible%21&amp;rft.date=2008-10-08&amp;rft.aulast=Yudkowsky&amp;rft.aufirst=Eliezer&amp;rft_id=http%3A%2F%2Flesswrong.com%2Flw%2Fup%2Fshut_up_and_do_the_impossible%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+box" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-11"><span class="mw-cite-backlink"><b><a href="#cite_ref-11">^</a></b></span> <span class="reference-text"><cite class="citation journal">Vinge, Vernor (1993). "The coming technological singularity: How to survive in the post-human era". <i>Vision-21: Interdisciplinary Science and Engineering in the Era of Cyberspace</i>: 11–22. <q>I argue that confinement is intrinsically impractical. For the case of physical confinement: Imagine yourself confined to your house with only limited data access to the outside, to your masters. If those masters thought at a rate -- say -- one million times slower than you, there is little doubt that over a period of years (your time) you could come up with 'helpful advice' that would incidentally set you free.</q></cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Vision-21%3A+Interdisciplinary+Science+and+Engineering+in+the+Era+of+Cyberspace&amp;rft.atitle=The+coming+technological+singularity%3A+How+to+survive+in+the+post-human+era&amp;rft.pages=11-22&amp;rft.date=1993&amp;rft.aulast=Vinge&amp;rft.aufirst=Vernor&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+box" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-12"><span class="mw-cite-backlink"><b><a href="#cite_ref-12">^</a></b></span> <span class="reference-text"><cite class="citation journal">Yampolskiy, Roman (2012). "Leakproofing the Singularity Artificial Intelligence Confinement Problem". <i><a href="/wiki/Journal_of_Consciousness_Studies" title="Journal of Consciousness Studies">Journal of Consciousness Studies</a></i>: 194–214.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Consciousness+Studies&amp;rft.atitle=Leakproofing+the+Singularity+Artificial+Intelligence+Confinement+Problem&amp;rft.pages=194-214&amp;rft.date=2012&amp;rft.aulast=Yampolskiy&amp;rft.aufirst=Roman&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+box" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-13"><span class="mw-cite-backlink"><b><a href="#cite_ref-13">^</a></b></span> <span class="reference-text"><cite class="citation news">Robbins, Martin (26 January 2016). <a rel="nofollow" class="external text" href="https://www.theguardian.com/science/the-lay-scientist/2016/jan/26/artificial-intelligence-gods-egos-and-ex-machina">"Artificial Intelligence: Gods, egos and Ex Machina"</a>. <i>the Guardian</i><span class="reference-accessdate">. Retrieved <span class="nowrap">9 April</span> 2018</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=the+Guardian&amp;rft.atitle=Artificial+Intelligence%3A+Gods%2C+egos+and+Ex+Machina&amp;rft.date=2016-01-26&amp;rft.aulast=Robbins&amp;rft.aufirst=Martin&amp;rft_id=https%3A%2F%2Fwww.theguardian.com%2Fscience%2Fthe-lay-scientist%2F2016%2Fjan%2F26%2Fartificial-intelligence-gods-egos-and-ex-machina&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+box" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-14"><span class="mw-cite-backlink"><b><a href="#cite_ref-14">^</a></b></span> <span class="reference-text"><cite class="citation news">Achenbach, Joel (30 December 2015). <a rel="nofollow" class="external text" href="https://www.washingtonpost.com/news/achenblog/wp/2015/12/30/ex-machina-and-the-paper-clips-of-doom/">"<span class="cs1-kern-left">"</span>Ex Machina" and the paper clips of doom"</a>. <i>Washington Post</i><span class="reference-accessdate">. Retrieved <span class="nowrap">9 April</span> 2018</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Washington+Post&amp;rft.atitle=%22Ex+Machina%22+and+the+paper+clips+of+doom&amp;rft.date=2015-12-30&amp;rft.aulast=Achenbach&amp;rft.aufirst=Joel&amp;rft_id=https%3A%2F%2Fwww.washingtonpost.com%2Fnews%2Fachenblog%2Fwp%2F2015%2F12%2F30%2Fex-machina-and-the-paper-clips-of-doom%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+box" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
</ol></div></div>
<h2><span class="mw-headline" id="External_links">External links</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=AI_box&amp;action=edit&amp;section=9" title="Edit section: External links">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<ul><li><a rel="nofollow" class="external text" href="http://yudkowsky.net/singularity/aibox">Eliezer Yudkowsky's description of his AI-box experiment</a>, including experimental protocols and suggestions for replication</li>
<li><a rel="nofollow" class="external text" href="https://www.youtube.com/watch?v=oAHIa651Wa0"><span class="plainlinks">"Presentation titled 'Thinking inside the box: using and controlling an Oracle AI'"</span></a> on <a href="/wiki/YouTube" title="YouTube">YouTube</a></li></ul>
<div role="navigation" class="navbox" aria-labelledby="Existential_risk_from_artificial_intelligence" style="padding:3px"><table class="nowraplinks mw-collapsible autocollapse navbox-inner" style="border-spacing:0;background:transparent;color:inherit"><tbody><tr><th scope="col" class="navbox-title" colspan="2"><div class="plainlinks hlist navbar mini"><ul><li class="nv-view"><a href="/wiki/Template:Existential_risk_from_artificial_intelligence" title="Template:Existential risk from artificial intelligence"><abbr title="View this template" style=";;background:none transparent;border:none;-moz-box-shadow:none;-webkit-box-shadow:none;box-shadow:none; padding:0;">v</abbr></a></li><li class="nv-talk"><a href="/wiki/Template_talk:Existential_risk_from_artificial_intelligence" title="Template talk:Existential risk from artificial intelligence"><abbr title="Discuss this template" style=";;background:none transparent;border:none;-moz-box-shadow:none;-webkit-box-shadow:none;box-shadow:none; padding:0;">t</abbr></a></li><li class="nv-edit"><a class="external text" href="https://en.wikipedia.org/w/index.php?title=Template:Existential_risk_from_artificial_intelligence&amp;action=edit"><abbr title="Edit this template" style=";;background:none transparent;border:none;-moz-box-shadow:none;-webkit-box-shadow:none;box-shadow:none; padding:0;">e</abbr></a></li></ul></div><div id="Existential_risk_from_artificial_intelligence" style="font-size:114%;margin:0 4em"><a href="/wiki/Existential_risk_from_artificial_general_intelligence" title="Existential risk from artificial general intelligence">Existential risk</a> from <a href="/wiki/Artificial_intelligence" title="Artificial intelligence">artificial intelligence</a></div></th></tr><tr><th scope="row" class="navbox-group" style="width:1%">Concepts</th><td class="navbox-list navbox-odd hlist" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/Accelerating_change" title="Accelerating change">Accelerating change</a></li>
<li><a class="mw-selflink selflink">AI box</a></li>
<li><a href="/wiki/AI_takeover" title="AI takeover">AI takeover</a></li>
<li><a href="/wiki/AI_control_problem" title="AI control problem">Control problem</a></li>
<li><a href="/wiki/Existential_risk_from_artificial_general_intelligence" title="Existential risk from artificial general intelligence">Existential risk from artificial general intelligence</a></li>
<li><a href="/wiki/Friendly_artificial_intelligence" title="Friendly artificial intelligence">Friendly artificial intelligence</a></li>
<li><a href="/wiki/Instrumental_convergence" title="Instrumental convergence">Instrumental convergence</a></li>
<li><a href="/wiki/Intelligence_explosion" class="mw-redirect" title="Intelligence explosion">Intelligence explosion</a></li>
<li><a href="/wiki/Machine_ethics" title="Machine ethics">Machine ethics</a></li>
<li><a href="/wiki/Superintelligence" title="Superintelligence">Superintelligence</a></li>
<li><a href="/wiki/Technological_singularity" title="Technological singularity">Technological singularity</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Organizations</th><td class="navbox-list navbox-even hlist" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/Allen_Institute_for_AI" title="Allen Institute for AI">Allen Institute for AI</a></li>
<li><a href="/wiki/Center_for_Applied_Rationality" title="Center for Applied Rationality">Center for Applied Rationality</a></li>
<li><a href="/wiki/Center_for_Human-Compatible_Artificial_Intelligence" title="Center for Human-Compatible Artificial Intelligence">Center for Human-Compatible Artificial Intelligence</a></li>
<li><a href="/wiki/Center_for_Security_and_Emerging_Technology" title="Center for Security and Emerging Technology">Center for Security and Emerging Technology</a></li>
<li><a href="/wiki/Centre_for_the_Study_of_Existential_Risk" title="Centre for the Study of Existential Risk">Centre for the Study of Existential Risk</a></li>
<li><a href="/wiki/DeepMind" title="DeepMind">DeepMind</a></li>
<li><a href="/wiki/Foundational_Questions_Institute" title="Foundational Questions Institute">Foundational Questions Institute</a></li>
<li><a href="/wiki/Future_of_Humanity_Institute" title="Future of Humanity Institute">Future of Humanity Institute</a></li>
<li><a href="/wiki/Future_of_Life_Institute" title="Future of Life Institute">Future of Life Institute</a></li>
<li><a href="/wiki/Humanity%2B" title="Humanity+">Humanity+</a></li>
<li><a href="/wiki/Institute_for_Ethics_and_Emerging_Technologies" title="Institute for Ethics and Emerging Technologies">Institute for Ethics and Emerging Technologies</a></li>
<li><a href="/wiki/Leverhulme_Centre_for_the_Future_of_Intelligence" title="Leverhulme Centre for the Future of Intelligence">Leverhulme Centre for the Future of Intelligence</a></li>
<li><a href="/wiki/Machine_Intelligence_Research_Institute" title="Machine Intelligence Research Institute">Machine Intelligence Research Institute</a></li>
<li><a href="/wiki/OpenAI" title="OpenAI">OpenAI</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">People</th><td class="navbox-list navbox-odd hlist" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/Nick_Bostrom" title="Nick Bostrom">Nick Bostrom</a></li>
<li><a href="/wiki/K._Eric_Drexler" title="K. Eric Drexler">Eric Drexler</a></li>
<li><a href="/wiki/Sam_Harris" title="Sam Harris">Sam Harris</a></li>
<li><a href="/wiki/Stephen_Hawking" title="Stephen Hawking">Stephen Hawking</a></li>
<li><a href="/wiki/Bill_Hibbard" title="Bill Hibbard">Bill Hibbard</a></li>
<li><a href="/wiki/Bill_Joy" title="Bill Joy">Bill Joy</a></li>
<li><a href="/wiki/Elon_Musk" title="Elon Musk">Elon Musk</a></li>
<li><a href="/wiki/Steve_Omohundro" title="Steve Omohundro">Steve Omohundro</a></li>
<li><a href="/wiki/Huw_Price" title="Huw Price">Huw Price</a></li>
<li><a href="/wiki/Martin_Rees" title="Martin Rees">Martin Rees</a></li>
<li><a href="/wiki/Stuart_J._Russell" title="Stuart J. Russell">Stuart J. Russell</a></li>
<li><a href="/wiki/Jaan_Tallinn" title="Jaan Tallinn">Jaan Tallinn</a></li>
<li><a href="/wiki/Max_Tegmark" title="Max Tegmark">Max Tegmark</a></li>
<li><a href="/wiki/Frank_Wilczek" title="Frank Wilczek">Frank Wilczek</a></li>
<li><a href="/wiki/Roman_Yampolskiy" title="Roman Yampolskiy">Roman Yampolskiy</a></li>
<li><a href="/wiki/Andrew_Yang" title="Andrew Yang">Andrew Yang</a></li>
<li><a href="/wiki/Eliezer_Yudkowsky" title="Eliezer Yudkowsky">Eliezer Yudkowsky</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Other</th><td class="navbox-list navbox-even hlist" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/Global_catastrophic_risk#Artificial_intelligence" title="Global catastrophic risk">Artificial intelligence as a global catastrophic risk</a></li>
<li><a href="/wiki/Artificial_general_intelligence#Controversies_and_dangers" title="Artificial general intelligence">Controversies and dangers of artificial general intelligence</a></li>
<li><a href="/wiki/Ethics_of_artificial_intelligence" title="Ethics of artificial intelligence">Ethics of artificial intelligence</a></li>
<li><i><a href="/wiki/Human_Compatible" title="Human Compatible">Human Compatible</a></i></li>
<li><a href="/wiki/Open_Letter_on_Artificial_Intelligence" title="Open Letter on Artificial Intelligence">Open Letter on Artificial Intelligence</a></li>
<li><i><a href="/wiki/Our_Final_Invention" title="Our Final Invention">Our Final Invention</a></i></li>
<li><i><a href="/wiki/The_Precipice:_Existential_Risk_and_the_Future_of_Humanity" title="The Precipice: Existential Risk and the Future of Humanity">The Precipice</a></i></li>
<li><i><a href="/wiki/Superintelligence:_Paths,_Dangers,_Strategies" title="Superintelligence: Paths, Dangers, Strategies">Superintelligence: Paths, Dangers, Strategies</a></i></li></ul>
</div></td></tr><tr><td class="navbox-abovebelow" colspan="2"><div><img alt="Category" src="//upload.wikimedia.org/wikipedia/en/thumb/4/48/Folder_Hexagonal_Icon.svg/16px-Folder_Hexagonal_Icon.svg.png" decoding="async" title="Category" width="16" height="14" srcset="//upload.wikimedia.org/wikipedia/en/thumb/4/48/Folder_Hexagonal_Icon.svg/24px-Folder_Hexagonal_Icon.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/4/48/Folder_Hexagonal_Icon.svg/32px-Folder_Hexagonal_Icon.svg.png 2x" data-file-width="36" data-file-height="31" /> <a href="/wiki/Category:Existential_risk_from_artificial_general_intelligence" title="Category:Existential risk from artificial general intelligence">Category</a></div></td></tr></tbody></table></div>
<!-- 
NewPP limit report
Parsed by mw1290
Cached time: 20200406164149
Cache expiry: 2592000
Dynamic content: false
Complications: [vary‐revision‐sha1]
CPU time usage: 0.248 seconds
Real time usage: 0.357 seconds
Preprocessor visited node count: 808/1000000
Post‐expand include size: 32147/2097152 bytes
Template argument size: 661/2097152 bytes
Highest expansion depth: 11/40
Expensive parser function count: 2/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 32143/5000000 bytes
Number of Wikibase entities loaded: 2/400
Lua time usage: 0.134/10.000 seconds
Lua memory usage: 4.35 MB/50 MB
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%  313.651      1 -total
 57.06%  178.976      1 Template:Reflist
 26.55%   83.279      2 Template:Cite_book
 17.73%   55.603      3 Template:Cite_journal
 17.63%   55.303      1 Template:Short_description
 12.40%   38.896      1 Template:Pagetype
  8.67%   27.198      1 Template:Existential_risk_from_artificial_intelligence
  7.51%   23.563      1 Template:Navbox
  7.22%   22.633      1 Template:YouTube
  4.90%   15.366      3 Template:Cite_news
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:31641770-0!canonical and timestamp 20200406164202 and revision id 949464398
 -->
</div><noscript><img src="//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" alt="" title="" width="1" height="1" style="border: none; position: absolute;" /></noscript></div>
		<div class="printfooter">Retrieved from "<a dir="ltr" href="https://en.wikipedia.org/w/index.php?title=AI_box&amp;oldid=949464398">https://en.wikipedia.org/w/index.php?title=AI_box&amp;oldid=949464398</a>"</div>
		<div id="catlinks" class="catlinks" data-mw="interface"><div id="mw-normal-catlinks" class="mw-normal-catlinks"><a href="/wiki/Help:Category" title="Help:Category">Categories</a>: <ul><li><a href="/wiki/Category:Singularitarianism" title="Category:Singularitarianism">Singularitarianism</a></li><li><a href="/wiki/Category:Philosophy_of_artificial_intelligence" title="Category:Philosophy of artificial intelligence">Philosophy of artificial intelligence</a></li><li><a href="/wiki/Category:Existential_risk_from_artificial_general_intelligence" title="Category:Existential risk from artificial general intelligence">Existential risk from artificial general intelligence</a></li></ul></div><div id="mw-hidden-catlinks" class="mw-hidden-catlinks mw-hidden-cats-hidden">Hidden categories: <ul><li><a href="/wiki/Category:Articles_with_short_description" title="Category:Articles with short description">Articles with short description</a></li></ul></div></div>
		<div class="visualClear"></div>
		
	</div>
</div>
<div id='mw-data-after-content'>
	<div class="read-more-container"></div>
</div>

<div id="mw-navigation">
    <h2>Navigation menu</h2>
    <div id="mw-head">
        
        <div id="p-personal" role="navigation" class="" aria-labelledby="p-personal-label">
        	<h3 id="p-personal-label">Personal tools</h3>
        	<ul >
        		
        		<li id="pt-anonuserpage">Not logged in</li>
        		<li id="pt-anontalk"><a href="/wiki/Special:MyTalk" title="Discussion about edits from this IP address [n]" accesskey="n">Talk</a></li><li id="pt-anoncontribs"><a href="/wiki/Special:MyContributions" title="A list of edits made from this IP address [y]" accesskey="y">Contributions</a></li><li id="pt-createaccount"><a href="/w/index.php?title=Special:CreateAccount&amp;returnto=AI+box" title="You are encouraged to create an account and log in; however, it is not mandatory">Create account</a></li><li id="pt-login"><a href="/w/index.php?title=Special:UserLogin&amp;returnto=AI+box" title="You&#039;re encouraged to log in; however, it&#039;s not mandatory. [o]" accesskey="o">Log in</a></li>
        	</ul>
        </div>
        <div id="left-navigation">
            <div id="p-namespaces" role="navigation" class="vectorTabs " aria-labelledby="p-namespaces-label">
            	<h3 id="p-namespaces-label">Namespaces</h3>
            	<ul >
            		<li id="ca-nstab-main" class="selected"><a href="/wiki/AI_box" title="View the content page [c]" accesskey="c">Article</a></li><li id="ca-talk"><a href="/wiki/Talk:AI_box" rel="discussion" title="Discuss improvements to the content page [t]" accesskey="t">Talk</a></li>
            	</ul>
            </div>
            <div id="p-variants" role="navigation" class="vectorMenu emptyPortlet" aria-labelledby="p-variants-label">
            	<input type="checkbox" class="vectorMenuCheckbox" aria-labelledby="p-variants-label" />
            	<h3 id="p-variants-label">
            		<span>Variants</span>
            	</h3>
            	<ul class="menu" >
            		
            	</ul>
            </div>
        </div>
        <div id="right-navigation">
            <div id="p-views" role="navigation" class="vectorTabs " aria-labelledby="p-views-label">
            	<h3 id="p-views-label">Views</h3>
            	<ul >
            		<li id="ca-view" class="collapsible selected"><a href="/wiki/AI_box">Read</a></li><li id="ca-edit" class="collapsible"><a href="/w/index.php?title=AI_box&amp;action=edit" title="Edit this page [e]" accesskey="e">Edit</a></li><li id="ca-history" class="collapsible"><a href="/w/index.php?title=AI_box&amp;action=history" title="Past revisions of this page [h]" accesskey="h">View history</a></li>
            	</ul>
            </div>
            <div id="p-cactions" role="navigation" class="vectorMenu emptyPortlet" aria-labelledby="p-cactions-label">
            	<input type="checkbox" class="vectorMenuCheckbox" aria-labelledby="p-cactions-label" />
            	<h3 id="p-cactions-label">
            		<span>More</span>
            	</h3>
            	<ul class="menu" >
            		
            	</ul>
            </div>
            <div id="p-search" role="search">
            	<h3 >
            		<label for="searchInput">Search</label>
            	</h3>
            	<form action="/w/index.php" id="searchform">
            		<div id="simpleSearch">
            			<input type="search" name="search" placeholder="Search Wikipedia" title="Search Wikipedia [f]" accesskey="f" id="searchInput"/>
            			<input type="hidden" value="Special:Search" name="title"/>
            			<input type="submit" name="fulltext" value="Search" title="Search Wikipedia for this text" id="mw-searchButton" class="searchButton mw-fallbackSearchButton"/>
            			<input type="submit" name="go" value="Go" title="Go to a page with this exact name if it exists" id="searchButton" class="searchButton"/>
            		</div>
            	</form>
            </div>
        </div>
    </div>
    
    <div id="mw-panel">
    	<div id="p-logo" role="banner">
    		<a  title="Visit the main page" class="mw-wiki-logo" href="/wiki/Main_Page"></a>
    	</div>
    	<div class="portal" role="navigation" id="p-navigation"  aria-labelledby="p-navigation-label">
    		<h3  id="p-navigation-label">
    			Navigation
    		</h3>
    		<div class="body">
    			<ul><li id="n-mainpage-description"><a href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z">Main page</a></li><li id="n-contents"><a href="/wiki/Wikipedia:Contents" title="Guides to browsing Wikipedia">Contents</a></li><li id="n-featuredcontent"><a href="/wiki/Wikipedia:Featured_content" title="Featured content – the best of Wikipedia">Featured content</a></li><li id="n-currentevents"><a href="/wiki/Portal:Current_events" title="Find background information on current events">Current events</a></li><li id="n-randompage"><a href="/wiki/Special:Random" title="Load a random article [x]" accesskey="x">Random article</a></li><li id="n-sitesupport"><a href="https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&amp;utm_medium=sidebar&amp;utm_campaign=C13_en.wikipedia.org&amp;uselang=en" title="Support us">Donate to Wikipedia</a></li><li id="n-shoplink"><a href="//shop.wikimedia.org" title="Visit the Wikipedia store">Wikipedia store</a></li></ul>
    			
    		</div>
    	</div>
    	
    	<div class="portal" role="navigation" id="p-interaction"  aria-labelledby="p-interaction-label">
    		<h3  id="p-interaction-label">
    			Interaction
    		</h3>
    		<div class="body">
    			<ul><li id="n-help"><a href="/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia">Help</a></li><li id="n-aboutsite"><a href="/wiki/Wikipedia:About" title="Find out about Wikipedia">About Wikipedia</a></li><li id="n-portal"><a href="/wiki/Wikipedia:Community_portal" title="About the project, what you can do, where to find things">Community portal</a></li><li id="n-recentchanges"><a href="/wiki/Special:RecentChanges" title="A list of recent changes in the wiki [r]" accesskey="r">Recent changes</a></li><li id="n-contactpage"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia">Contact page</a></li></ul>
    			
    		</div>
    	</div>
    	
    	<div class="portal" role="navigation" id="p-tb"  aria-labelledby="p-tb-label">
    		<h3  id="p-tb-label">
    			Tools
    		</h3>
    		<div class="body">
    			<ul><li id="t-whatlinkshere"><a href="/wiki/Special:WhatLinksHere/AI_box" title="List of all English Wikipedia pages containing links to this page [j]" accesskey="j">What links here</a></li><li id="t-recentchangeslinked"><a href="/wiki/Special:RecentChangesLinked/AI_box" rel="nofollow" title="Recent changes in pages linked from this page [k]" accesskey="k">Related changes</a></li><li id="t-upload"><a href="/wiki/Wikipedia:File_Upload_Wizard" title="Upload files [u]" accesskey="u">Upload file</a></li><li id="t-specialpages"><a href="/wiki/Special:SpecialPages" title="A list of all special pages [q]" accesskey="q">Special pages</a></li><li id="t-permalink"><a href="/w/index.php?title=AI_box&amp;oldid=949464398" title="Permanent link to this revision of the page">Permanent link</a></li><li id="t-info"><a href="/w/index.php?title=AI_box&amp;action=info" title="More information about this page">Page information</a></li><li id="t-wikibase"><a href="https://www.wikidata.org/wiki/Special:EntityPage/Q4652026" title="Link to connected data repository item [g]" accesskey="g">Wikidata item</a></li><li id="t-cite"><a href="/w/index.php?title=Special:CiteThisPage&amp;page=AI_box&amp;id=949464398&amp;wpFormIdentifier=titleform" title="Information on how to cite this page">Cite this page</a></li></ul>
    			
    		</div>
    	</div>
    	
    	<div class="portal" role="navigation" id="p-coll-print_export"  aria-labelledby="p-coll-print_export-label">
    		<h3  id="p-coll-print_export-label">
    			Print/export
    		</h3>
    		<div class="body">
    			<ul><li id="coll-create_a_book"><a href="/w/index.php?title=Special:Book&amp;bookcmd=book_creator&amp;referer=AI+box">Create a book</a></li><li id="coll-download-as-rl"><a href="/w/index.php?title=Special:ElectronPdf&amp;page=AI+box&amp;action=show-download-screen">Download as PDF</a></li><li id="t-print"><a href="/w/index.php?title=AI_box&amp;printable=yes" title="Printable version of this page [p]" accesskey="p">Printable version</a></li></ul>
    			
    		</div>
    	</div>
    	
    	<div class="portal" role="navigation" id="p-lang"  aria-labelledby="p-lang-label">
    		<h3  id="p-lang-label">
    			Languages
    		</h3>
    		<div class="body">
    			<ul><li class="interlanguage-link interwiki-es"><a href="https://es.wikipedia.org/wiki/AI_box" title="AI box – Spanish" lang="es" hreflang="es" class="interlanguage-link-target">Español</a></li></ul>
    			<div class="after-portlet after-portlet-lang"><span class="wb-langlinks-edit wb-langlinks-link"><a href="https://www.wikidata.org/wiki/Special:EntityPage/Q4652026#sitelinks-wikipedia" title="Edit interlanguage links" class="wbc-editpage">Edit links</a></span></div>
    		</div>
    	</div>
    	
    </div>
</div>

<div id="footer" role="contentinfo" >
	<ul id="footer-info" class="">
		<li id="footer-info-lastmod"> This page was last edited on 6 April 2020, at 16:42<span class="anonymous-show">&#160;(UTC)</span>.</li>
		<li id="footer-info-copyright">Text is available under the <a rel="license" href="//en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License">Creative Commons Attribution-ShareAlike License</a><a rel="license" href="//creativecommons.org/licenses/by-sa/3.0/" style="display:none;"></a>;
additional terms may apply.  By using this site, you agree to the <a href="//foundation.wikimedia.org/wiki/Terms_of_Use">Terms of Use</a> and <a href="//foundation.wikimedia.org/wiki/Privacy_policy">Privacy Policy</a>. Wikipedia® is a registered trademark of the <a href="//www.wikimediafoundation.org/">Wikimedia Foundation, Inc.</a>, a non-profit organization.</li>
	</ul>
	<ul id="footer-places" class="">
		<li id="footer-places-privacy"><a href="https://foundation.wikimedia.org/wiki/Privacy_policy" class="extiw" title="wmf:Privacy policy">Privacy policy</a></li>
		<li id="footer-places-about"><a href="/wiki/Wikipedia:About" title="Wikipedia:About">About Wikipedia</a></li>
		<li id="footer-places-disclaimer"><a href="/wiki/Wikipedia:General_disclaimer" title="Wikipedia:General disclaimer">Disclaimers</a></li>
		<li id="footer-places-contact"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us">Contact Wikipedia</a></li>
		<li id="footer-places-developers"><a href="https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute">Developers</a></li>
		<li id="footer-places-statslink"><a href="https://stats.wikimedia.org/#/en.wikipedia.org">Statistics</a></li>
		<li id="footer-places-cookiestatement"><a href="https://foundation.wikimedia.org/wiki/Cookie_statement">Cookie statement</a></li>
		<li id="footer-places-mobileview"><a href="//en.m.wikipedia.org/w/index.php?title=AI_box&amp;mobileaction=toggle_view_mobile" class="noprint stopMobileRedirectToggle">Mobile view</a></li>
	</ul>
	<ul id="footer-icons" class="noprint">
		<li id="footer-copyrightico"><a href="https://wikimediafoundation.org/"><img src="/static/images/wikimedia-button.png" srcset="/static/images/wikimedia-button-1.5x.png 1.5x, /static/images/wikimedia-button-2x.png 2x" width="88" height="31" alt="Wikimedia Foundation"/></a></li>
		<li id="footer-poweredbyico"><a href="https://www.mediawiki.org/"><img src="/static/images/poweredby_mediawiki_88x31.png" alt="Powered by MediaWiki" srcset="/static/images/poweredby_mediawiki_132x47.png 1.5x, /static/images/poweredby_mediawiki_176x62.png 2x" width="88" height="31"/></a></li>
	</ul>
	<div style="clear: both;"></div>
</div>

<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgPageParseReport":{"limitreport":{"cputime":"0.248","walltime":"0.357","ppvisitednodes":{"value":808,"limit":1000000},"postexpandincludesize":{"value":32147,"limit":2097152},"templateargumentsize":{"value":661,"limit":2097152},"expansiondepth":{"value":11,"limit":40},"expensivefunctioncount":{"value":2,"limit":500},"unstrip-depth":{"value":1,"limit":20},"unstrip-size":{"value":32143,"limit":5000000},"entityaccesscount":{"value":2,"limit":400},"timingprofile":["100.00%  313.651      1 -total"," 57.06%  178.976      1 Template:Reflist"," 26.55%   83.279      2 Template:Cite_book"," 17.73%   55.603      3 Template:Cite_journal"," 17.63%   55.303      1 Template:Short_description"," 12.40%   38.896      1 Template:Pagetype","  8.67%   27.198      1 Template:Existential_risk_from_artificial_intelligence","  7.51%   23.563      1 Template:Navbox","  7.22%   22.633      1 Template:YouTube","  4.90%   15.366      3 Template:Cite_news"]},"scribunto":{"limitreport-timeusage":{"value":"0.134","limit":"10.000"},"limitreport-memusage":{"value":4563401,"limit":52428800}},"cachereport":{"origin":"mw1290","timestamp":"20200406164149","ttl":2592000,"transientcontent":false}}});});</script>
<script type="application/ld+json">{"@context":"https:\/\/schema.org","@type":"Article","name":"AI box","url":"https:\/\/en.wikipedia.org\/wiki\/AI_box","sameAs":"http:\/\/www.wikidata.org\/entity\/Q4652026","mainEntity":"http:\/\/www.wikidata.org\/entity\/Q4652026","author":{"@type":"Organization","name":"Contributors to Wikimedia projects"},"publisher":{"@type":"Organization","name":"Wikimedia Foundation, Inc.","logo":{"@type":"ImageObject","url":"https:\/\/www.wikimedia.org\/static\/images\/wmf-hor-googpub.png"}},"datePublished":"2011-05-01T00:21:45Z","dateModified":"2020-04-06T16:42:02Z","headline":"hypothetical isolated computer hardware system where a possibly dangerous artificial intelligence is kept constrained in a virtual prison and not allowed to manipulate events in the external world"}</script>
<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgBackendResponseTime":115,"wgHostname":"mw1365"});});</script></body></html>
