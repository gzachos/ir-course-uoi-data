
<!DOCTYPE html>
<html class="client-nojs" lang="en" dir="ltr">
<head>
<meta charset="UTF-8"/>
<title>Instrumental convergence - Wikipedia</title>
<script>document.documentElement.className="client-js";RLCONF={"wgBreakFrames":!1,"wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgRequestId":"Xn1RtQpAMOIAAEWVsKQAAABW","wgCSPNonce":!1,"wgCanonicalNamespace":"","wgCanonicalSpecialPageName":!1,"wgNamespaceNumber":0,"wgPageName":"Instrumental_convergence","wgTitle":"Instrumental convergence","wgCurRevisionId":942082132,"wgRevisionId":942082132,"wgArticleId":43591208,"wgIsArticle":!0,"wgIsRedirect":!1,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":["Articles with short description","All articles with incomplete citations","Articles with incomplete citations from September 2018","Wikipedia articles needing page number citations from September 2018","Artificial intelligence","Goal","Intention","Risk",
"Existential risk from artificial general intelligence"],"wgPageContentLanguage":"en","wgPageContentModel":"wikitext","wgRelevantPageName":"Instrumental_convergence","wgRelevantArticleId":43591208,"wgIsProbablyEditable":!0,"wgRelevantPageIsProbablyEditable":!0,"wgRestrictionEdit":[],"wgRestrictionMove":[],"wgMediaViewerOnClick":!0,"wgMediaViewerEnabledByDefault":!0,"wgPopupsReferencePreviews":!1,"wgPopupsConflictsWithNavPopupGadget":!1,"wgVisualEditor":{"pageLanguageCode":"en","pageLanguageDir":"ltr","pageVariantFallbacks":"en"},"wgMFDisplayWikibaseDescriptions":{"search":!0,"nearby":!0,"watchlist":!0,"tagline":!1},"wgWMESchemaEditAttemptStepOversample":!1,"wgULSCurrentAutonym":"English","wgNoticeProject":"wikipedia","wgWikibaseItemId":"Q18208100","wgCentralAuthMobileDomain":!1,"wgEditSubmitButtonLabelPublish":!0};RLSTATE={"ext.globalCssJs.user.styles":"ready","site.styles":"ready","noscript":"ready","user.styles":"ready","ext.globalCssJs.user":"ready",
"user":"ready","user.options":"loading","ext.cite.styles":"ready","jquery.makeCollapsible.styles":"ready","mediawiki.toc.styles":"ready","skins.vector.styles.legacy":"ready","wikibase.client.init":"ready","ext.visualEditor.desktopArticleTarget.noscript":"ready","ext.uls.interlanguage":"ready","ext.wikimediaBadges":"ready"};RLPAGEMODULES=["ext.cite.ux-enhancements","site","mediawiki.page.startup","skins.vector.js","mediawiki.page.ready","jquery.makeCollapsible","mediawiki.toc","ext.gadget.ReferenceTooltips","ext.gadget.charinsert","ext.gadget.refToolbar","ext.gadget.extra-toolbar-buttons","ext.gadget.switcher","ext.centralauth.centralautologin","mmv.head","mmv.bootstrap.autostart","ext.popups","ext.visualEditor.desktopArticleTarget.init","ext.visualEditor.targetLoader","ext.eventLogging","ext.wikimediaEvents","ext.navigationTiming","ext.uls.compactlinks","ext.uls.interface","ext.cx.eventlogging.campaigns","ext.quicksurveys.init","ext.centralNotice.geoIP","ext.centralNotice.startUp"];</script>
<script>(RLQ=window.RLQ||[]).push(function(){mw.loader.implement("user.options@1hzgi",function($,jQuery,require,module){/*@nomin*/mw.user.tokens.set({"patrolToken":"+\\","watchToken":"+\\","csrfToken":"+\\"});
});});</script>
<link rel="stylesheet" href="/w/load.php?lang=en&amp;modules=ext.cite.styles%7Cext.uls.interlanguage%7Cext.visualEditor.desktopArticleTarget.noscript%7Cext.wikimediaBadges%7Cjquery.makeCollapsible.styles%7Cmediawiki.toc.styles%7Cskins.vector.styles.legacy%7Cwikibase.client.init&amp;only=styles&amp;skin=vector"/>
<script async="" src="/w/load.php?lang=en&amp;modules=startup&amp;only=scripts&amp;raw=1&amp;skin=vector"></script>
<meta name="ResourceLoaderDynamicStyles" content=""/>
<link rel="stylesheet" href="/w/load.php?lang=en&amp;modules=site.styles&amp;only=styles&amp;skin=vector"/>
<meta name="generator" content="MediaWiki 1.35.0-wmf.25"/>
<meta name="referrer" content="origin"/>
<meta name="referrer" content="origin-when-crossorigin"/>
<meta name="referrer" content="origin-when-cross-origin"/>
<link rel="alternate" type="application/x-wiki" title="Edit this page" href="/w/index.php?title=Instrumental_convergence&amp;action=edit"/>
<link rel="edit" title="Edit this page" href="/w/index.php?title=Instrumental_convergence&amp;action=edit"/>
<link rel="apple-touch-icon" href="/static/apple-touch/wikipedia.png"/>
<link rel="shortcut icon" href="/static/favicon/wikipedia.ico"/>
<link rel="search" type="application/opensearchdescription+xml" href="/w/opensearch_desc.php" title="Wikipedia (en)"/>
<link rel="EditURI" type="application/rsd+xml" href="//en.wikipedia.org/w/api.php?action=rsd"/>
<link rel="license" href="//creativecommons.org/licenses/by-sa/3.0/"/>
<link rel="alternate" type="application/atom+xml" title="Wikipedia Atom feed" href="/w/index.php?title=Special:RecentChanges&amp;feed=atom"/>
<link rel="canonical" href="https://en.wikipedia.org/wiki/Instrumental_convergence"/>
<link rel="dns-prefetch" href="//login.wikimedia.org"/>
<link rel="dns-prefetch" href="//meta.wikimedia.org" />
<!--[if lt IE 9]><script src="/w/resources/lib/html5shiv/html5shiv.js"></script><![endif]-->
</head>
<body class="mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject mw-editable page-Instrumental_convergence rootpage-Instrumental_convergence skin-vector action-view">
<div id="mw-page-base" class="noprint"></div>
<div id="mw-head-base" class="noprint"></div>
<div id="content" class="mw-body" role="main">
	<a id="top"></a>
	<div id="siteNotice" class="mw-body-content"><!-- CentralNotice --></div>
	<div class="mw-indicators mw-body-content">
</div>

	<h1 id="firstHeading" class="firstHeading" lang="en">Instrumental convergence</h1>
	
	<div id="bodyContent" class="mw-body-content">
		<div id="siteSub" class="noprint">From Wikipedia, the free encyclopedia</div>
		<div id="contentSub"></div>
		
		
		<div id="jump-to-nav"></div>
		<a class="mw-jump-link" href="#mw-head">Jump to navigation</a>
		<a class="mw-jump-link" href="#p-search">Jump to search</a>
		<div id="mw-content-text" lang="en" dir="ltr" class="mw-content-ltr"><div class="mw-parser-output"><div class="shortdescription nomobile noexcerpt noprint searchaux" style="display:none">Hypothesis about intelligent agents</div>
<p><b>Instrumental convergence</b> is the hypothetical tendency for most sufficiently <a href="/wiki/Intelligent_agent" title="Intelligent agent">intelligent agents</a> to pursue potentially unbounded instrumental goals such as <a href="/wiki/Self-preservation" title="Self-preservation">self-preservation</a> and <a href="/wiki/Resource" title="Resource">resource</a> acquisition, provided that their ultimate goals are themselves unbounded.
</p><p>Instrumental convergence suggests that an intelligent agent with unbounded but apparently harmless goals can act in surprisingly harmful ways. For example, a computer with the sole, unconstrained goal of solving the <a href="/wiki/Riemann_hypothesis" title="Riemann hypothesis">Riemann hypothesis</a> could attempt to turn the entire Earth into <a href="/wiki/Computronium" title="Computronium">computronium</a> in an effort to increase its computing power so that it can succeed in its calculations.<sup id="cite_ref-aama_1-0" class="reference"><a href="#cite_note-aama-1">&#91;1&#93;</a></sup>
</p><p>Proposed <b>basic AI drives</b> include utility function or goal-content integrity, self-protection, freedom from interference, <a href="/wiki/Self-improvement" class="mw-redirect" title="Self-improvement">self-improvement</a>, and non-satiable acquisition of additional resources.
</p>
<div id="toc" class="toc" role="navigation" aria-labelledby="mw-toc-heading"><input type="checkbox" role="button" id="toctogglecheckbox" class="toctogglecheckbox" style="display:none" /><div class="toctitle" lang="en" dir="ltr"><h2 id="mw-toc-heading">Contents</h2><span class="toctogglespan"><label class="toctogglelabel" for="toctogglecheckbox"></label></span></div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="#Instrumental_and_final_goals"><span class="tocnumber">1</span> <span class="toctext">Instrumental and final goals</span></a></li>
<li class="toclevel-1 tocsection-2"><a href="#Hypothetical_examples_of_convergence"><span class="tocnumber">2</span> <span class="toctext">Hypothetical examples of convergence</span></a>
<ul>
<li class="toclevel-2 tocsection-3"><a href="#Paperclip_maximizer"><span class="tocnumber">2.1</span> <span class="toctext">Paperclip maximizer</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-4"><a href="#Basic_AI_drives"><span class="tocnumber">3</span> <span class="toctext">Basic AI drives</span></a>
<ul>
<li class="toclevel-2 tocsection-5"><a href="#Goal-content_integrity"><span class="tocnumber">3.1</span> <span class="toctext">Goal-content integrity</span></a>
<ul>
<li class="toclevel-3 tocsection-6"><a href="#In_artificial_intelligence"><span class="tocnumber">3.1.1</span> <span class="toctext">In artificial intelligence</span></a></li>
</ul>
</li>
<li class="toclevel-2 tocsection-7"><a href="#Resource_acquisition"><span class="tocnumber">3.2</span> <span class="toctext">Resource acquisition</span></a></li>
<li class="toclevel-2 tocsection-8"><a href="#Cognitive_enhancement"><span class="tocnumber">3.3</span> <span class="toctext">Cognitive enhancement</span></a></li>
<li class="toclevel-2 tocsection-9"><a href="#Technological_perfection"><span class="tocnumber">3.4</span> <span class="toctext">Technological perfection</span></a></li>
<li class="toclevel-2 tocsection-10"><a href="#Self-preservation"><span class="tocnumber">3.5</span> <span class="toctext">Self-preservation</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-11"><a href="#Instrumental_convergence_thesis"><span class="tocnumber">4</span> <span class="toctext">Instrumental convergence thesis</span></a></li>
<li class="toclevel-1 tocsection-12"><a href="#Impact"><span class="tocnumber">5</span> <span class="toctext">Impact</span></a></li>
<li class="toclevel-1 tocsection-13"><a href="#See_also"><span class="tocnumber">6</span> <span class="toctext">See also</span></a></li>
<li class="toclevel-1 tocsection-14"><a href="#Notes"><span class="tocnumber">7</span> <span class="toctext">Notes</span></a></li>
<li class="toclevel-1 tocsection-15"><a href="#References"><span class="tocnumber">8</span> <span class="toctext">References</span></a></li>
</ul>
</div>

<h2><span class="mw-headline" id="Instrumental_and_final_goals">Instrumental and final goals</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Instrumental_convergence&amp;action=edit&amp;section=1" title="Edit section: Instrumental and final goals">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div role="note" class="hatnote navigation-not-searchable">See also: <a href="/wiki/Instrumental_and_intrinsic_value" title="Instrumental and intrinsic value">Instrumental and intrinsic value</a></div>
<p>Final goals, or final values, are intrinsically valuable to an intelligent agent, whether an <a href="/wiki/Artificial_intelligence" title="Artificial intelligence">artificial intelligence</a> or a human being, as an <a href="/wiki/Ends-in-themselves" class="mw-redirect" title="Ends-in-themselves">end in itself</a>. In contrast, instrumental goals, or instrumental values, are only valuable to an agent as a means toward accomplishing its final goals. The contents and tradeoffs of a completely rational agent's "final goal" system can in principle be formalized into a <a href="/wiki/Utility_function" class="mw-redirect" title="Utility function">utility function</a>.
</p>
<h2><span class="mw-headline" id="Hypothetical_examples_of_convergence">Hypothetical examples of convergence</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Instrumental_convergence&amp;action=edit&amp;section=2" title="Edit section: Hypothetical examples of convergence">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>One hypothetical example of instrumental convergence is provided by the <a href="/wiki/Riemann_hypothesis" title="Riemann hypothesis">Riemann hypothesis</a> catastrophe. <a href="/wiki/Marvin_Minsky" title="Marvin Minsky">Marvin Minsky</a>, the co-founder of <a href="/wiki/MIT" class="mw-redirect" title="MIT">MIT</a>'s AI laboratory, has suggested that an artificial intelligence designed to solve the Riemann hypothesis might decide to take over all of Earth's resources to build supercomputers to help achieve its goal.<sup id="cite_ref-aama_1-1" class="reference"><a href="#cite_note-aama-1">&#91;1&#93;</a></sup> If the computer had instead been programmed to produce as many paper clips as possible, it would still decide to take all of Earth's resources to meet its final goal.<sup id="cite_ref-2" class="reference"><a href="#cite_note-2">&#91;2&#93;</a></sup> Even though these two final goals are different, both of them produce a <i>convergent</i> instrumental goal of taking over Earth's resources.<sup id="cite_ref-bostrom_chapter_7_3-0" class="reference"><a href="#cite_note-bostrom_chapter_7-3">&#91;3&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Paperclip_maximizer">Paperclip maximizer</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Instrumental_convergence&amp;action=edit&amp;section=3" title="Edit section: Paperclip maximizer">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>The paperclip maximizer is a <a href="/wiki/Thought_experiment" title="Thought experiment">thought experiment</a> described by Swedish philosopher <a href="/wiki/Nick_Bostrom" title="Nick Bostrom">Nick Bostrom</a> in 2003. It illustrates the <a href="/wiki/Existential_risk_from_advanced_artificial_intelligence" class="mw-redirect" title="Existential risk from advanced artificial intelligence">existential risk</a> that an <a href="/wiki/Artificial_general_intelligence" title="Artificial general intelligence">artificial general intelligence</a> may pose to human beings when programmed to pursue even seemingly-harmless goals, and the necessity of incorporating <a href="/wiki/Machine_ethics" title="Machine ethics">machine ethics</a> into <a href="/wiki/Artificial_intelligence" title="Artificial intelligence">artificial intelligence</a> design. The scenario describes an advanced artificial intelligence tasked with manufacturing paperclips. If such a machine were not programmed to value human life, or to use only designated resources in bounded time, then given enough power its optimized goal would be to turn all matter in the universe, including human beings, into either paperclips or machines which manufacture paperclips.<sup id="cite_ref-:0_4-0" class="reference"><a href="#cite_note-:0-4">&#91;4&#93;</a></sup>
</p>
<style data-mw-deduplicate="TemplateStyles:r886047036">.mw-parser-output .templatequote{overflow:hidden;margin:1em 0;padding:0 40px}.mw-parser-output .templatequote .templatequotecite{line-height:1.5em;text-align:left;padding-left:1.6em;margin-top:0}</style><blockquote class="templatequote"><p>Suppose we have an AI whose only goal is to make as many paper clips as possible. The AI will realize quickly that it would be much better if there were no humans because humans might decide to switch it off. Because if humans do so, there would be fewer paper clips. Also, human bodies contain a lot of atoms that could be made into paper clips. The future that the AI would be trying to gear towards would be one in which there were a lot of paper clips but no humans.</p><div class="templatequotecite">—&#8201;<cite><a href="/wiki/Nick_Bostrom" title="Nick Bostrom">Nick Bostrom</a>, as quoted in <cite class="citation news">Miles, Kathleen (2014-08-22). <a rel="nofollow" class="external text" href="https://www.huffingtonpost.com/2014/08/22/artificial-intelligence-oxford_n_5689858.html">"Artificial Intelligence May Doom The Human Race Within A Century, Oxford Professor Says"</a>. <i>Huffington Post</i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Huffington+Post&amp;rft.atitle=Artificial+Intelligence+May+Doom+The+Human+Race+Within+A+Century%2C+Oxford+Professor+Says&amp;rft.date=2014-08-22&amp;rft.aulast=Miles&amp;rft.aufirst=Kathleen&amp;rft_id=https%3A%2F%2Fwww.huffingtonpost.com%2F2014%2F08%2F22%2Fartificial-intelligence-oxford_n_5689858.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AInstrumental+convergence" class="Z3988"></span><style data-mw-deduplicate="TemplateStyles:r935243608">.mw-parser-output cite.citation{font-style:inherit}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Lock-gray-alt-2.svg/9px-Lock-gray-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Lock-red-alt-2.svg/9px-Lock-red-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/12px-Wikisource-logo.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:inherit;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-maint{display:none;color:#33aa33;margin-left:0.3em}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}</style><sup id="cite_ref-5" class="reference"><a href="#cite_note-5">&#91;5&#93;</a></sup></cite></div></blockquote><p>Bostrom has emphasised that he does not believe the paperclip maximiser scenario <i>per se</i> will actually occur; rather, his intention is to illustrate the dangers of creating <a href="/wiki/Superintelligence" title="Superintelligence">superintelligent</a> machines without knowing how to safely program them to eliminate existential risk to human beings.<sup id="cite_ref-6" class="reference"><a href="#cite_note-6">&#91;6&#93;</a></sup> The paperclip maximizer example illustrates the broad problem of managing powerful systems that lack human values.<sup id="cite_ref-7" class="reference"><a href="#cite_note-7">&#91;7&#93;</a></sup>
</p><h2><span class="mw-headline" id="Basic_AI_drives">Basic AI drives</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Instrumental_convergence&amp;action=edit&amp;section=4" title="Edit section: Basic AI drives">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p><a href="/wiki/Steve_Omohundro" title="Steve Omohundro">Steve Omohundro</a> has itemized several convergent instrumental goals, including <a href="/wiki/Self-preservation" title="Self-preservation">self-preservation</a> or self-protection, utility function or goal-content integrity, self-improvement, and resource acquisition. He refers to these as the "basic AI drives". A "drive" here denotes a "tendency which will be present unless specifically counteracted";<sup id="cite_ref-8" class="reference"><a href="#cite_note-8">&#91;8&#93;</a></sup> this is different from the psychological term "<a href="/wiki/Drive_theory" title="Drive theory">drive</a>", denoting an excitatory state produced by a homeostatic disturbance.<sup id="cite_ref-9" class="reference"><a href="#cite_note-9">&#91;9&#93;</a></sup> A tendency for a person to fill out income tax forms every year is a "drive" in Omohundro's sense, but not in the psychological sense.<sup id="cite_ref-10" class="reference"><a href="#cite_note-10">&#91;10&#93;</a></sup> Daniel Dewey of the <a href="/wiki/Machine_Intelligence_Research_Institute" title="Machine Intelligence Research Institute">Machine Intelligence Research Institute</a> argues that even an initially introverted self-rewarding AGI may continue to acquire free energy, space, time, and freedom from interference to ensure that it will not be stopped from self-rewarding.<sup id="cite_ref-11" class="reference"><a href="#cite_note-11">&#91;11&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Goal-content_integrity">Goal-content integrity</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Instrumental_convergence&amp;action=edit&amp;section=5" title="Edit section: Goal-content integrity">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>In humans, maintenance of final goals can be explained with a thought experiment. Suppose a man named "Gandhi" has a pill that, if he took it, would cause him to want to kill people. This Gandhi is currently a pacifist: one of his explicit final goals is to never kill anyone. Gandhi is likely to refuse to take the pill, because Gandhi knows that if in the future he wants to kill people, he is likely to actually kill people, and thus the goal of "not killing people" would not be satisfied.<sup id="cite_ref-12" class="reference"><a href="#cite_note-12">&#91;12&#93;</a></sup>
</p><p>However, in other cases, people seem happy to let their final values drift. Humans are complicated, and their goals can be inconsistent or unknown, even to themselves.<sup id="cite_ref-13" class="reference"><a href="#cite_note-13">&#91;13&#93;</a></sup>
</p>
<h4><span class="mw-headline" id="In_artificial_intelligence">In artificial intelligence</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Instrumental_convergence&amp;action=edit&amp;section=6" title="Edit section: In artificial intelligence">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>In 2009, <a href="/wiki/J%C3%BCrgen_Schmidhuber" title="Jürgen Schmidhuber">Jürgen Schmidhuber</a> concluded, in a setting where agents search for proofs about possible self-modifications, "that any rewrites of the utility function can happen only if the <a href="/wiki/G%C3%B6del_machine" title="Gödel machine">Gödel machine</a> first can prove that the rewrite is useful according to the present utility function."<sup id="cite_ref-14" class="reference"><a href="#cite_note-14">&#91;14&#93;</a></sup><sup id="cite_ref-hibbard_15-0" class="reference"><a href="#cite_note-hibbard-15">&#91;15&#93;</a></sup> An analysis by <a href="/wiki/Bill_Hibbard" title="Bill Hibbard">Bill Hibbard</a> of a different scenario is similarly consistent with maintenance of goal content integrity.<sup id="cite_ref-hibbard_15-1" class="reference"><a href="#cite_note-hibbard-15">&#91;15&#93;</a></sup> Hibbard also argues that in a utility maximizing framework the only goal is maximizing expected utility, so that instrumental goals should be called unintended instrumental actions.<sup id="cite_ref-16" class="reference"><a href="#cite_note-16">&#91;16&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Resource_acquisition">Resource acquisition</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Instrumental_convergence&amp;action=edit&amp;section=7" title="Edit section: Resource acquisition">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Many instrumental goals, such as [...] resource acquisition, are valuable to an agent because they increase its <i>freedom of action</i>.<sup id="cite_ref-formalizing_17-0" class="reference"><a href="#cite_note-formalizing-17">&#91;17&#93;</a></sup><sup class="noprint Inline-Template" style="white-space:nowrap;">&#91;<i><a href="/wiki/Wikipedia:Citing_sources#What_information_to_include" title="Wikipedia:Citing sources"><span title="A complete citation is needed (September 2018)">full citation needed</span></a></i>&#93;</sup>
</p><p>For almost any open-ended, non-trivial reward function (or set of goals), possessing more resources (such as equipment, raw materials, or energy) can enable the AI to find a more "optimal" solution. Resources can benefit some AIs directly, through being able to create more of whatever stuff its reward function values: "The AI neither hates you, nor loves you, but you are made out of atoms that it can use for something else."<sup id="cite_ref-18" class="reference"><a href="#cite_note-18">&#91;18&#93;</a></sup><sup id="cite_ref-shanahan_7.5_19-0" class="reference"><a href="#cite_note-shanahan_7.5-19">&#91;19&#93;</a></sup> In addition, almost all AIs can benefit from having more resources to spend on other instrumental goals, such as self-preservation.<sup id="cite_ref-shanahan_7.5_19-1" class="reference"><a href="#cite_note-shanahan_7.5-19">&#91;19&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Cognitive_enhancement">Cognitive enhancement</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Instrumental_convergence&amp;action=edit&amp;section=8" title="Edit section: Cognitive enhancement">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>"If the agent's final goals are fairly unbounded and the agent is in a position to become the first superintelligence and thereby obtain a decisive strategic advantage, [...] according to its preferences. At least in this special case, a rational intelligent agent would place a very *high instrumental value on cognitive enhancement*" <sup id="cite_ref-super_20-0" class="reference"><a href="#cite_note-super-20">&#91;20&#93;</a></sup><sup class="noprint Inline-Template" style="white-space:nowrap;">&#91;<i><a href="/wiki/Wikipedia:Citing_sources" title="Wikipedia:Citing sources"><span title="This citation requires a reference to the specific page or range of pages in which the material appears. (September 2018)">page&#160;needed</span></a></i>&#93;</sup>
</p>
<h3><span class="mw-headline" id="Technological_perfection">Technological perfection</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Instrumental_convergence&amp;action=edit&amp;section=9" title="Edit section: Technological perfection">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Many instrumental goals, such as [...] technological advancement, are valuable to an agent because they increase its <i>freedom of action</i>.<sup id="cite_ref-formalizing_17-1" class="reference"><a href="#cite_note-formalizing-17">&#91;17&#93;</a></sup><sup class="noprint Inline-Template" style="white-space:nowrap;">&#91;<i><a href="/wiki/Wikipedia:Citing_sources#What_information_to_include" title="Wikipedia:Citing sources"><span title="A complete citation is needed (September 2018)">full citation needed</span></a></i>&#93;</sup>
</p>
<h3><span class="mw-headline" id="Self-preservation">Self-preservation</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Instrumental_convergence&amp;action=edit&amp;section=10" title="Edit section: Self-preservation">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Many instrumental goals, such as [...] self-preservation, are valuable to an agent because they increase its <i>freedom of action</i>.<sup id="cite_ref-formalizing_17-2" class="reference"><a href="#cite_note-formalizing-17">&#91;17&#93;</a></sup><sup class="noprint Inline-Template" style="white-space:nowrap;">&#91;<i><a href="/wiki/Wikipedia:Citing_sources#What_information_to_include" title="Wikipedia:Citing sources"><span title="A complete citation is needed (September 2018)">full citation needed</span></a></i>&#93;</sup>
</p>
<h2><span class="mw-headline" id="Instrumental_convergence_thesis">Instrumental convergence thesis</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Instrumental_convergence&amp;action=edit&amp;section=11" title="Edit section: Instrumental convergence thesis">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>The instrumental convergence thesis, as outlined by philosopher <a href="/wiki/Nick_Bostrom" title="Nick Bostrom">Nick Bostrom</a>, states:
</p>
<blockquote><p>Several instrumental values can be identified which are convergent in the sense that their attainment would increase the chances of the agent's goal being realized for a wide range of final goals and a wide range of situations, implying that these instrumental values are likely to be pursued by a broad spectrum of situated intelligent agents.</p></blockquote>
<p>The instrumental convergence thesis applies only to instrumental goals; intelligent agents may have a wide variety of possible final goals.<sup id="cite_ref-bostrom_chapter_7_3-1" class="reference"><a href="#cite_note-bostrom_chapter_7-3">&#91;3&#93;</a></sup> Note that by Bostrom's <a href="/w/index.php?title=Orthogonality_Thesis&amp;action=edit&amp;redlink=1" class="new" title="Orthogonality Thesis (page does not exist)">Orthogonality Thesis</a>,<sup id="cite_ref-bostrom_chapter_7_3-2" class="reference"><a href="#cite_note-bostrom_chapter_7-3">&#91;3&#93;</a></sup> final goals of highly intelligent agents may be well-bounded in space, time, and resources; well-bounded ultimate goals do not, in general, engender unbounded instrumental goals.<sup id="cite_ref-21" class="reference"><a href="#cite_note-21">&#91;21&#93;</a></sup>
</p>
<h2><span class="mw-headline" id="Impact">Impact</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Instrumental_convergence&amp;action=edit&amp;section=12" title="Edit section: Impact">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Agents can acquire resources by trade or by conquest. A rational agent will, by definition, choose whatever option will maximize its implicit utility function; therefore a rational agent will trade for a subset of another agent's resources only if outright seizing the resources is too risky or costly (compared with the gains from taking all the resources), or if some other element in its utility function bars it from the seizure. In the case of a powerful, self-interested, rational superintelligence interacting with a lesser intelligence, peaceful trade (rather than unilateral seizure) seems unnecessary and suboptimal, and therefore unlikely.<sup id="cite_ref-formalizing_17-3" class="reference"><a href="#cite_note-formalizing-17">&#91;17&#93;</a></sup><sup class="noprint Inline-Template" style="white-space:nowrap;">&#91;<i><a href="/wiki/Wikipedia:Citing_sources#What_information_to_include" title="Wikipedia:Citing sources"><span title="A complete citation is needed (September 2018)">full citation needed</span></a></i>&#93;</sup>
</p><p>Some observers, such as Skype's <a href="/wiki/Jaan_Tallinn" title="Jaan Tallinn">Jaan Tallinn</a> and physicist <a href="/wiki/Max_Tegmark" title="Max Tegmark">Max Tegmark</a>, believe that "basic AI drives", and other <a href="/wiki/Unintended_consequences" title="Unintended consequences">unintended consequences</a> of superintelligent AI programmed by well-meaning programmers, could pose a significant threat to <a href="/wiki/Human_survival" class="mw-redirect" title="Human survival">human survival</a>, especially if an "intelligence explosion" abruptly occurs due to <a href="/wiki/Recursive" class="mw-redirect" title="Recursive">recursive</a> self-improvement. Since nobody knows how to predict beforehand when <a href="/wiki/Superintelligence" title="Superintelligence">superintelligence</a> will arrive, such observers call for research into <a href="/wiki/Friendly_artificial_intelligence" title="Friendly artificial intelligence">friendly artificial intelligence</a> as a possible way to mitigate <a href="/wiki/Existential_risk_from_artificial_general_intelligence" title="Existential risk from artificial general intelligence">existential risk from artificial general intelligence</a>.<sup id="cite_ref-22" class="reference"><a href="#cite_note-22">&#91;22&#93;</a></sup>
</p>
<h2><span class="mw-headline" id="See_also">See also</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Instrumental_convergence&amp;action=edit&amp;section=13" title="Edit section: See also">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<ul><li><a href="/wiki/AI_control_problem" title="AI control problem">AI control problem</a></li>
<li><a href="/wiki/Friendly_artificial_intelligence" title="Friendly artificial intelligence">Friendly artificial intelligence</a></li>
<li><a href="/wiki/Instrumental_and_intrinsic_value" title="Instrumental and intrinsic value">Instrumental and intrinsic value</a></li></ul>
<h2><span class="mw-headline" id="Notes">Notes</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Instrumental_convergence&amp;action=edit&amp;section=14" title="Edit section: Notes">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div class="reflist columns references-column-width" style="-moz-column-width: 30em; -webkit-column-width: 30em; column-width: 30em; list-style-type: decimal;">
<ol class="references">
<li id="cite_note-aama-1"><span class="mw-cite-backlink">^ <a href="#cite_ref-aama_1-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-aama_1-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation book">Russell, Stuart J.; Norvig, Peter (2003). "Section 26.3: The Ethics and Risks of Developing Artificial Intelligence". <a href="/wiki/Artificial_Intelligence:_A_Modern_Approach" title="Artificial Intelligence: A Modern Approach"><i>Artificial Intelligence: A Modern Approach</i></a>. Upper Saddle River, N.J.: Prentice Hall. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-0137903955" title="Special:BookSources/978-0137903955"><bdi>978-0137903955</bdi></a>. <q>Similarly, Marvin Minsky once suggested that an AI program designed to solve the Riemann Hypothesis might end up taking over all the resources of Earth to build more powerful supercomputers to help achieve its goal.</q></cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Section+26.3%3A+The+Ethics+and+Risks+of+Developing+Artificial+Intelligence&amp;rft.btitle=Artificial+Intelligence%3A+A+Modern+Approach&amp;rft.place=Upper+Saddle+River%2C+N.J.&amp;rft.pub=Prentice+Hall&amp;rft.date=2003&amp;rft.isbn=978-0137903955&amp;rft.aulast=Russell&amp;rft.aufirst=Stuart+J.&amp;rft.au=Norvig%2C+Peter&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AInstrumental+convergence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-2"><span class="mw-cite-backlink"><b><a href="#cite_ref-2">^</a></b></span> <span class="reference-text">Bostrom 2014, Chapter 8, p. 123. "An AI, designed to manage production in a factory, is given the final goal of maximizing the manufacturing of paperclips, and proceeds by converting first the Earth and then increasingly large chunks of the observable universe into paperclips."</span>
</li>
<li id="cite_note-bostrom_chapter_7-3"><span class="mw-cite-backlink">^ <a href="#cite_ref-bostrom_chapter_7_3-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-bostrom_chapter_7_3-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-bostrom_chapter_7_3-2"><sup><i><b>c</b></i></sup></a></span> <span class="reference-text">Bostrom 2014, chapter 7.</span>
</li>
<li id="cite_note-:0-4"><span class="mw-cite-backlink"><b><a href="#cite_ref-:0_4-0">^</a></b></span> <span class="reference-text"><cite class="citation web">Bostrom, Nick (2003). <a rel="nofollow" class="external text" href="http://www.nickbostrom.com/ethics/ai.html">"Ethical Issues in Advanced Artificial Intelligence"</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Ethical+Issues+in+Advanced+Artificial+Intelligence&amp;rft.date=2003&amp;rft.aulast=Bostrom&amp;rft.aufirst=Nick&amp;rft_id=http%3A%2F%2Fwww.nickbostrom.com%2Fethics%2Fai.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AInstrumental+convergence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-5"><span class="mw-cite-backlink"><b><a href="#cite_ref-5">^</a></b></span> <span class="reference-text"><cite class="citation news">Miles, Kathleen (2014-08-22). <a rel="nofollow" class="external text" href="https://www.huffingtonpost.com/2014/08/22/artificial-intelligence-oxford_n_5689858.html">"Artificial Intelligence May Doom The Human Race Within A Century, Oxford Professor Says"</a>. <i>Huffington Post</i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Huffington+Post&amp;rft.atitle=Artificial+Intelligence+May+Doom+The+Human+Race+Within+A+Century%2C+Oxford+Professor+Says&amp;rft.date=2014-08-22&amp;rft.aulast=Miles&amp;rft.aufirst=Kathleen&amp;rft_id=https%3A%2F%2Fwww.huffingtonpost.com%2F2014%2F08%2F22%2Fartificial-intelligence-oxford_n_5689858.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AInstrumental+convergence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-6"><span class="mw-cite-backlink"><b><a href="#cite_ref-6">^</a></b></span> <span class="reference-text"><cite class="citation web">Ford, Paul (11 February 2015). <a rel="nofollow" class="external text" href="http://www.technologyreview.com/review/534871/our-fear-of-artificial-intelligence/">"Are We Smart Enough to Control Artificial Intelligence?"</a>. <i>MIT Technology Review</i><span class="reference-accessdate">. Retrieved <span class="nowrap">25 January</span> 2016</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=MIT+Technology+Review&amp;rft.atitle=Are+We+Smart+Enough+to+Control+Artificial+Intelligence%3F&amp;rft.date=2015-02-11&amp;rft.aulast=Ford&amp;rft.aufirst=Paul&amp;rft_id=http%3A%2F%2Fwww.technologyreview.com%2Freview%2F534871%2Four-fear-of-artificial-intelligence%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AInstrumental+convergence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-7"><span class="mw-cite-backlink"><b><a href="#cite_ref-7">^</a></b></span> <span class="reference-text"><cite class="citation news">Friend, Tad (3 October 2016). <a rel="nofollow" class="external text" href="https://www.newyorker.com/magazine/2016/10/10/sam-altmans-manifest-destiny">"Sam Altman's Manifest Destiny"</a>. <i>The New Yorker</i><span class="reference-accessdate">. Retrieved <span class="nowrap">25 November</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+New+Yorker&amp;rft.atitle=Sam+Altman%27s+Manifest+Destiny&amp;rft.date=2016-10-03&amp;rft.aulast=Friend&amp;rft.aufirst=Tad&amp;rft_id=https%3A%2F%2Fwww.newyorker.com%2Fmagazine%2F2016%2F10%2F10%2Fsam-altmans-manifest-destiny&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AInstrumental+convergence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-8"><span class="mw-cite-backlink"><b><a href="#cite_ref-8">^</a></b></span> <span class="reference-text">Omohundro, S. M. (2008, February). The basic AI drives. In AGI (Vol. 171, pp. 483-492).</span>
</li>
<li id="cite_note-9"><span class="mw-cite-backlink"><b><a href="#cite_ref-9">^</a></b></span> <span class="reference-text">Seward, J. (1956). Drive, incentive, and reinforcement. Psychological Review, 63, 19-203.</span>
</li>
<li id="cite_note-10"><span class="mw-cite-backlink"><b><a href="#cite_ref-10">^</a></b></span> <span class="reference-text">Bostrom 2014, footnote 8 to chapter 7.</span>
</li>
<li id="cite_note-11"><span class="mw-cite-backlink"><b><a href="#cite_ref-11">^</a></b></span> <span class="reference-text">Dewey, Daniel. "Learning what to value." Artificial General Intelligence (2011): 309-314.</span>
</li>
<li id="cite_note-12"><span class="mw-cite-backlink"><b><a href="#cite_ref-12">^</a></b></span> <span class="reference-text">Yudkowsky, Eliezer. "Complex value systems in friendly AI." In Artificial general intelligence, pp. 388-393. Springer Berlin Heidelberg, 2011.</span>
</li>
<li id="cite_note-13"><span class="mw-cite-backlink"><b><a href="#cite_ref-13">^</a></b></span> <span class="reference-text">Bostrom 2014, chapter 7, p. 110. "We humans often seem happy to let our final values drift... For example, somebody deciding to have a child might predict that they will come to value the child for its own sake, even though at the time of the decision they may not particularly value their future child... Humans are complicated, and many factors might be in play in a situation like this... one might have a final value that involves having certain experiences and occupying a certain social role; and become a parent&#8212; and undergoing the attendant goal shift&#8212; might be a necessary aspect of that..."</span>
</li>
<li id="cite_note-14"><span class="mw-cite-backlink"><b><a href="#cite_ref-14">^</a></b></span> <span class="reference-text"><cite class="citation journal">Schmidhuber, J. R. (2009). "Ultimate Cognition à la Gödel". <i>Cognitive Computation</i>. <b>1</b> (2): 177–193. <a href="/wiki/CiteSeerX" title="CiteSeerX">CiteSeerX</a>&#160;<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.218.3323">10.1.1.218.3323</a></span>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1007%2Fs12559-009-9014-y">10.1007/s12559-009-9014-y</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Cognitive+Computation&amp;rft.atitle=Ultimate+Cognition+%C3%A0+la+G%C3%B6del&amp;rft.volume=1&amp;rft.issue=2&amp;rft.pages=177-193&amp;rft.date=2009&amp;rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.218.3323&amp;rft_id=info%3Adoi%2F10.1007%2Fs12559-009-9014-y&amp;rft.aulast=Schmidhuber&amp;rft.aufirst=J.+R.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AInstrumental+convergence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-hibbard-15"><span class="mw-cite-backlink">^ <a href="#cite_ref-hibbard_15-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-hibbard_15-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal">Hibbard, B. (2012). "Model-based Utility Functions". <i>Journal of Artificial General Intelligence</i>. <b>3</b>: 1–24. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.2478%2Fv10229-011-0013-5">10.2478/v10229-011-0013-5</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Artificial+General+Intelligence&amp;rft.atitle=Model-based+Utility+Functions&amp;rft.volume=3&amp;rft.pages=1-24&amp;rft.date=2012&amp;rft_id=info%3Adoi%2F10.2478%2Fv10229-011-0013-5&amp;rft.aulast=Hibbard&amp;rft.aufirst=B.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AInstrumental+convergence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-16"><span class="mw-cite-backlink"><b><a href="#cite_ref-16">^</a></b></span> <span class="reference-text">Hibbard, Bill (2014): Ethical Artificial Intelligence. <a rel="nofollow" class="external free" href="https://arxiv.org/abs/1411.1373">https://arxiv.org/abs/1411.1373</a></span>
</li>
<li id="cite_note-formalizing-17"><span class="mw-cite-backlink">^ <a href="#cite_ref-formalizing_17-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-formalizing_17-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-formalizing_17-2"><sup><i><b>c</b></i></sup></a> <a href="#cite_ref-formalizing_17-3"><sup><i><b>d</b></i></sup></a></span> <span class="reference-text">Benson-Tilsen, T., &amp; Soares, N. (2016, March). Formalizing Convergent Instrumental Goals. In AAAI Workshop: AI, Ethics, and Society.</span>
</li>
<li id="cite_note-18"><span class="mw-cite-backlink"><b><a href="#cite_ref-18">^</a></b></span> <span class="reference-text">Yudkowsky, Eliezer. "Artificial intelligence as a positive and negative factor in global risk." Global catastrophic risks (2008): 303. p. 333.</span>
</li>
<li id="cite_note-shanahan_7.5-19"><span class="mw-cite-backlink">^ <a href="#cite_ref-shanahan_7.5_19-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-shanahan_7.5_19-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><a href="/wiki/Murray_Shanahan" title="Murray Shanahan">Murray Shanahan</a>. <i>The Technological Singularity</i>. MIT Press, 2015. Chapter 7, Section 5: "Safe Superintelligence".</span>
</li>
<li id="cite_note-super-20"><span class="mw-cite-backlink"><b><a href="#cite_ref-super_20-0">^</a></b></span> <span class="reference-text">Bostrom, N. (2016). Superintelligence, Oxford University Press</span>
</li>
<li id="cite_note-21"><span class="mw-cite-backlink"><b><a href="#cite_ref-21">^</a></b></span> <span class="reference-text"><a rel="nofollow" class="external text" href="https://www.fhi.ox.ac.uk/wp-content/uploads/Reframing_Superintelligence_FHI-TR-2019-1.1-1.pdf">Reframing Superintelligence: Comprehensive AI Services as General Intelligence, Technical Report, 2019</a>, Future of Humanity Institute</span>
</li>
<li id="cite_note-22"><span class="mw-cite-backlink"><b><a href="#cite_ref-22">^</a></b></span> <span class="reference-text"><cite class="citation news"><a rel="nofollow" class="external text" href="https://www.chronicle.com/article/Is-Artificial-Intelligence-a/148763">"Is Artificial Intelligence a Threat?"</a>. <i><a href="/wiki/The_Chronicle_of_Higher_Education" title="The Chronicle of Higher Education">The Chronicle of Higher Education</a></i>. 11 September 2014<span class="reference-accessdate">. Retrieved <span class="nowrap">25 November</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+Chronicle+of+Higher+Education&amp;rft.atitle=Is+Artificial+Intelligence+a+Threat%3F&amp;rft.date=2014-09-11&amp;rft_id=https%3A%2F%2Fwww.chronicle.com%2Farticle%2FIs-Artificial-Intelligence-a%2F148763&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AInstrumental+convergence" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
</ol></div>
<h2><span class="mw-headline" id="References">References</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Instrumental_convergence&amp;action=edit&amp;section=15" title="Edit section: References">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<ul><li><a href="/wiki/Nick_Bostrom" title="Nick Bostrom">Nick Bostrom</a> (2014). <i><a href="/wiki/Superintelligence:_Paths,_Dangers,_Strategies" title="Superintelligence: Paths, Dangers, Strategies">Superintelligence: Paths, Dangers, Strategies</a></i>. Oxford: Oxford University Press. <link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/><a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/9780199678112" title="Special:BookSources/9780199678112">9780199678112</a>.</li></ul>
<div role="navigation" class="navbox" aria-labelledby="Existential_risk_from_artificial_intelligence" style="padding:3px"><table class="nowraplinks mw-collapsible autocollapse navbox-inner" style="border-spacing:0;background:transparent;color:inherit"><tbody><tr><th scope="col" class="navbox-title" colspan="2"><div class="plainlinks hlist navbar mini"><ul><li class="nv-view"><a href="/wiki/Template:Existential_risk_from_artificial_intelligence" title="Template:Existential risk from artificial intelligence"><abbr title="View this template" style=";;background:none transparent;border:none;-moz-box-shadow:none;-webkit-box-shadow:none;box-shadow:none; padding:0;">v</abbr></a></li><li class="nv-talk"><a href="/wiki/Template_talk:Existential_risk_from_artificial_intelligence" title="Template talk:Existential risk from artificial intelligence"><abbr title="Discuss this template" style=";;background:none transparent;border:none;-moz-box-shadow:none;-webkit-box-shadow:none;box-shadow:none; padding:0;">t</abbr></a></li><li class="nv-edit"><a class="external text" href="https://en.wikipedia.org/w/index.php?title=Template:Existential_risk_from_artificial_intelligence&amp;action=edit"><abbr title="Edit this template" style=";;background:none transparent;border:none;-moz-box-shadow:none;-webkit-box-shadow:none;box-shadow:none; padding:0;">e</abbr></a></li></ul></div><div id="Existential_risk_from_artificial_intelligence" style="font-size:114%;margin:0 4em"><a href="/wiki/Existential_risk_from_artificial_general_intelligence" title="Existential risk from artificial general intelligence">Existential risk</a> from <a href="/wiki/Artificial_intelligence" title="Artificial intelligence">artificial intelligence</a></div></th></tr><tr><th scope="row" class="navbox-group" style="width:1%">Concepts</th><td class="navbox-list navbox-odd hlist" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/Accelerating_change" title="Accelerating change">Accelerating change</a></li>
<li><a href="/wiki/AI_box" title="AI box">AI box</a></li>
<li><a href="/wiki/AI_takeover" title="AI takeover">AI takeover</a></li>
<li><a href="/wiki/AI_control_problem" title="AI control problem">Control problem</a></li>
<li><a href="/wiki/Existential_risk_from_artificial_general_intelligence" title="Existential risk from artificial general intelligence">Existential risk from artificial general intelligence</a></li>
<li><a href="/wiki/Friendly_artificial_intelligence" title="Friendly artificial intelligence">Friendly artificial intelligence</a></li>
<li><a class="mw-selflink selflink">Instrumental convergence</a></li>
<li><a href="/wiki/Intelligence_explosion" class="mw-redirect" title="Intelligence explosion">Intelligence explosion</a></li>
<li><a href="/wiki/Machine_ethics" title="Machine ethics">Machine ethics</a></li>
<li><a href="/wiki/Superintelligence" title="Superintelligence">Superintelligence</a></li>
<li><a href="/wiki/Technological_singularity" title="Technological singularity">Technological singularity</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Organizations</th><td class="navbox-list navbox-even hlist" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/Allen_Institute_for_AI" title="Allen Institute for AI">Allen Institute for AI</a></li>
<li><a href="/wiki/Center_for_Applied_Rationality" title="Center for Applied Rationality">Center for Applied Rationality</a></li>
<li><a href="/wiki/Center_for_Human-Compatible_Artificial_Intelligence" title="Center for Human-Compatible Artificial Intelligence">Center for Human-Compatible Artificial Intelligence</a></li>
<li><a href="/wiki/Center_for_Security_and_Emerging_Technology" title="Center for Security and Emerging Technology">Center for Security and Emerging Technology</a></li>
<li><a href="/wiki/Centre_for_the_Study_of_Existential_Risk" title="Centre for the Study of Existential Risk">Centre for the Study of Existential Risk</a></li>
<li><a href="/wiki/DeepMind" title="DeepMind">DeepMind</a></li>
<li><a href="/wiki/Foundational_Questions_Institute" title="Foundational Questions Institute">Foundational Questions Institute</a></li>
<li><a href="/wiki/Future_of_Humanity_Institute" title="Future of Humanity Institute">Future of Humanity Institute</a></li>
<li><a href="/wiki/Future_of_Life_Institute" title="Future of Life Institute">Future of Life Institute</a></li>
<li><a href="/wiki/Humanity%2B" title="Humanity+">Humanity+</a></li>
<li><a href="/wiki/Institute_for_Ethics_and_Emerging_Technologies" title="Institute for Ethics and Emerging Technologies">Institute for Ethics and Emerging Technologies</a></li>
<li><a href="/wiki/Leverhulme_Centre_for_the_Future_of_Intelligence" title="Leverhulme Centre for the Future of Intelligence">Leverhulme Centre for the Future of Intelligence</a></li>
<li><a href="/wiki/Machine_Intelligence_Research_Institute" title="Machine Intelligence Research Institute">Machine Intelligence Research Institute</a></li>
<li><a href="/wiki/OpenAI" title="OpenAI">OpenAI</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">People</th><td class="navbox-list navbox-odd hlist" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/Nick_Bostrom" title="Nick Bostrom">Nick Bostrom</a></li>
<li><a href="/wiki/K._Eric_Drexler" title="K. Eric Drexler">Eric Drexler</a></li>
<li><a href="/wiki/Sam_Harris" title="Sam Harris">Sam Harris</a></li>
<li><a href="/wiki/Stephen_Hawking" title="Stephen Hawking">Stephen Hawking</a></li>
<li><a href="/wiki/Bill_Hibbard" title="Bill Hibbard">Bill Hibbard</a></li>
<li><a href="/wiki/Bill_Joy" title="Bill Joy">Bill Joy</a></li>
<li><a href="/wiki/Elon_Musk" title="Elon Musk">Elon Musk</a></li>
<li><a href="/wiki/Steve_Omohundro" title="Steve Omohundro">Steve Omohundro</a></li>
<li><a href="/wiki/Huw_Price" title="Huw Price">Huw Price</a></li>
<li><a href="/wiki/Martin_Rees" title="Martin Rees">Martin Rees</a></li>
<li><a href="/wiki/Stuart_J._Russell" title="Stuart J. Russell">Stuart J. Russell</a></li>
<li><a href="/wiki/Jaan_Tallinn" title="Jaan Tallinn">Jaan Tallinn</a></li>
<li><a href="/wiki/Max_Tegmark" title="Max Tegmark">Max Tegmark</a></li>
<li><a href="/wiki/Frank_Wilczek" title="Frank Wilczek">Frank Wilczek</a></li>
<li><a href="/wiki/Roman_Yampolskiy" title="Roman Yampolskiy">Roman Yampolskiy</a></li>
<li><a href="/wiki/Andrew_Yang" title="Andrew Yang">Andrew Yang</a></li>
<li><a href="/wiki/Eliezer_Yudkowsky" title="Eliezer Yudkowsky">Eliezer Yudkowsky</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Other</th><td class="navbox-list navbox-even hlist" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/Global_catastrophic_risk#Artificial_intelligence" title="Global catastrophic risk">Artificial intelligence as a global catastrophic risk</a></li>
<li><a href="/wiki/Artificial_general_intelligence#Controversies_and_dangers" title="Artificial general intelligence">Controversies and dangers of artificial general intelligence</a></li>
<li><a href="/wiki/Ethics_of_artificial_intelligence" title="Ethics of artificial intelligence">Ethics of artificial intelligence</a></li>
<li><i><a href="/wiki/Human_Compatible" title="Human Compatible">Human Compatible</a></i></li>
<li><a href="/wiki/Open_Letter_on_Artificial_Intelligence" title="Open Letter on Artificial Intelligence">Open Letter on Artificial Intelligence</a></li>
<li><i><a href="/wiki/Our_Final_Invention" title="Our Final Invention">Our Final Invention</a></i></li>
<li><i><a href="/wiki/The_Precipice:_Existential_Risk_and_the_Future_of_Humanity" title="The Precipice: Existential Risk and the Future of Humanity">The Precipice</a></i></li>
<li><i><a href="/wiki/Superintelligence:_Paths,_Dangers,_Strategies" title="Superintelligence: Paths, Dangers, Strategies">Superintelligence: Paths, Dangers, Strategies</a></i></li></ul>
</div></td></tr><tr><td class="navbox-abovebelow" colspan="2"><div><img alt="Category" src="//upload.wikimedia.org/wikipedia/en/thumb/4/48/Folder_Hexagonal_Icon.svg/16px-Folder_Hexagonal_Icon.svg.png" decoding="async" title="Category" width="16" height="14" srcset="//upload.wikimedia.org/wikipedia/en/thumb/4/48/Folder_Hexagonal_Icon.svg/24px-Folder_Hexagonal_Icon.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/4/48/Folder_Hexagonal_Icon.svg/32px-Folder_Hexagonal_Icon.svg.png 2x" data-file-width="36" data-file-height="31" /> <a href="/wiki/Category:Existential_risk_from_artificial_general_intelligence" title="Category:Existential risk from artificial general intelligence">Category</a></div></td></tr></tbody></table></div>
<!-- 
NewPP limit report
Parsed by mw1364
Cached time: 20200322032149
Cache expiry: 2592000
Dynamic content: false
Complications: [vary‐revision‐sha1]
CPU time usage: 0.356 seconds
Real time usage: 0.510 seconds
Preprocessor visited node count: 1731/1000000
Post‐expand include size: 46167/2097152 bytes
Template argument size: 5769/2097152 bytes
Highest expansion depth: 13/40
Expensive parser function count: 4/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 36751/5000000 bytes
Number of Wikibase entities loaded: 2/400
Lua time usage: 0.171/10.000 seconds
Lua memory usage: 5.18 MB/50 MB
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%  459.427      1 -total
 23.19%  106.540      1 Template:Quote
 22.35%  102.674      1 Template:Reflist
 17.38%   79.856      4 Template:Cite_news
 14.25%   65.468      5 Template:Fix
 14.23%   65.397      4 Template:Fcn
 13.69%   62.903      1 Template:Short_description
 11.61%   53.341      2 Template:Cite_journal
 10.00%   45.936      1 Template:Pagetype
  8.92%   41.001      1 Template:ISBN
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:43591208-0!canonical and timestamp 20200322032149 and revision id 942082132
 -->
</div><noscript><img src="//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" alt="" title="" width="1" height="1" style="border: none; position: absolute;" /></noscript></div>
		<div class="printfooter">Retrieved from "<a dir="ltr" href="https://en.wikipedia.org/w/index.php?title=Instrumental_convergence&amp;oldid=942082132">https://en.wikipedia.org/w/index.php?title=Instrumental_convergence&amp;oldid=942082132</a>"</div>
		<div id="catlinks" class="catlinks" data-mw="interface"><div id="mw-normal-catlinks" class="mw-normal-catlinks"><a href="/wiki/Help:Category" title="Help:Category">Categories</a>: <ul><li><a href="/wiki/Category:Artificial_intelligence" title="Category:Artificial intelligence">Artificial intelligence</a></li><li><a href="/wiki/Category:Goal" title="Category:Goal">Goal</a></li><li><a href="/wiki/Category:Intention" title="Category:Intention">Intention</a></li><li><a href="/wiki/Category:Risk" title="Category:Risk">Risk</a></li><li><a href="/wiki/Category:Existential_risk_from_artificial_general_intelligence" title="Category:Existential risk from artificial general intelligence">Existential risk from artificial general intelligence</a></li></ul></div><div id="mw-hidden-catlinks" class="mw-hidden-catlinks mw-hidden-cats-hidden">Hidden categories: <ul><li><a href="/wiki/Category:Articles_with_short_description" title="Category:Articles with short description">Articles with short description</a></li><li><a href="/wiki/Category:All_articles_with_incomplete_citations" title="Category:All articles with incomplete citations">All articles with incomplete citations</a></li><li><a href="/wiki/Category:Articles_with_incomplete_citations_from_September_2018" title="Category:Articles with incomplete citations from September 2018">Articles with incomplete citations from September 2018</a></li><li><a href="/wiki/Category:Wikipedia_articles_needing_page_number_citations_from_September_2018" title="Category:Wikipedia articles needing page number citations from September 2018">Wikipedia articles needing page number citations from September 2018</a></li></ul></div></div>
		<div class="visualClear"></div>
		
	</div>
</div>
<div id='mw-data-after-content'>
	<div class="read-more-container"></div>
</div>

<div id="mw-navigation">
    <h2>Navigation menu</h2>
    <div id="mw-head">
        
        <div id="p-personal" role="navigation" class="" aria-labelledby="p-personal-label">
        	<h3 id="p-personal-label">Personal tools</h3>
        	<ul >
        		
        		<li id="pt-anonuserpage">Not logged in</li>
        		<li id="pt-anontalk"><a href="/wiki/Special:MyTalk" title="Discussion about edits from this IP address [n]" accesskey="n">Talk</a></li><li id="pt-anoncontribs"><a href="/wiki/Special:MyContributions" title="A list of edits made from this IP address [y]" accesskey="y">Contributions</a></li><li id="pt-createaccount"><a href="/w/index.php?title=Special:CreateAccount&amp;returnto=Instrumental+convergence" title="You are encouraged to create an account and log in; however, it is not mandatory">Create account</a></li><li id="pt-login"><a href="/w/index.php?title=Special:UserLogin&amp;returnto=Instrumental+convergence" title="You&#039;re encouraged to log in; however, it&#039;s not mandatory. [o]" accesskey="o">Log in</a></li>
        	</ul>
        </div>
        <div id="left-navigation">
            <div id="p-namespaces" role="navigation" class="vectorTabs " aria-labelledby="p-namespaces-label">
            	<h3 id="p-namespaces-label">Namespaces</h3>
            	<ul >
            		<li id="ca-nstab-main" class="selected"><a href="/wiki/Instrumental_convergence" title="View the content page [c]" accesskey="c">Article</a></li><li id="ca-talk"><a href="/wiki/Talk:Instrumental_convergence" rel="discussion" title="Discussion about the content page [t]" accesskey="t">Talk</a></li>
            	</ul>
            </div>
            <div id="p-variants" role="navigation" class="vectorMenu emptyPortlet" aria-labelledby="p-variants-label">
            	<input type="checkbox" class="vectorMenuCheckbox" aria-labelledby="p-variants-label" />
            	<h3 id="p-variants-label">
            		<span>Variants</span>
            	</h3>
            	<ul class="menu" >
            		
            	</ul>
            </div>
        </div>
        <div id="right-navigation">
            <div id="p-views" role="navigation" class="vectorTabs " aria-labelledby="p-views-label">
            	<h3 id="p-views-label">Views</h3>
            	<ul >
            		<li id="ca-view" class="collapsible selected"><a href="/wiki/Instrumental_convergence">Read</a></li><li id="ca-edit" class="collapsible"><a href="/w/index.php?title=Instrumental_convergence&amp;action=edit" title="Edit this page [e]" accesskey="e">Edit</a></li><li id="ca-history" class="collapsible"><a href="/w/index.php?title=Instrumental_convergence&amp;action=history" title="Past revisions of this page [h]" accesskey="h">View history</a></li>
            	</ul>
            </div>
            <div id="p-cactions" role="navigation" class="vectorMenu emptyPortlet" aria-labelledby="p-cactions-label">
            	<input type="checkbox" class="vectorMenuCheckbox" aria-labelledby="p-cactions-label" />
            	<h3 id="p-cactions-label">
            		<span>More</span>
            	</h3>
            	<ul class="menu" >
            		
            	</ul>
            </div>
            <div id="p-search" role="search">
            	<h3 >
            		<label for="searchInput">Search</label>
            	</h3>
            	<form action="/w/index.php" id="searchform">
            		<div id="simpleSearch">
            			<input type="search" name="search" placeholder="Search Wikipedia" title="Search Wikipedia [f]" accesskey="f" id="searchInput"/>
            			<input type="hidden" value="Special:Search" name="title"/>
            			<input type="submit" name="fulltext" value="Search" title="Search Wikipedia for this text" id="mw-searchButton" class="searchButton mw-fallbackSearchButton"/>
            			<input type="submit" name="go" value="Go" title="Go to a page with this exact name if it exists" id="searchButton" class="searchButton"/>
            		</div>
            	</form>
            </div>
        </div>
    </div>
    
    <div id="mw-panel">
    	<div id="p-logo" role="banner">
    		<a  title="Visit the main page" class="mw-wiki-logo" href="/wiki/Main_Page"></a>
    	</div>
    	<div class="portal" role="navigation" id="p-navigation"  aria-labelledby="p-navigation-label">
    		<h3  id="p-navigation-label">
    			Navigation
    		</h3>
    		<div class="body">
    			<ul><li id="n-mainpage-description"><a href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z">Main page</a></li><li id="n-contents"><a href="/wiki/Wikipedia:Contents" title="Guides to browsing Wikipedia">Contents</a></li><li id="n-featuredcontent"><a href="/wiki/Wikipedia:Featured_content" title="Featured content – the best of Wikipedia">Featured content</a></li><li id="n-currentevents"><a href="/wiki/Portal:Current_events" title="Find background information on current events">Current events</a></li><li id="n-randompage"><a href="/wiki/Special:Random" title="Load a random article [x]" accesskey="x">Random article</a></li><li id="n-sitesupport"><a href="https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&amp;utm_medium=sidebar&amp;utm_campaign=C13_en.wikipedia.org&amp;uselang=en" title="Support us">Donate to Wikipedia</a></li><li id="n-shoplink"><a href="//shop.wikimedia.org" title="Visit the Wikipedia store">Wikipedia store</a></li></ul>
    			
    		</div>
    	</div>
    	
    	<div class="portal" role="navigation" id="p-interaction"  aria-labelledby="p-interaction-label">
    		<h3  id="p-interaction-label">
    			Interaction
    		</h3>
    		<div class="body">
    			<ul><li id="n-help"><a href="/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia">Help</a></li><li id="n-aboutsite"><a href="/wiki/Wikipedia:About" title="Find out about Wikipedia">About Wikipedia</a></li><li id="n-portal"><a href="/wiki/Wikipedia:Community_portal" title="About the project, what you can do, where to find things">Community portal</a></li><li id="n-recentchanges"><a href="/wiki/Special:RecentChanges" title="A list of recent changes in the wiki [r]" accesskey="r">Recent changes</a></li><li id="n-contactpage"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia">Contact page</a></li></ul>
    			
    		</div>
    	</div>
    	
    	<div class="portal" role="navigation" id="p-tb"  aria-labelledby="p-tb-label">
    		<h3  id="p-tb-label">
    			Tools
    		</h3>
    		<div class="body">
    			<ul><li id="t-whatlinkshere"><a href="/wiki/Special:WhatLinksHere/Instrumental_convergence" title="List of all English Wikipedia pages containing links to this page [j]" accesskey="j">What links here</a></li><li id="t-recentchangeslinked"><a href="/wiki/Special:RecentChangesLinked/Instrumental_convergence" rel="nofollow" title="Recent changes in pages linked from this page [k]" accesskey="k">Related changes</a></li><li id="t-upload"><a href="/wiki/Wikipedia:File_Upload_Wizard" title="Upload files [u]" accesskey="u">Upload file</a></li><li id="t-specialpages"><a href="/wiki/Special:SpecialPages" title="A list of all special pages [q]" accesskey="q">Special pages</a></li><li id="t-permalink"><a href="/w/index.php?title=Instrumental_convergence&amp;oldid=942082132" title="Permanent link to this revision of the page">Permanent link</a></li><li id="t-info"><a href="/w/index.php?title=Instrumental_convergence&amp;action=info" title="More information about this page">Page information</a></li><li id="t-wikibase"><a href="https://www.wikidata.org/wiki/Special:EntityPage/Q18208100" title="Link to connected data repository item [g]" accesskey="g">Wikidata item</a></li><li id="t-cite"><a href="/w/index.php?title=Special:CiteThisPage&amp;page=Instrumental_convergence&amp;id=942082132&amp;wpFormIdentifier=titleform" title="Information on how to cite this page">Cite this page</a></li></ul>
    			
    		</div>
    	</div>
    	
    	<div class="portal" role="navigation" id="p-coll-print_export"  aria-labelledby="p-coll-print_export-label">
    		<h3  id="p-coll-print_export-label">
    			Print/export
    		</h3>
    		<div class="body">
    			<ul><li id="coll-create_a_book"><a href="/w/index.php?title=Special:Book&amp;bookcmd=book_creator&amp;referer=Instrumental+convergence">Create a book</a></li><li id="coll-download-as-rl"><a href="/w/index.php?title=Special:ElectronPdf&amp;page=Instrumental+convergence&amp;action=show-download-screen">Download as PDF</a></li><li id="t-print"><a href="/w/index.php?title=Instrumental_convergence&amp;printable=yes" title="Printable version of this page [p]" accesskey="p">Printable version</a></li></ul>
    			
    		</div>
    	</div>
    	
    	<div class="portal" role="navigation" id="p-lang"  aria-labelledby="p-lang-label">
    		<h3  id="p-lang-label">
    			Languages
    		</h3>
    		<div class="body">
    			<ul><li class="interlanguage-link interwiki-es"><a href="https://es.wikipedia.org/wiki/Convergencia_instrumental" title="Convergencia instrumental – Spanish" lang="es" hreflang="es" class="interlanguage-link-target">Español</a></li></ul>
    			<div class="after-portlet after-portlet-lang"><span class="wb-langlinks-edit wb-langlinks-link"><a href="https://www.wikidata.org/wiki/Special:EntityPage/Q18208100#sitelinks-wikipedia" title="Edit interlanguage links" class="wbc-editpage">Edit links</a></span></div>
    		</div>
    	</div>
    	
    </div>
</div>

<div id="footer" role="contentinfo" >
	<ul id="footer-info" class="">
		<li id="footer-info-lastmod"> This page was last edited on 22 February 2020, at 13:50<span class="anonymous-show">&#160;(UTC)</span>.</li>
		<li id="footer-info-copyright">Text is available under the <a rel="license" href="//en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License">Creative Commons Attribution-ShareAlike License</a><a rel="license" href="//creativecommons.org/licenses/by-sa/3.0/" style="display:none;"></a>;
additional terms may apply.  By using this site, you agree to the <a href="//foundation.wikimedia.org/wiki/Terms_of_Use">Terms of Use</a> and <a href="//foundation.wikimedia.org/wiki/Privacy_policy">Privacy Policy</a>. Wikipedia® is a registered trademark of the <a href="//www.wikimediafoundation.org/">Wikimedia Foundation, Inc.</a>, a non-profit organization.</li>
	</ul>
	<ul id="footer-places" class="">
		<li id="footer-places-privacy"><a href="https://foundation.wikimedia.org/wiki/Privacy_policy" class="extiw" title="wmf:Privacy policy">Privacy policy</a></li>
		<li id="footer-places-about"><a href="/wiki/Wikipedia:About" title="Wikipedia:About">About Wikipedia</a></li>
		<li id="footer-places-disclaimer"><a href="/wiki/Wikipedia:General_disclaimer" title="Wikipedia:General disclaimer">Disclaimers</a></li>
		<li id="footer-places-contact"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us">Contact Wikipedia</a></li>
		<li id="footer-places-developers"><a href="https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute">Developers</a></li>
		<li id="footer-places-statslink"><a href="https://stats.wikimedia.org/#/en.wikipedia.org">Statistics</a></li>
		<li id="footer-places-cookiestatement"><a href="https://foundation.wikimedia.org/wiki/Cookie_statement">Cookie statement</a></li>
		<li id="footer-places-mobileview"><a href="//en.m.wikipedia.org/w/index.php?title=Instrumental_convergence&amp;mobileaction=toggle_view_mobile" class="noprint stopMobileRedirectToggle">Mobile view</a></li>
	</ul>
	<ul id="footer-icons" class="noprint">
		<li id="footer-copyrightico"><a href="https://wikimediafoundation.org/"><img src="/static/images/wikimedia-button.png" srcset="/static/images/wikimedia-button-1.5x.png 1.5x, /static/images/wikimedia-button-2x.png 2x" width="88" height="31" alt="Wikimedia Foundation"/></a></li>
		<li id="footer-poweredbyico"><a href="https://www.mediawiki.org/"><img src="/static/images/poweredby_mediawiki_88x31.png" alt="Powered by MediaWiki" srcset="/static/images/poweredby_mediawiki_132x47.png 1.5x, /static/images/poweredby_mediawiki_176x62.png 2x" width="88" height="31"/></a></li>
	</ul>
	<div style="clear: both;"></div>
</div>

<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgPageParseReport":{"limitreport":{"cputime":"0.356","walltime":"0.510","ppvisitednodes":{"value":1731,"limit":1000000},"postexpandincludesize":{"value":46167,"limit":2097152},"templateargumentsize":{"value":5769,"limit":2097152},"expansiondepth":{"value":13,"limit":40},"expensivefunctioncount":{"value":4,"limit":500},"unstrip-depth":{"value":1,"limit":20},"unstrip-size":{"value":36751,"limit":5000000},"entityaccesscount":{"value":2,"limit":400},"timingprofile":["100.00%  459.427      1 -total"," 23.19%  106.540      1 Template:Quote"," 22.35%  102.674      1 Template:Reflist"," 17.38%   79.856      4 Template:Cite_news"," 14.25%   65.468      5 Template:Fix"," 14.23%   65.397      4 Template:Fcn"," 13.69%   62.903      1 Template:Short_description"," 11.61%   53.341      2 Template:Cite_journal"," 10.00%   45.936      1 Template:Pagetype","  8.92%   41.001      1 Template:ISBN"]},"scribunto":{"limitreport-timeusage":{"value":"0.171","limit":"10.000"},"limitreport-memusage":{"value":5427405,"limit":52428800}},"cachereport":{"origin":"mw1364","timestamp":"20200322032149","ttl":2592000,"transientcontent":false}}});});</script>
<script type="application/ld+json">{"@context":"https:\/\/schema.org","@type":"Article","name":"Instrumental convergence","url":"https:\/\/en.wikipedia.org\/wiki\/Instrumental_convergence","sameAs":"http:\/\/www.wikidata.org\/entity\/Q18208100","mainEntity":"http:\/\/www.wikidata.org\/entity\/Q18208100","author":{"@type":"Organization","name":"Contributors to Wikimedia projects"},"publisher":{"@type":"Organization","name":"Wikimedia Foundation, Inc.","logo":{"@type":"ImageObject","url":"https:\/\/www.wikimedia.org\/static\/images\/wmf-hor-googpub.png"}},"datePublished":"2014-08-18T05:14:31Z","dateModified":"2020-02-22T13:50:27Z","headline":"hypothetical tendency for sufficiently intelligent agents to pursue unbounded instrumental goals such as self-preservation and resource acquisition"}</script>
<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgBackendResponseTime":84,"wgHostname":"mw1384"});});</script></body></html>
